<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Times New Roman:300,300italic,400,400italic,700,700italic|18:300,300italic,400,400italic,700,700italic|Courier New:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hello, World" />










<meta name="description" content="深度学习引言什么是神经网络上图说明了最简的神经网络，小圆圈表示一个单独的神经元，连接你的网络实现左边图的功能。稍微大点的神经网络如下图所示。当实现它之后,要做的只是输入x，就能得到输出y。因为它可以自己计算训练集中样本的数目以及所有的中间过程。 结构化数据和非结构化数据  结构化数据意思是每个特征，比如说房屋大小、卧室数量，或者是一个用户的年龄，都有一个很好的定义。例如在房价预测中，可能有一">
<meta property="og:type" content="article">
<meta property="og:title" content="01.神经网络和深度学习">
<meta property="og:url" content="https://muyi110.github.io/2018/01-神经网络和深度学习/index.html">
<meta property="og:site_name" content="MuYi&#39;s Blog">
<meta property="og:description" content="深度学习引言什么是神经网络上图说明了最简的神经网络，小圆圈表示一个单独的神经元，连接你的网络实现左边图的功能。稍微大点的神经网络如下图所示。当实现它之后,要做的只是输入x，就能得到输出y。因为它可以自己计算训练集中样本的数目以及所有的中间过程。 结构化数据和非结构化数据  结构化数据意思是每个特征，比如说房屋大小、卧室数量，或者是一个用户的年龄，都有一个很好的定义。例如在房价预测中，可能有一">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_001.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_002.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_003.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_004.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_006.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_005.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_007.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_008.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_009.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_010.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_011.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_012.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_013.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_014.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_015.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_016.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_017.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_018.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_019.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_020.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_021.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_022.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_023.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_024.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_025.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_026.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_027.png">
<meta property="og:updated_time" content="2018-09-16T04:39:31.523Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="01.神经网络和深度学习">
<meta name="twitter:description" content="深度学习引言什么是神经网络上图说明了最简的神经网络，小圆圈表示一个单独的神经元，连接你的网络实现左边图的功能。稍微大点的神经网络如下图所示。当实现它之后,要做的只是输入x，就能得到输出y。因为它可以自己计算训练集中样本的数目以及所有的中间过程。 结构化数据和非结构化数据  结构化数据意思是每个特征，比如说房屋大小、卧室数量，或者是一个用户的年龄，都有一个很好的定义。例如在房价预测中，可能有一">
<meta name="twitter:image" content="https://muyi110.github.io/images/deeplearning_ai_001.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://muyi110.github.io/2018/01-神经网络和深度学习/"/>





  <title>01.神经网络和深度学习 | MuYi's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">MuYi's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">此博客创建于2018-07-10</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-reprinted_article">
          <a href="/reprinted-article/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heart"></i> <br />
            
            转载文章
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://muyi110.github.io/2018/01-神经网络和深度学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="穆义">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/wukong.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MuYi's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">01.神经网络和深度学习</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-27T20:28:08+08:00">
                2018-07-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习课程-Andrew-Ng-学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习课程(Andrew Ng)学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h1 id="深度学习引言"><a href="#深度学习引言" class="headerlink" title="深度学习引言"></a>深度学习引言</h1><h2 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络"></a>什么是神经网络</h2><p><img src="/images/deeplearning_ai_001.png" alt="简单神经网络引子"><br>上图说明了最简的神经网络，小圆圈表示一个单独的神经元，连接你的网络实现左边图的功能。稍微大点的神经网络如下图所示。<br><img src="/images/deeplearning_ai_002.png" alt="稍微大点神经网络"><br>当实现它之后,要做的只是输入<code>x</code>，就能得到输出<code>y</code>。因为它可以自己计算训练集中样本的数目以及所有的中间过程。</p>
<h2 id="结构化数据和非结构化数据"><a href="#结构化数据和非结构化数据" class="headerlink" title="结构化数据和非结构化数据"></a>结构化数据和非结构化数据</h2><p><img src="/images/deeplearning_ai_003.png" alt="结构化数据和非结构化数据"></p>
<ol>
<li><strong>结构化数据</strong>意思是每个特征，比如说房屋大小、卧室数量，或者是一个用户的年龄，都有一个很好的定义。例如在房价预测中，可能有一个数据库，有专门的几列数据说明卧室的大小和数量，这就是结构化数据。或预测用户是否会点击广告，可能会得到关于用户的信息，比如年龄以及关于广告的一些信息，然后对预测分类标注，这就是结构化数据。</li>
<li><strong>非结构化数据</strong>是指比如音频或者你想要识别的图像或文本中的内容。这里的特征可能是图像中的像素值或文本中的单个单词。</li>
</ol>
<h2 id="深度学习兴起的原因"><a href="#深度学习兴起的原因" class="headerlink" title="深度学习兴起的原因"></a>深度学习兴起的原因</h2><p><img src="/images/deeplearning_ai_004.png" alt="规模驱动深度学习">事实上，在神经网络上获得更好的性能的可靠方法是<strong>要么训练一个更大的神经网络，要么投入更多的数据</strong>，这只能在一定程度上起作用，因为最终会耗尽数据，或者最终网络规模很大导致花费很长时间去训练。<img src="/images/deeplearning_ai_006.png" alt="深度学习兴起总结"></p>
<blockquote>
<p>许多算法的创新都在尝试使神经网络跑得更快，一个例子如下图（激活函数的改变）。<br><img src="/images/deeplearning_ai_005.png" alt="激活函数创新">使用<code>sigmoid</code>函数问题是，在横坐标值很大或很小区域，<code>sigmoid</code>函数的梯度会接近零，学习的速度会变得非常缓慢，当实现梯度下降以及梯度接近零的时候，参数会更新的很慢，所以学习的速率会变的很慢，通过改变激活函数为<code>ReLU</code>的函数(修正线性单元)，<code>ReLU</code>的梯度对于所有输入的负值都是零，因此梯度不会趋向逐渐减少到零。通过将<code>Sigmod</code>函数转换成<code>ReLU</code>函数，能够使得梯度下降<code>(gradient descent)</code>算法运行的更快。</p>
</blockquote>
<h1 id="神经网络编程基础"><a href="#神经网络编程基础" class="headerlink" title="神经网络编程基础"></a>神经网络编程基础</h1><blockquote>
</blockquote>
<ul>
<li>实现神经网络的时候，通常不使用<code>for</code>循环遍历整个训练集（使用向量化的手段实现）。</li>
<li>神经网络的计算中，通常先有<strong>前向传播</strong><code>(foward propagation)</code>步骤，接着有<strong>反向传播</strong><code>(backward propagation)</code>步骤。 </li>
</ul>
<h2 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a>logistic回归</h2><p><strong>以<code>logistic regression</code>为基础，来引入神经网络。</strong><code>logistic regression</code>是一个用于处理二分类<code>(binary classification)</code>的算法。<code>logistic regression</code>使用的参数如下：</p>
<ul>
<li>输入特征向量：\(x\in R^{n_x}\)，其中\(n_x\)表示特征向量的维数</li>
<li>样本标签：\(y\in \{0,1\}\)</li>
<li>权重：\(w\in R^{n_x}\)</li>
<li>偏置：\(b\in R\)</li>
<li>输出：\(\widehat {y}=\sigma(w^Tx+b)\)，其中\(\sigma\)是<code>Sigmoid</code>函数</li>
</ul>
<p><code>logistic regression</code>可以看成一个很小的神经网络，例子如下图所示。<img src="/images/deeplearning_ai_007.png" alt="logistic 回归例子"></p>
<ul>
<li><strong>下面给出相关符号定义：</strong><br>x:表示一个\(n_x\)维数据，为输入，维度为\((n_x,1)\)<br>y:表示输出结果，取值为\((0,1)\)<br>\((x^{(i)},y^{(i)})\):表示第\(i\)组数据<br>\(X=[x^{(1)},x^{(2)},\ldots ,x^{(m)}]\):表示数据集的所有输入值，\(X\)为\(n_x \times m\)的矩阵，其中\(m\)表示样本数目（样本横着放）<br>\(Y=[y^{(1)},y^{(2)},\ldots ,y^{(m)}]\):表示数据集的所有输出值，维度为\(1\times m\)(样本横着放）</li>
</ul>
<h2 id="代价函数-Cost-Function"><a href="#代价函数-Cost-Function" class="headerlink" title="代价函数(Cost Function)"></a>代价函数(Cost Function)</h2><blockquote>
<p>通过训练<strong>代价函数</strong>来得到<code>logistic regression</code>的参数\(w\)与\(b\)</p>
</blockquote>
<p><strong>损失函数</strong>用来衡量输出值和实际值的接近程度，一般情况损失函数定义为平方差损失，如下：$$L(\widehat {y},y)=\dfrac{1}{2}(\widehat y -y)^2$$但<code>logistic</code>回归不这么做，因为之后的优化目标变为<strong>非凸的</strong>，梯度下降法可能找不到全局最优。<br><code>logistic</code>回归的损失函数一般用$$L(\widehat {y},y)=-(y\log{\widehat y})-(1-y)\log{(1-\widehat y)}$$损失函数是在<strong>单个</strong>训练样本中定义，衡量算法在单个训练样本中表现如何。<strong>代价函数</strong>衡量算法在全部训练样本上的表现，是对\(m\)个样本损失函数求和并除以\(m\)，公式如下$$J(w,b)=\dfrac{1}{m}\sum_{i=1}^{m}L(\widehat {y}^{(i)},y^{(i)})=\dfrac{1}{m}\sum_{i=1}^{m}\Big(-y^{(i)}\log{\widehat {y}^{(i)}}-(1-y^{(i)})\log(1-\widehat {y}^{(i)})\Big)$$可以通过极大似然推导。</p>
<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><blockquote>
<p>梯度：函数在某一点的梯度是这样一个向量，它的方向与取得最大<a href="http://math.fudan.edu.cn/gdsx/KEJIAN/%E6%96%B9%E5%90%91%E5%AF%BC%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6.pdf" target="_blank" rel="noopener">方向导数</a>的方向一致，而它的模为<a href="http://math.fudan.edu.cn/gdsx/KEJIAN/%E6%96%B9%E5%90%91%E5%AF%BC%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6.pdf" target="_blank" rel="noopener">方向导数</a>的最大值。<strong>沿梯度方向，函数值增加最快（此时方向导数是正值，取最大）</strong>。</p>
</blockquote>
<p>模型的训练目标是找到合适的参数\(w\)与\(b\)以最小化<strong>代价函数</strong>，为便于演示，假设参数都是一维的，可以得到如下参数\(w\)与\(b\)和代价函数\(J\)的关系图:<img src="/images/deeplearning_ai_008.png" alt="代价函数图">代价函数<code>J</code>是一个凸函数，有一个全局最低点（无论如何选择初始值，都可找到最优值）。<br>参数\(w\)的更新公式为$$w:=w-\alpha\dfrac{\partial {J(w,b)}}{\partial w}$$参数\(b\)的更新公式$$b:=b-\alpha\dfrac{\partial {J(w,b)}}{\partial b}$$其中\(\alpha\)表示学习率，即每次更新\(w\)的步长。</p>
<h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>神经网络中的计算是由多个计算网络输出的<strong>前向传播</strong>与<strong>计算梯度的后向传播</strong>构成。所谓的<strong>反向传播</strong>也就是计算梯度，用于更新参数的。计算图例子如下:<img src="/images/deeplearning_ai_009.png" alt="计算图例子"></p>
<h2 id="Logistic-回归的梯度下降法"><a href="#Logistic-回归的梯度下降法" class="headerlink" title="Logistic 回归的梯度下降法"></a>Logistic 回归的梯度下降法</h2><p>假设输入的特征向量是\(2\)维的，则输入参数共有\(x_1\)，\(x_2\)，\(w_1\)，\(w_2\)，\(b\)五个参数，得到其计算图如下:<img src="/images/deeplearning_ai_010.png" alt="logistic 计算图">首先反向求出\(L\)对\(a\)的倒数:$$da=\dfrac{dL(a,y)}{da}=-\dfrac{y}{a}+\dfrac{1-y}{1-a}$$继续反向求出\(L\)对\(z\)的导数:$$dz=\dfrac{dL}{dz}=\dfrac{dL}{da}\dfrac{da}{dz}=a-y$$类似反向传播可以求出损失函数\(L\)对参数\(w_1\)，\(w_2\)，\(b\)的导数。根据如下公式对参数更新:$$w_1:=w_1-\alpha dw_1$$$$w_2:=w_2-\alpha dw_2$$$$b:=b-\alpha db$$开始将单个样本拓展到整个训练集的代价函数，如下：$$J(w,b)=\dfrac{1}{m}\sum_{i=1}^{m}L(\widehat {y}^{(i)},y^{(i)})$$$$a^{(i)}=\widehat {y}^{(i)}=\sigma (z^{(i)})=\sigma (w^{T}x^{(i)}+b)$$对其中的某个参数求导（其他类似）：$$\dfrac{dJ(w,b)}{dw_1}=\dfrac{1}{m}\sum_{i=1}^{m}\dfrac{dL(a^{(i)},y^{(i)})}{dw_1}$$下面给出完整的<code>Logistic</code>回归的训练流程（假设输入特征为二维）:<img src="/images/deeplearning_ai_011.png" alt="logistic 回归流程图">以上完成梯度下降的一次迭代。<br>上述的流畅图的效率较低，因为需要编写两个<code>for</code>循环（第一个循环\(m\)个样本，第二个循环\(n\)个特征）。<strong>向量化</strong>可以显示解决<code>for</code>循环效率低的问题。</p>
<h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><blockquote>
<p>经验法则：编写代码，只要有其他可能就不要使用<code>for</code>循环，比如可以考虑<strong>向量化</strong></p>
</blockquote>
<ul>
<li><p>杂谈：向量化与<code>for</code>循环效率对比:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">tic = time.time()</span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">toc = time.time()</span><br><span class="line">print(c)</span><br><span class="line">print(<span class="string">'Vectorized version: '</span> + str((toc-tic)*<span class="number">1000</span>) + <span class="string">'ms'</span>)</span><br><span class="line"></span><br><span class="line">c=<span class="number">0</span></span><br><span class="line">tic = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    c += a[i] * b[i]</span><br><span class="line">toc = time.time()</span><br><span class="line">print(c)</span><br><span class="line">print(<span class="string">'For loop: '</span> + str((toc-tic)*<span class="number">1000</span>) + <span class="string">'ms'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>结果:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">250235.98496807975</span><br><span class="line">Vectorized version: 1.1947154998779297ms</span><br><span class="line">250235.98496807023</span><br><span class="line">For loop: 445.22929191589355ms</span><br><span class="line">可以说是差距巨大。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>回到<code>Logistic</code>回归上，其<strong>向量化</strong>版本的流程如下：<img src="/images/deeplearning_ai_012.png" alt="向量化版本">注意：上述只是完成一次迭代，多次迭代依然需要<code>for</code>循环。</p>
<h2 id="Numpy-中广播-broadcasting"><a href="#Numpy-中广播-broadcasting" class="headerlink" title="Numpy 中广播(broadcasting)"></a>Numpy 中广播(broadcasting)</h2><p><img src="/images/deeplearning_ai_013.png" alt="广播">详细介绍参考<a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html" target="_blank" rel="noopener">官方文档</a></p>
<h2 id="编程建议"><a href="#编程建议" class="headerlink" title="编程建议"></a>编程建议</h2><ol>
<li>不要使用形如<code>a.shape=(n,)</code>的向量，而应该使用形如<code>a.shape=(n,1)</code>的向量；</li>
<li>程序中要适当使用<code>assert(a.shape==(n,1))</code>，不要担心效率问题;</li>
</ol>
<h1 id="浅层神经网络"><a href="#浅层神经网络" class="headerlink" title="浅层神经网络"></a>浅层神经网络</h1><h2 id="神经网络表示"><a href="#神经网络表示" class="headerlink" title="神经网络表示"></a>神经网络表示</h2><p><img src="/images/deeplearning_ai_014.png" alt="神经网络表示">竖向堆叠的特征（对应一个样本）称为神经网络的<strong>输入层(the input layer)</strong>；<br><strong>隐藏层(hidden layer)</strong>的含义指在训练集中，中间节点的真正值无法看到；<br><strong>输出层(the output layer)</strong>负责输出预测值。<br>上图是一个<strong>双层神经网络</strong>，也称为<strong>单隐层神经网络</strong>。一般计算神经网络层数时，不考虑输入层。<br><strong>下面是约定的符号表示：</strong></p>
<ul>
<li>输入层的激活值为\(a^{[0]}\)(\(a^{[0]}=x)\)</li>
<li>隐藏层的激活值为\(a^{[1]}\)，其中第一个单元（或节点）表示为\(a_{1}^{[1]}\)，输出层类似。在上图中$$a^{[1]}=\begin{bmatrix} a_{1}^{[1]} \\ a_{2}^{[1]}  \\ a_{3}^{[1]}  \\ a_{4}^{[1]}  \end{bmatrix}$$</li>
<li>隐藏层和输出层都是带有参数\(W\)与\(b\)的，使用上标\([i]\)表示第\(i\)层的参数。在上图中\(W^{[1]}\)是一个\(4\times 3\)矩阵，\(b^{[1]}\)是一个\(4\times 1\)矩阵（因为有\(4\)个节点，\(3\)个输入特征）,同理\(W^{[2]}\)是一个\(1\times 4\)矩阵，\(b^{[2]}\)是一个\(1\times 1\)矩阵</li>
</ul>
<h2 id="计算神经网络的输出（前向传播）"><a href="#计算神经网络的输出（前向传播）" class="headerlink" title="计算神经网络的输出（前向传播）"></a>计算神经网络的输出（前向传播）</h2><p><img src="/images/deeplearning_ai_015.png" alt="神经网络计算">神经网络不过是将<code>Logistic</code>回归的计算步骤重复多次。详细的计算结果如下<img src="/images/deeplearning_ai_016.png" alt="神经网络计算结果">用向量表示如下$$z^{[1]}=(W^{[1]})a^{[0]}+b^{[1]}$$$$a^{[1]}=\sigma(z^{[1]})$$<br><img src="/images/deeplearning_ai_017.png" alt="向量表示">其中$$a^{[1]}=\begin{bmatrix} a_{1}^{[1]} \\ a_{2}^{[1]}  \\ a_{3}^{[1]}  \\ a_{4}^{[1]}  \end{bmatrix}$$同理对于输出层有$$z^{[2]}=(W^{[2]})a^{[1]}+b^{[2]}$$$$\widehat y=a^{[2]}=\sigma(z^{[2]})$$以上都是针对单个样本的情况，对于有\(m\)个样本，向量化的方式如下:<br><img src="/images/deeplearning_ai_018.png" alt="多向量表示">进而得到如下公式<img src="/images/deeplearning_ai_019.png" alt="向量表示结果"></p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>之前的激活函数选择都是<code>sigmoid</code>函数，单有时其他的激活函数效果可能会更好。<br>可选的激活函数有：</p>
<ul>
<li><code>tanh</code>函数（双曲正切函数）:$$a=\dfrac{e^z-e^{-z}}{e^z+e^{-z}}$$效果几乎总是比<code>sigmoid</code>函数好（除了<strong>二元分类的输出层</strong>，因为希望输出结果介于\(0\)与\(1\)之间），因为函数输出值介于\(-1\)与\(1\)之间，激活函数的平均值接近\(0\)，有类似数据中心化效果，方便下一层的学习。<br>但<code>tanh</code>函数和<code>sigmoid</code>函数有一个共同缺点：当\(z\)很大或很小时，函数梯度趋向于\(0\)，使得梯度下降算法很慢。</li>
<li><code>ReLu</code>函数(the rectified linear unit，修正线性单元):$$a=max(0,z)$$当\(z&gt;0\)时，梯度为\(1\)提高神经网络基于梯度的算法，编程时，在\(z=0\)处导数设为\(1\)或\(0\)都行。</li>
<li><code>Leaky ReLu</code>函数（带泄漏的<code>ReLu</code>）:$$a=max(0.01z,z)$$不常用（这里的\(0.01\)根据实际情况可修改）。</li>
</ul>
<p>四个激活函数的图形如下:<img src="/images/deeplearning_ai_020.png" alt="激活函数图形"><strong>选择激活函数的经验法则是：如果输出值是\(0\)，\(1\)值（二分类），则输出层选择<code>sigmoid</code>函数，其他的单元选择<code>ReLu</code>函数。这是很多激活函数的默认选择，如果在隐藏层上不确定选择哪个激活函数，通常选择<code>ReLu</code>函数，有时也选择<code>tanh</code>函数。当然不同的层可以选择不同的激活函数。</strong></p>
<h2 id="使用非线性激活函数的原因"><a href="#使用非线性激活函数的原因" class="headerlink" title="使用非线性激活函数的原因"></a>使用非线性激活函数的原因</h2><p>若使用线性激活函数，无论神经网络有多少层，输出都是输入的线性组合，与<strong>没有隐藏层</strong>效果相当，就成了最原始的感知机了。</p>
<h2 id="激活函数的导数"><a href="#激活函数的导数" class="headerlink" title="激活函数的导数"></a>激活函数的导数</h2><p>在神经网络使用反向传播时，需要计算激活函数的梯度，这里直接给出之前介绍的激活函数的梯度（导数）。</p>
<ul>
<li><code>sigmoid</code>函数:$$a=\dfrac{1}{1+e^{-z}}$$其导数为\(a(1-a)\)。</li>
<li><code>tanh</code>函数（双曲正切函数）:$$a=\dfrac{e^z-e^{-z}}{e^z+e^{-z}}$$其导数为\(1-a^2\)。</li>
<li><code>ReLu</code>函数(the rectified linear unit，修正线性单元):$$a=max(0,z)$$其导数为\(a’=\begin{cases}0,z&gt;0\\ 1,z&lt;0\\ undefine,z=0 \end{cases}\)。</li>
</ul>
<h2 id="神经网络梯度下降法"><a href="#神经网络梯度下降法" class="headerlink" title="神经网络梯度下降法"></a>神经网络梯度下降法</h2><p>前向传播（计算输出值）:<img src="/images/deeplearning_ai_021.png" alt="前向传播"><br>反向传播（计算梯度）:反向梯度下降公式（左）和代码向量化（右）<img src="/images/deeplearning_ai_022.png" alt="前向传播"></p>
<h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>如果在初始时将两个隐藏神经元的参数设置为相同的大小，那么两个隐藏神经元对输出单元的影响也是相同的，通过反向梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单位仍然是对称的。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。<br>初始化时候，参数\(W\)要进行随机初始化，不可设置为\(0\)。参数\(b\) 因为不存在对称性的问题，可以设置为\(0\)。<br>以\(2\)个输入，\(2\)个隐藏神经元为例，<code>python</code>代码如下:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = numpy.random.rand(<span class="number">2</span>,<span class="number">2</span>)*<span class="number">0.01</span></span><br><span class="line">b = numpy.zeros((<span class="number">2</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p>
<p><strong>注意</strong>：这里将\(W\)的值乘以\(0.01\)（或者其他的常数值）的原因是为了使得权重\(W\)初始化为较小的值，这是因为使用<code>sigmoid</code>函数或者<code>tanh</code>函数作为激活函数时，\(W\)比较小，则\(Z=WX+b\) 所得的值趋近于\(0\)，梯度较大，能够提高算法的更新速度。而如果\(W\)设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。<code>ReLU</code>和<code>Leaky ReLU</code>作为激活函数时不存在这种问题，因为在大于\(0\)的时候，梯度均为\(1\)。</p>
<h2 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h2><ol>
<li><code>python</code>中<code>numpy.sum()</code>函数有一个<code>keepdims</code>关键字很有用。</li>
<li>编程中要核对网络各层的维数；</li>
<li>遇到新的问题要从单层（例如<code>Logistic</code>回归）到多层（也就是从简单到复杂）。</li>
</ol>
<h1 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h1><h2 id="深层神经网络符号约定"><a href="#深层神经网络符号约定" class="headerlink" title="深层神经网络符号约定"></a>深层神经网络符号约定</h2><p><img src="/images/deeplearning_ai_023.png" alt="深层神经网络">\(L\)表示层数: 上图中\(L=4\)，输入层的索引为\(0\)；<br>第一个隐藏层\(n^{[1]}=5\)表示有\(5\)个隐藏神经元，同理\(n^{[2]}=5\)，\(n^{[3]}=3\)，\(n^{[4]}=1\)。输入层\(n^{[0]}=n_x=3\)；<br>\(a^{[l]}\)表示第\(l\)层激活后结果；<br>\(W^{[l]}\)表示第\(l\)层计算\(Z^{[l]}\)的权重，类似\(Z^{[l]}\)中的\(b^{[l]}\)也一样。<br>输入特征为\(x\)，约定\(x=a^{[0]}\)。</p>
<h2 id="前向传播和反向传播"><a href="#前向传播和反向传播" class="headerlink" title="前向传播和反向传播"></a>前向传播和反向传播</h2><p><strong>前向传播:</strong><br>输入：\(a^{[l-1]}\)<br>输出：\(a^{[l]}\)，cache(\(z^{[l]})\)，cache(\(w^{[l]})\)，cache(\(b^{[l]})\)<br>公式：$$z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}$$$$a^{[l]}=g^{[l]}(z^{[l]})$$向量化后的公式$$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$$$$A^{[l]}=g^{[l]}(Z^{[l]})$$<br><strong>反向传播</strong><br>输入：\(da^{[l]}\)<br>输出：\(da^{[l-1]}\)，\(dW^{[l]}\)，\(db^{[l]}\)<br>公式：<img src="/images/deeplearning_ai_024.png" alt="深层神经网络反向公式"></p>
<h2 id="搭建深层神经网络块"><a href="#搭建深层神经网络块" class="headerlink" title="搭建深层神经网络块"></a>搭建深层神经网络块</h2><p><img src="/images/deeplearning_ai_025.png" alt="深层神经网络反向公式">神经网络的一步训练（一个梯度下降循环），包含了从\(a^{[0]}\)（即\(x\)）经过一系列正向传播计算得到\(\widehat y\)（即\(a^{[l]}\)）。然后计算\(da^{[l]}\)，开始实现反向传播，得到所有的导数项，\(W\)和\(b\)也会在每一层被更新。<br><strong>在代码实现时，可以将正向传播过程中计算出来的\(z\)值缓存下来，待到反向传播计算时使用。</strong></p>
<h2 id="网络矩阵维数"><a href="#网络矩阵维数" class="headerlink" title="网络矩阵维数"></a>网络矩阵维数</h2><p><img src="/images/deeplearning_ai_026.png" alt="矩阵维数">在向量化之前有$$z^{[l]},a^{[l]}:(n^{[l]},1)$$在向量化后有$$Z^{[l]},A^{[l]}:(n^{[l]},m)$$计算反向传播时，\(dZ\)、\(dA\)与\(Z\)和\(A\)维数一样。</p>
<h2 id="使用深层神经网络原因"><a href="#使用深层神经网络原因" class="headerlink" title="使用深层神经网络原因"></a>使用深层神经网络原因</h2><p>以人脸识别为例子<img src="/images/deeplearning_ai_027.png" alt="人脸识别">对于人脸识别，神经网络的第一层从原始图片中提取人脸的轮廓和边缘，每个神经元学习到不同边缘的信息；网络的第二层将第一层学得的边缘信息组合起来，形成人脸的一些局部的特征，例如眼睛、嘴巴等；后面的几层逐步将上一层的特征组合起来，形成人脸的模样。随着神经网络层数的增加，特征也从原来的边缘逐步扩展为人脸的整体，由整体到局部，由简单到复杂。层数越多，那么模型学习的效果也就越精确。<br><strong>随着神经网络的深度加深，模型能学习到更加复杂的问题，功能也更加强大。</strong></p>
<h2 id="参数VS超参数"><a href="#参数VS超参数" class="headerlink" title="参数VS超参数"></a>参数VS超参数</h2><p><strong>参数</strong>即是我们在过程中想要模型学习到的信息（<strong>模型自己能计算出来的</strong>），例如\(W^{[l]}\)和\(b^{[l]}\)等。而<strong>超参数（hyper parameters）</strong>即为控制参数的输出值的一些网络信息（<strong>需要人经验判断</strong>）。超参数的改变会导致最终得到的参数\(W^{[l]}\)和\(b^{[l]}\)等的改变。<br><strong>典型的超参数有：</strong></p>
<ul>
<li>学习速率: \(\alpha\)</li>
<li>迭代次数: \(N\)</li>
<li>隐藏层层数: \(L\)</li>
<li>每一层神经元个数: \(n^{[l]}\)</li>
<li>激活函数\(g(z)\)选择</li>
</ul>
<p>开发新应用时，预先很难准确知道超参数的最优值应该是什么。因此，通常需要尝试很多不同的值。应用深度学习领域是一个很大程度基于经验的过程。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/浅谈机器学习中应用的变分法/" rel="next" title="浅谈机器学习中应用的变分法">
                <i class="fa fa-chevron-left"></i> 浅谈机器学习中应用的变分法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/02-改善深层神经网络/" rel="prev" title="02.改善深层神经网络">
                02.改善深层神经网络 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/wukong.png"
                alt="穆义" />
            
              <p class="site-author-name" itemprop="name">穆义</p>
              <p class="site-description motion-element" itemprop="description">既已无岸，不必回头，唯有向前，踏碎云霄，放肆桀骜</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#深度学习引言"><span class="nav-number">1.</span> <span class="nav-text">深度学习引言</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是神经网络"><span class="nav-number">1.1.</span> <span class="nav-text">什么是神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#结构化数据和非结构化数据"><span class="nav-number">1.2.</span> <span class="nav-text">结构化数据和非结构化数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#深度学习兴起的原因"><span class="nav-number">1.3.</span> <span class="nav-text">深度学习兴起的原因</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络编程基础"><span class="nav-number">2.</span> <span class="nav-text">神经网络编程基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#logistic回归"><span class="nav-number">2.1.</span> <span class="nav-text">logistic回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代价函数-Cost-Function"><span class="nav-number">2.2.</span> <span class="nav-text">代价函数(Cost Function)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降法"><span class="nav-number">2.3.</span> <span class="nav-text">梯度下降法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#计算图"><span class="nav-number">2.4.</span> <span class="nav-text">计算图</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic-回归的梯度下降法"><span class="nav-number">2.5.</span> <span class="nav-text">Logistic 回归的梯度下降法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#向量化"><span class="nav-number">2.6.</span> <span class="nav-text">向量化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Numpy-中广播-broadcasting"><span class="nav-number">2.7.</span> <span class="nav-text">Numpy 中广播(broadcasting)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#编程建议"><span class="nav-number">2.8.</span> <span class="nav-text">编程建议</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#浅层神经网络"><span class="nav-number">3.</span> <span class="nav-text">浅层神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络表示"><span class="nav-number">3.1.</span> <span class="nav-text">神经网络表示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#计算神经网络的输出（前向传播）"><span class="nav-number">3.2.</span> <span class="nav-text">计算神经网络的输出（前向传播）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#激活函数"><span class="nav-number">3.3.</span> <span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用非线性激活函数的原因"><span class="nav-number">3.4.</span> <span class="nav-text">使用非线性激活函数的原因</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#激活函数的导数"><span class="nav-number">3.5.</span> <span class="nav-text">激活函数的导数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络梯度下降法"><span class="nav-number">3.6.</span> <span class="nav-text">神经网络梯度下降法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#随机初始化"><span class="nav-number">3.7.</span> <span class="nav-text">随机初始化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#建议"><span class="nav-number">3.8.</span> <span class="nav-text">建议</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#深层神经网络"><span class="nav-number">4.</span> <span class="nav-text">深层神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#深层神经网络符号约定"><span class="nav-number">4.1.</span> <span class="nav-text">深层神经网络符号约定</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向传播和反向传播"><span class="nav-number">4.2.</span> <span class="nav-text">前向传播和反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#搭建深层神经网络块"><span class="nav-number">4.3.</span> <span class="nav-text">搭建深层神经网络块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#网络矩阵维数"><span class="nav-number">4.4.</span> <span class="nav-text">网络矩阵维数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用深层神经网络原因"><span class="nav-number">4.5.</span> <span class="nav-text">使用深层神经网络原因</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参数VS超参数"><span class="nav-number">4.6.</span> <span class="nav-text">参数VS超参数</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">穆义</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  

  

  

</body>
</html>
