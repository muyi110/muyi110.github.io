<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Times New Roman:300,300italic,400,400italic,700,700italic|18:300,300italic,400,400italic,700,700italic|Courier New:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hello, World" />










<meta name="description" content="循环序列模型序列模型的应用下图是序列模型的部分应用场景：在深度学习中，序列模型的代表是循环神经网络（ Recurrent NeuralNetwork, RNN ） 数学符号对于某一个序列数据 \(x\) （输入时间序列数据）相关符号约定如下：  \(x ^{\langle t\rangle}\) ：表示这个数据中的第 \(t\) 的元素； \(y ^{\langle t\rangle}\)">
<meta property="og:type" content="article">
<meta property="og:title" content="05.序列模型">
<meta property="og:url" content="https://muyi110.github.io/2018/05-序列模型/index.html">
<meta property="og:site_name" content="MuYi&#39;s Blog">
<meta property="og:description" content="循环序列模型序列模型的应用下图是序列模型的部分应用场景：在深度学习中，序列模型的代表是循环神经网络（ Recurrent NeuralNetwork, RNN ） 数学符号对于某一个序列数据 \(x\) （输入时间序列数据）相关符号约定如下：  \(x ^{\langle t\rangle}\) ：表示这个数据中的第 \(t\) 的元素； \(y ^{\langle t\rangle}\)">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_100.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_101.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_102.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_103.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_104.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_105.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_106.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_107.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_108.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_109.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_110.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_111.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_112.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_113.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_114.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_115.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_116.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_117.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_118.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_119.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_120.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_121.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_122.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_123.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_124.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_125.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_126.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_127.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_128.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_130.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_129.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_131.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_132.png">
<meta property="og:updated_time" content="2018-08-22T03:40:34.412Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="05.序列模型">
<meta name="twitter:description" content="循环序列模型序列模型的应用下图是序列模型的部分应用场景：在深度学习中，序列模型的代表是循环神经网络（ Recurrent NeuralNetwork, RNN ） 数学符号对于某一个序列数据 \(x\) （输入时间序列数据）相关符号约定如下：  \(x ^{\langle t\rangle}\) ：表示这个数据中的第 \(t\) 的元素； \(y ^{\langle t\rangle}\)">
<meta name="twitter:image" content="https://muyi110.github.io/images/deeplearning_ai_100.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://muyi110.github.io/2018/05-序列模型/"/>





  <title>05.序列模型 | MuYi's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">MuYi's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">此博客创建于2018-07-10</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-reprinted_article">
          <a href="/reprinted-article/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heart"></i> <br />
            
            转载文章
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://muyi110.github.io/2018/05-序列模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="穆义">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/wukong.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MuYi's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">05.序列模型</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-19T20:34:37+08:00">
                2018-08-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习课程-Andrew-Ng-学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习课程(Andrew Ng)学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h1 id="循环序列模型"><a href="#循环序列模型" class="headerlink" title="循环序列模型"></a>循环序列模型</h1><h2 id="序列模型的应用"><a href="#序列模型的应用" class="headerlink" title="序列模型的应用"></a>序列模型的应用</h2><p>下图是<strong>序列模型</strong>的部分应用场景：<img src="/images/deeplearning_ai_100.png" alt="序列模型应用场景">在深度学习中，序列模型的代表是<strong>循环神经网络</strong>（ <code>Recurrent NeuralNetwork, RNN</code> ）</p>
<h2 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h2><p>对于某一个序列数据 \(x\) （输入时间序列数据）相关符号约定如下：</p>
<ul>
<li>\(x ^{\langle t\rangle}\) ：表示这个数据中的第 \(t\) 的元素；</li>
<li>\(y ^{\langle t\rangle}\) ：表示这个数据中的对应的第 \(t\) 的标签；</li>
<li>\(T _x\) ：输入的数据长度；</li>
<li>\(T _y\) ：输出的数据长度；</li>
<li>\(x ^{\left (i\right )\langle t\rangle}\)：第 \(i\) 个输入序列数据（第 \(i\) 个样本）的第 \(t\) 个元素；</li>
<li>\(y ^{\left (i\right )\langle t\rangle}\)：第 \(i\) 个输出（第 \(i\) 个样本对应的输出）的第 \(t\) 个标签；</li>
<li>\(T _x ^{(i)}\) ：第 \(i\) 个输入序列数据（第 \(i\) 个样本）的长度；</li>
<li>\(T _y ^{(i)}\) ：第 \(i\) 个输出（第 \(i\) 个样本）的长度。</li>
</ul>
<p>例如输入是一段话， \(x ^{\langle t\rangle}\) 的表示方式为：首先建立一个<strong>词汇表</strong>（或者称为<strong>字典</strong>），根据要表示单词在词汇表中的位置，采用 <code>one-hot</code> 编码表示 \(x ^{\langle t\rangle}\) 对应的单词。例如单词 <code>zulu</code> 在词汇表的最后一位，则 \(x ^{\langle T _x\rangle}\) 表示为：$$\begin {bmatrix} 0\\0\\\vdots\\1\end {bmatrix}$$</p>
<ul>
<li>杂谈： <code>one-hot</code> 编码的缺点：<br>每个单词被表示为完全独立的个体， 因此单词间的相似度无法体现。例如单词 <code>hotel</code> 和 <code>motel</code> 意思相近，但与 <code>cat</code> 不同，但：$$(x ^{\langle \text {hotel}\rangle}) ^T x ^{\langle \text {motel}\rangle} = (x ^{\langle \text {hotel}\rangle}) ^T x ^{\langle \text {cat}\rangle} = 0$$</li>
</ul>
<h2 id="循环神经网络模型"><a href="#循环神经网络模型" class="headerlink" title="循环神经网络模型"></a>循环神经网络模型</h2><p>对于序列数据，如果使用标准的神经网络则存在如下问题：</p>
<ul>
<li>对于不同的例子，输入和输出可能有不同的长度，因此输入层和输出层的神经元个数无法确定；</li>
<li>从文本的不同位置学习到的特征无法共享；</li>
<li>模型参数多，计算量大。</li>
</ul>
<p><strong>循环神经网络</strong>（ <code>Recurrent Neural Network, RNN</code> ）则没有上述问题。基本的 <code>RNN</code> 网络结构如下图所示：<img src="/images/deeplearning_ai_101.png" alt="基本RNN结构">当元素 \(x ^{\langle t\rangle}\) <strong>输入相对应的时间步的隐藏层的同时</strong>，该隐藏层同时接收上一时间步的隐藏层的激活值 \(a ^{\langle t-1\rangle}\) 。图中的 \(a ^ {\langle 0\rangle}\) 一般初始化为零向量，每个时间步对应输出一个预测值 \(\widehat y ^{\langle t\rangle}\) 。<br><code>RNN</code> 网络从左到右扫描输入数据，<strong>每个时间步对应参数共享</strong>。输入、激活、输出对应的参数分别为：\(W _{ax}\)、\(W _{aa}\)、\(W _{ya}\)（<strong>注：图中是 \(W _{ay}\) 最好改过来和视频一致</strong>）。<br>下图是一个基本的 <code>RNN</code> 网络的单元：<img src="/images/deeplearning_ai_102.png" alt="基本RNN单元">前向传播的计算公式为：$$a ^{\langle t\rangle}=g _1 (W _{aa}a ^{\langle t-1\rangle}+W _{ax}x ^{\langle t\rangle}+b _a)$$$$\widehat y ^{\langle t\rangle}= g _2 (W _{ya}a ^{\langle t\rangle}+b _y)$$$$a ^{\langle 0\rangle}=\overrightarrow 0$$激活函数 \(g _1\) 一般选择 <code>tanh</code> 函数，偶尔也选择 <code>Relu</code> 函数。激活函数 \(g _2\) 选择 <code>Sigmoid</code> 或者 <code>Softmax</code> 函数，取决于具体的应用（多分类还是二分类）。<br>将上述的前向传播公式符号简化如下：$$W _a =[W _{ax}, W _{aa}]$$$$a ^{\langle t\rangle} = g _1 (W _a [a ^{\langle t-1\rangle}, x ^{\langle t\rangle}]+ b _a)$$$$\widehat y ^{\langle t\rangle} = g _2 (W _y a ^{\langle t\rangle}+ b _y)$$将 \(W _{aa}\) 和 \(W _{ax}\) <strong>水平并列</strong>为一个矩阵 \(W _a\) 将 \(a ^{\langle t-1\rangle}\) 和 \(x ^{\langle t\rangle}\) <strong>堆叠</strong>为一个矩阵。<br>下图是 <code>RNN</code> 网络的前向传播示意图：<img src="/images/deeplearning_ai_103.png" alt="RNN前向传播"></p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>为了计算反向传播，需要定义一个损失函数。一个元素的损失函数（一个时间步）定义为<strong>交叉损失函数</strong>，如下：$$L ^{\langle t\rangle}(\widehat y ^{\langle t\rangle}, y ^{\langle t\rangle}) = -y ^{\langle t\rangle}\log \widehat y ^{\langle t\rangle} - (1- y ^{\langle t\rangle})\log (1- \widehat y ^{\langle t\rangle})$$进而得到整个序列的损失函数如下：$$J = \sum _{t=1} ^{T _x}L ^{\langle t\rangle}(\widehat y ^{\langle t\rangle}, y ^{\langle t\rangle})$$基本<code>RNN</code> 网络的反向传播公式如下图所示：<img src="/images/deeplearning_ai_104.png" alt="RNN反向传播"></p>
<h2 id="不同类型的循环神经网络"><a href="#不同类型的循环神经网络" class="headerlink" title="不同类型的循环神经网络"></a>不同类型的循环神经网络</h2><p>某些应用场景中，输入和输出的长度可能不一样，根据输入和输出的序列长度，可以将循环神经网络划分为如下几种结构：<img src="/images/deeplearning_ai_105.png" alt="RNN的几种结构">目前介绍的模型只是使用序列之前的信息来预测当前的值，没有使用后文的信息。可以引入<strong>双向循环神经网络</strong>（ <code>Bidirectional RNN, BRNN</code> ）解决。</p>
<h2 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h2><p><strong>语言模型会告诉特点句子出现的概率是多少</strong>，并以此为依据作出准确判断。建立语言模型所采用的训练集是一个大型的<strong>语料库</strong>，指众多句子组成的文本。第一步需要将输入的某个数据 \(x\) <strong>标记化</strong>，即建立字典，采用 <code>one-hot</code> 编码。<br>将标记化后的训练集用于训练 <code>RNN</code> 网络，如下图所示：<img src="/images/deeplearning_ai_106.png" alt="RNN训练">在第一个时间步中，输入 \(a ^{\langle 0\rangle}\) 和 \(x ^{\langle 1\rangle}\) 其都是零向量，\(\widehat y ^{\langle 1\rangle}\)是通过 <code>Softmax</code> 预测出的字典中每个单词作为该句子第一个单词出现的概率；<br>在第二的时间步中，输入 \(x ^{\langle 2\rangle}\) 和 上一层的激活值 \(a ^{\langle 1\rangle}\) ，其中 \(x ^{\langle 2\rangle}\) 是训练样本标签中的第一个单词 \(y ^{\langle 1\rangle}\)（即单词 <code>cats</code> ），输出的 \(y ^{\langle 2\rangle}\) 是通过 <code>Softmax</code> 函数预测的以单词 <code>cats</code> 为条件出现字典中其他每个词的条件概率，以此类推得到整个句子出现的概率。<br>训练网络定义的损失函数为：$$L(\widehat y ^{\langle t\rangle}, y ^{\langle t\rangle}) = -\sum _{i}y _i ^{\langle t\rangle}\log \widehat y _i ^{\langle t\rangle}$$总体损失函数为：$$L = \sum _{t}L ^{\langle t\rangle}(\widehat y ^{\langle t\rangle}, y ^{\langle t\rangle})$$将训练好的网络用于新的句子，例如一个新句子只有三个单词 \(y ^{\langle 1\rangle}\) 、\(y ^{\langle 2\rangle}\) 、\(y ^{\langle 3\rangle}\) 如要按公式：$$P(y ^{\langle 1\rangle}, y ^{\langle 2\rangle}, y ^{\langle 3\rangle})= P(y ^{\langle 1\rangle})P(y ^{\langle 2\rangle}|y ^{\langle 1\rangle})P(y ^{\langle 3\rangle}|y ^{\langle 2\rangle}, y ^{\langle 1\rangle})$$得到整个句子的概率。</p>
<h2 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h2><p>当训练好一个语言模型后，可以通过非正式方法<strong>采样</strong>新的序列来了解模型学到的什么。<br><img src="/images/deeplearning_ai_107.png" alt="采样">在第一个时间步中，输入 \(a ^{\langle 0\rangle}\) 和 \(x ^{\langle 1\rangle }\) 其都是零向量，输出预测出字典中每个词作为第一个词的概率，根据 <code>Softmax</code> 分布进行随机采样（ <code>np.random.choice</code> ），将采样得到的 \(\widehat y ^{\langle 1\rangle}\) 作为第二个时间步的输入 \(x ^{\langle 2\rangle }\) 。以此类推，直到采样到句子结尾标志（例如 <code>EOS</code> 等）或采样达到特定的时间步长，最后该模型可以生成一些句子。<br>实际中也可以考虑建立基于字符的语言模型（上述是基于词汇的语言模型），基于字符的语言模型得到的序列过长，计算成本高。</p>
<h2 id="RNN-的梯度消失"><a href="#RNN-的梯度消失" class="headerlink" title="RNN 的梯度消失"></a>RNN 的梯度消失</h2><p>$$\text {The cat, which already ate a bunch of food, was full.}$$$$\text {The cats, which already ate a bunch of food, were full.}$$对于上面的两句话，后面谓语动词单复数形式由前面主语名词的单复数形式决定。但<strong>基本的 <code>RNN</code>网络不擅长获取这种长期依赖关系</strong>。其原因主要是由于梯度消失，在反向传播时，后面层的输出误差很难影响前面层的计算，网络很难调整前面层的计算。反向传播如下图所示：<img src="/images/deeplearning_ai_108.png" alt="反向传播">在反向传播过程中，也有可能出现梯度爆炸问题（较容易发现，参数会快速膨胀到数值溢出，可能显示 <code>NaN</code> 。可以采用<strong>梯度修剪</strong>解决。观察梯度值，如果大于某个阈值，则缩放梯度向量保证其不会太大。相比之下，梯度消失问题更难解决，目前<strong>GRU 和 LSTM 可以作为缓解梯度消失问题的解决方案</strong>。</p>
<h2 id="GRU（门控循环单元）"><a href="#GRU（门控循环单元）" class="headerlink" title="GRU（门控循环单元）"></a>GRU（门控循环单元）</h2><p>对于句子$$\text {The cat, which already ate a bunch of food, was full.}$$当从左到右读的时候，<code>GRU</code> 有一个新的变量称为 <code>c</code> ，表示<strong>记忆细胞</strong>（<code>Memory Cell</code> ），其提供记忆的能力，例如记住前面的主语是单数函数复数。在时间 <code>t</code> 记忆细胞的值 \(c ^{\langle t\rangle}\) 等于激活值 \(a ^{\langle t\rangle}\) 。\(\widehat c ^{\langle t\rangle}\) 表示下一个 \(c\) 的候选值。\(\Gamma _u\) 表示<strong>更新门</strong>，用于决定什么时候更新记忆细胞的值。具体公式如下：$$\widehat c ^{\langle t\rangle}= \tanh (W _c [c ^{\langle t-1\rangle}, x ^{\langle t\rangle}] +b _c )$$$$\Gamma _u = \sigma (W _u [c ^{\langle t-1\rangle}, x ^{\langle t\rangle}] + b _u )$$$$c ^{\langle t\rangle} = \Gamma _u \times \widehat c ^{\langle t\rangle} + (1-\Gamma _u)\times c ^{\langle t-1\rangle}$$$$a ^{\langle t\rangle}= c ^{\langle t\rangle}$$上述公式是简化过的 <code>GRU</code> 单元，实际中完整的 <code>GRU</code> 单元还有一个新的<strong>相关门</strong> \(\Gamma _r\) ，表示 \(\widehat c ^{\langle t\rangle}\) 和 \(c ^{\langle t-1\rangle}\) 间的相关性，完整的公式如下：$$\widehat c ^{\langle t\rangle}= \tanh (W _c [\Gamma _r c ^{\langle t-1\rangle}, x ^{\langle t\rangle}] +b _c )$$$$\Gamma _u = \sigma (W _u [c ^{\langle t-1\rangle}, x ^{\langle t\rangle}] + b _u )$$$$\Gamma _r = \sigma (W _r [c ^{\langle t-1\rangle}, x ^{\langle t\rangle}] + b _r )$$$$c ^{\langle t\rangle} = \Gamma _u \times \widehat c ^{\langle t\rangle} + (1-\Gamma _u)\times c ^{\langle t-1\rangle}$$$$a ^{\langle t\rangle}= c ^{\langle t\rangle}$$为了便于理解，给出下图（下图中符号和上面公式表示不同）：<img src="/images/deeplearning_ai_109.png" alt="GRU结构"><br>相关博客：<br><a href="https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be" target="_blank" rel="noopener">Understanding GRU networks</a><br>相关论文：<br><a href="https://arxiv.org/pdf/1409.1259.pdf" target="_blank" rel="noopener">Cho et al., 2014. On the properties of neural machine translation: Encoder-decoder approaches</a><br><a href="https://arxiv.org/pdf/1412.3555.pdf" target="_blank" rel="noopener">Chung et al., 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a></p>
<h2 id="LSTM-Long-Short-Term-Memory"><a href="#LSTM-Long-Short-Term-Memory" class="headerlink" title="LSTM (Long Short Term Memory)"></a>LSTM (Long Short Term Memory)</h2><p><code>LSTM</code> 比 <code>GRU</code> 更加灵活，其引入了<strong>遗忘门</strong> \(\Gamma _f\) 和<strong>输出门</strong> \(\Gamma _o\) ，其一个单元结构如下图所示：<img src="/images/deeplearning_ai_110.png" alt="LSTM单元结构"><code>LSTM</code> 网络如下图所示：<img src="/images/deeplearning_ai_111.png" alt="LSTM网络">实际使用中，几个门值不仅取决于 \(a ^{\langle t-1\rangle}\) 和 \(x ^{\langle t\rangle}\) ，有时候可以偷窥上个记忆细胞的输入值 \(c ^{\langle t-1\rangle}\) 此被称为<strong>窥视连接</strong>。<br>\(c ^{\langle 0\rangle}\) 通常初始化为零向量。<br>相关博客：<br><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></p>
<h2 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h2><p>单向的循环神经网络在某一时刻的预测结果只能使用之前输入的序列信息。双向循环神经网络（<code>Bidirectional RNN，BRNN</code> ）可以在序列的任意位置使用之前和之后的数据。其工作原理是增加一个反向循环层，如下图所示：<img src="/images/deeplearning_ai_112.png" alt="BRNN结构">根据上图有：$$y ^{\langle t\rangle} = g(W _y [\overrightarrow a ^{\langle t\rangle}, \overleftarrow a ^{\langle t\rangle}]+ b_y )$$其中基本单元不仅可以是标准的 <code>RNN</code> 模块，也可以是 <code>LSTM</code> 或 <code>GRU</code> 模块。缺点是需要完整的序列数据，才能预测任意位置的结果。例如构建语音识别系统，需要等待用户说完并获取整个语音表达，才能处理这段语音并进一步做语音识别。因此，实际应用会有更加复杂的模块。</p>
<h2 id="深度循环神经网络-DRNN"><a href="#深度循环神经网络-DRNN" class="headerlink" title="深度循环神经网络( DRNN )"></a>深度循环神经网络( DRNN )</h2><p>深度循环神经网络（<code>Deep RNN</code>)的结构如下：<img src="/images/deeplearning_ai_113.png" alt="DRNN结构">以 \(a ^{[2]\langle 3\rangle}\) 为例，有：$$a ^{[2]\langle 3\rangle} = g(W _a ^{[2]}[a ^{[2]\langle 2\rangle}, a ^{[1]\langle 3\rangle}]+ b _a ^{[2]})$$</p>
<h1 id="自然语言处理和词嵌入"><a href="#自然语言处理和词嵌入" class="headerlink" title="自然语言处理和词嵌入"></a>自然语言处理和词嵌入</h1><h2 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h2><p><code>one-hot</code> 表示法将每个单词都表示为完全独立的个体，不同词的表示向量都是<strong>正交</strong>的，不同单词间的相似性无法体现（上面的杂谈中有提到这点）。<br>采用特征化表示法（词嵌入）可以避免提到的问题。可以通过用语义特征作为维度来表示一个词，因此语义相近的词，其词向量也相近。<br><strong>词嵌入</strong>（ <code>Word Embedding</code> ）是指把一个维数为字典中所有词的高维空间（<code>one-hot</code> 表示法）嵌入到一个维数低的连续向量空间中，每个单词被映射为实数域上的向量。对大量词汇进行词嵌入后获得的词向量，可用于 <code>NLP</code> 相关的任务。<br>将高维的词嵌入到一个二维空间中，可以进行可视化，常用的可视化算法是 <code>t-SNE</code> 算法。通过复杂的非线性的方法映射到二维空间后，每个词会根据语义和相关程度聚在一起。<br>相关论文<a href="https://www.seas.harvard.edu/courses/cs281/papers/tsne.pdf" target="_blank" rel="noopener">van der Maaten and Hinton., 2008. Visualizing Data using t-SNE</a></p>
<h2 id="词嵌入和迁移学习"><a href="#词嵌入和迁移学习" class="headerlink" title="词嵌入和迁移学习"></a>词嵌入和迁移学习</h2><p>对于小样本的学习任务，迁移学习可以达到较好的效果，步骤如下：</p>
<ol>
<li>Learn word embedding from large text corpus.(Or download pre-trained embedding online.)</li>
<li>Transfer embedding to new task with smaller training set.</li>
<li>Optional: Continue to fine-tune the word embedding with new data.</li>
</ol>
<p>注：词嵌入和之前介绍的用于人脸验证的 <code>siamese</code> 网络相似， <code>siamese</code> 网络是将输入的人脸图像通过卷积神经网络编码为特征向量，而词嵌入也是将词嵌入到某个维度相对低的空间中。</p>
<h2 id="词嵌入和类比推理"><a href="#词嵌入和类比推理" class="headerlink" title="词嵌入和类比推理"></a>词嵌入和类比推理</h2><p>词嵌入可以用于类比推理。例如给定对应关系$$\text {男性 man}\rightarrow \text {女性 woman}$$$$ \text {then}$$$$ \text {国王 king}\rightarrow ?$$根据词嵌入，则有$$e _{\text {man}}- e _{\text {woman}}\approx e _{\text {king}}-e _{?}$$因此需要找到词向量 \(w\) 使得相似度 \(\text {sim}(e _w , e _{\text {king}}-e _{\text {man}}+e _{\text {woman}})\) 最大。常用的计算相似度函数是<strong>余弦相似度</strong>：$$\text {sim}(u, v)=\dfrac {u ^T v}{||u|| _2 ||v|| _2}$$相关论文：<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf" target="_blank" rel="noopener">Mikolov et. al., 2013, Linguistic regularities in continuous space word representations</a></p>
<h2 id="嵌入矩阵"><a href="#嵌入矩阵" class="headerlink" title="嵌入矩阵"></a>嵌入矩阵</h2><p>我们的目标是需要学习一个<strong>词嵌入矩阵 E</strong> ，矩阵中的行代表用于表示每个词的特征向量，矩阵中的列表示字典中的每个词，如下图所示：<img src="/images/deeplearning_ai_114.png" alt="词嵌入矩阵">将字典中位置为 \(i\) 的词对应的 <code>one-hot</code> 表示记为 \(o _i\) ，词嵌入后生成词向量用 \(e _i\) 表示，有：$$E\cdot o _i = e _i$$由于 \(o _i\) 是很高维的，上式效率比较低，实际中一般不用上式找 \(e _i\)，深度学习框架中有专门的函数查找矩阵 \(E\) 的特定列。</p>
<h2 id="学习词嵌入"><a href="#学习词嵌入" class="headerlink" title="学习词嵌入"></a>学习词嵌入</h2><p>利用神经网络构建语言模型也可以学习词嵌入，神经网络语言模型如下：<img src="/images/deeplearning_ai_115.png" alt="神经网络语言模型">训练过程中，将语料库中的某些词作为目标词，以目标词的部分上下文作为输入，<code>Softmax</code> 输出的预测结果为目标词。嵌入矩阵 <code>E</code> 和 <code>w</code> 、<code>b</code> 为需要通过训练得到的参数。这样，在得到嵌入矩阵后，就可以得到词嵌入后生成的词向量。<br>如果目标是建立语言模型，通常选择目标词的前几个词作为上下文，如果是学习词嵌入矩阵，则可以选择其他的上下文。<br>相关论文：<a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener">Bengio et. al., 2003, A neural probabilistic language model</a></p>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><blockquote>
<p>相比于神经语言模型学习词嵌入，<code>Word2Vec</code> 是一种简单而且更加高效的方式来学习词嵌入。</p>
</blockquote>
<p><code>Word2Vec</code> 有两种模型：</p>
<ul>
<li><code>Skip-gram</code> 模型：用一个词语作为输入，预测其周围的上下文；</li>
<li><code>CBOW</code> 模型：用一个词语的上下文作为输入，预测这个词语本身。</li>
</ul>
<p>每种模型都包含<strong>负采样</strong>和<strong>分级 Softmax</strong>两种训练技巧。<br><strong>下图中训练神经网络时候的隐藏层参数即是学习到的词嵌入。</strong><br><strong>Skip-gram</strong><img src="/images/deeplearning_ai_116.png" alt="skip-gram模型">上图中的输入层和输出层都采用 <code>One-hot</code> 编码，输出层是 <code>Softmax</code> 层，\(W\) 是要学习的词嵌入矩阵，\(W’\) 是输出层参数。<br>设某个词为 \(c\) ，在该词一定词距内选取一些目标上下文 \(t\) ，输出层是 <code>Softmax</code> 层，则对应的条件概率为：$$P(t|c)= \dfrac {\text {exp}(\theta _t ^T e _c)}{\sum _{j=1} ^{m}\text {exp}(\theta _j ^T e _c)}$$其中 \(\theta _t\) 是一个与输出 \(t\) 有关的参数（上式省略的偏差参数），损失函数可选择交叉熵：$$L(\widehat y , y)=-\sum _{i=1} ^{m}y _i\log \widehat y _i$$由于每次计算条件概率需要对词典中所有词求和，计算量大，因此解决方案之一是采用<strong>分级 Softmax</strong> ，实际中采用非平衡的二叉树形式，保证常用的此可以很快找到。<br><strong>CBOW</strong><img src="/images/deeplearning_ai_117.png" alt="CBOW模型">其工作方式与 <code>Skip-gram</code> 相反。<br>相关文章：<br><a href="https://zhuanlan.zhihu.com/p/26306795" target="_blank" rel="noopener">[NLP] 秒懂词向量Word2vec的本质</a><br><a href="http://www.hankcs.com/nlp/word2vec.html" target="_blank" rel="noopener">word2vec原理推导与代码分析-码农场</a><br><a href="https://github.com/stanfordnlp/cs224n-winter17-notes/blob/master/notes1.pdf" target="_blank" rel="noopener">课程 cs224n 的 notes1</a><br>相关论文：<br><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Mikolov et. al., 2013. Efficient estimation of word representations in vector space.</a><br><a href="https://arxiv.org/pdf/1411.2738.pdf" target="_blank" rel="noopener">word2vec Parameter Learning Explained</a></p>
<h2 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h2><p><strong>生成数据的方式</strong>：在给的一段话中选择一个上下文词，再选择一个目标词，组成一个正样本。然后给定 \(k\) 次，用相同的上下文词，再从字典中选择随机的词，组成负样本，若从字典中选择的词刚好出现在给的一段话的词距（在一定的词距内选择目标词）内也没关系。<img src="/images/deeplearning_ai_118.png" alt="负采样">对于小数据集，\(k\) 取 \(5-20\) 较合适，对于大数据集，\(k\) 取 \(2-5\)较合适。<br>问题为监督学习，输出改用 <code>Sigmoid</code> ，输入是上下文-目标词 （ \((c, t)\) ），对应的条件概率为：$$P(y=1|c, t)=\sigma (\theta _t ^T e _c)$$其中，\(\theta _t\) 和 \(e _c\) 参数和之前一样。<br>之前训练中每次要更新 \(n\) 维的多分类 <code>Softmax</code> 单元（ \(n\) 为词典中词的数量）。现在每次只需要更新 <code>k+1</code> 维的二分类 <code>Sigmoid</code> 单元，计算量大大降低。<br>关于计算选择某个词作为负样本的概率，推荐采用以下公式（而非经验频率或均匀分布）：$$p(w _i)=\dfrac {f(w _i) ^{\frac {3}{4}}}{\sum _{j=0} ^{m}f(w _j) ^{\frac {3}{4}}}$$其中，\(f(w _i)\) 表示语料中单词 \(w _i\) 出现的频率。<br>相关论文：<br><a href="https://arxiv.org/pdf/1310.4546.pdf" target="_blank" rel="noopener">Mikolov et. al., 2013. Distributed representation of words and phrases and their compositionality</a></p>
<h2 id="GloVe-词向量"><a href="#GloVe-词向量" class="headerlink" title="GloVe 词向量"></a>GloVe 词向量</h2><p><code>GloVe (global vectors for word representation)</code> 是另一种学习词嵌入的方法。其代价函数如下：$$J = \sum _{i=1} ^{N}\sum _{j=1} ^{N}f(X _{ij})(\theta _i ^T e _j + b _i + b _j - \log(X _{ij})) ^2$$其中 \(X _{ij}\) 表示单词 \(i\) 和单词 \(j\) 为 <code>上下文-目标词</code> 的次数。参数 \(b _i\) 、\(b _j\) 和 \(f()\) 函数用来避免 \(X _{ij}=0\) 时 \(\log (X _{ij})\) 为负无穷大情况，并在其他情况下调整权重。<br><code>上下文-目标词</code> 可以代表两个词出现在同一个窗口。在这种情况下，\(\theta _i\) 和 \(e _j\) 是完全对称的。因此，在训练时可以一致地初始化二者，使用梯度下降法处理完以后取平均值作为二者共同的值。<br>相关论文：<br><a href="https://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="noopener">Pennington st. al., 2014. Glove: Global Vectors for Word Representation</a></p>
<h2 id="情感分类"><a href="#情感分类" class="headerlink" title="情感分类"></a>情感分类</h2><blockquote>
<p>情感分类是指分析一段文本对某个对象的情感是正面的还是负面的。情感分类的问题之一是标记好的训练数据不足。但是有了词嵌入得到的词向量，中等规模的标记训练数据也能构建出一个效果不错的情感分类器。</p>
</blockquote>
<p><img src="/images/deeplearning_ai_119.png" alt="情感分类模型">上图中 \(E\) 表示词嵌入法获得的矩阵，计算出单词的词向量取平均，输入 <code>Softmax</code> 单元，得到预测结果。缺点是没有考虑词的顺序，对于包含多个正面评价词的负面评价，效果不好。<br>使用 <code>RNN</code> 网络可以得到一个有效的情感分类器：<img src="/images/deeplearning_ai_120.png" alt="RNN 情感分类器"></p>
<h2 id="词嵌入除偏"><a href="#词嵌入除偏" class="headerlink" title="词嵌入除偏"></a>词嵌入除偏</h2><p>此部分暂时先不写了，具体可参考视频类容。</p>
<h1 id="序列模型和注意力机制"><a href="#序列模型和注意力机制" class="headerlink" title="序列模型和注意力机制"></a>序列模型和注意力机制</h1><h2 id="Seq2Seq-模型"><a href="#Seq2Seq-模型" class="headerlink" title="Seq2Seq 模型"></a>Seq2Seq 模型</h2><p><code>Seq2Seq ( Sequence-to-Sequence )</code> 模型能够应用于机器翻译、语音识别等各种序列到序列的转换问题。<code>Seq2Seq</code> 模型包含编码器（ <code>Encoder</code>）和解码器（ <code>Decoder</code> ）两部分，通常是两个不同的 <code>RNN</code> 。如下图所示，将编码器的输出作为解码器的输入，由解码器负责输出正确的翻译结果。<img src="/images/deeplearning_ai_121.png" alt="序列到序列模型"><br>相关论文：</p>
<ul>
<li><a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="noopener">Sutskever et al., 2014. Sequence to sequence learning with neural networks</a></li>
<li><a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">Cho et al., 2014. Learning phrase representaions using RNN encoder-decoder for statistical machine translation</a></li>
</ul>
<p><strong>图像描述</strong>中也用到类似的结构。如下图所示：<img src="/images/deeplearning_ai_122.png" alt="图像描述网络">将 <code>AlexNet</code> 网络作为编码器，最后的 <code>Softmax</code> 层替换为 <code>RNN</code> 网络作为解码器，该网络的输出是对输入图像的描述（一段话）。<br>相关论文：</p>
<ul>
<li><a href="">Mao et. al., 2014. Deep captioning with multimodal recurrent neural networks</a></li>
<li><a href="">Vinyals et. al., 2014. Show and tell: Neural image caption generator</a></li>
<li><a href="">Karpathy and Fei Fei, 2015. Deep visual-semantic alignments for generating image descriptions</a></li>
</ul>
<h2 id="选择最可能的句子"><a href="#选择最可能的句子" class="headerlink" title="选择最可能的句子"></a>选择最可能的句子</h2><p><code>Seq2Seq</code> 机器翻译模型和之前介绍的语言模型有很多相似的地方，也有区别。主要区别就是用编码器的输出作为解码器的输入，而不是输入零向量。因此机器翻译过程相当于建立一个<strong>条件语言模型</strong>。<img src="/images/deeplearning_ai_123.png" alt="条件语言模型">我们不想让解码器的输出是随机的（结果可能有好有坏），需要一种算法找到使得条件概率最大化的翻译，即：$$\text {arg max} _{y ^{\langle 1\rangle}, \ldots , y ^{\langle t\rangle}}P(y ^{\langle 1\rangle}, \ldots , y ^{\langle t\rangle}|x)$$由于贪心算法得到的结果难以满足要求，实际中使用<strong>集束搜索</strong> ( <code>Beam Search</code> ) 。</p>
<h2 id="集束搜索"><a href="#集束搜索" class="headerlink" title="集束搜索"></a>集束搜索</h2><p><strong>集束搜索</strong>（ <code>Beam Search</code> ）会考虑每个时间步多个可能的选择。有一个参数<strong>集束宽</strong>（ <code>Beam Width</code> ）\(B\) ，表示解码器中每个时间步保留的预选单词数量。本例中设 \(B=3\) 。<br><img src="/images/deeplearning_ai_124.png" alt="集束搜索">第一步：将第一个时间步最可能的三个预选单词及其概率值 \(P(\widehat y ^{\langle 1\rangle}|x)\) 保存。<br>第二步：分别将第一步保存的三个预选词作为第二时间步的输入，得到 \(P(\widehat y ^{\langle 2\rangle}|x, \widehat y ^{\langle 1\rangle})\) 。由于实际需要第一个单词和第二个单词的联合概率最大（不是只有第二个单词条件概率最大），因此根据条件概率公式有：$$P(\widehat y ^{\langle 1\rangle}, \widehat y ^{\langle 2\rangle}|x)= P(\widehat y ^{\langle 1\rangle}|x)P(\widehat y ^{\langle 2\rangle}|x, \widehat y ^{\langle 1\rangle})$$设字典中有 \(N\) 个单词，当 \(B=3\) 时，有 \(3N\) 个 \(P(\widehat y ^{\langle 1\rangle}, \widehat y ^{\langle 2\rangle}|x)\) ，取概率最大的 \(3\) 个保存。以此类推，直到最后输出一个最优结果，满足下列公式：$$\text {arg max}\prod _{t=1} ^{T _y}P(\widehat y ^{\langle t\rangle}|x, \widehat y ^{\langle 1\rangle}, \ldots , \widehat y ^{\langle t-1\rangle})$$当设置 \(B=1\)时，集束搜索算法变为贪心算法。</p>
<h2 id="改进集束搜索"><a href="#改进集束搜索" class="headerlink" title="改进集束搜索"></a>改进集束搜索</h2><blockquote>
<p><strong>长度标准化</strong> ( <code>Length Normalization</code> ) 是对集束搜索算法的优化方式</p>
</blockquote>
<p>对于公式：$$\text {arg max}\prod _{t=1} ^{T _y}P(\widehat y ^{\langle t\rangle}|x, \widehat y ^{\langle 1\rangle}, \ldots , \widehat y ^{\langle t-1\rangle})$$当有多个小于 \(1\) 的概率值相乘时，可能会有<strong>数值下溢</strong>的问题，实际中会取 \(\log\) 将乘法变为加法。$$\text {arg max}\sum _{t=1} ^{T _y}\log P(\widehat y ^{\langle t\rangle}|x, \widehat y ^{\langle 1\rangle}, \ldots , \widehat y ^{\langle t-1\rangle})$$但上面两个式子存在问题，即当输出的句子比较长时，上式变得更小，因此上式趋向于更短的输出。因此需要归一化，即：$$\text {arg max}\dfrac {1}{T _y ^{\alpha}}\sum _{t=1} ^{T _y}\log P(\widehat y ^{\langle t\rangle}|x, \widehat y ^{\langle 1\rangle}, \ldots , \widehat y ^{\langle t-1\rangle})$$其中 \(T _y\) 表示输出结果单词数，\(\alpha\) 是一个需要调整的超参数（\(0-1\) 之间）。归一化用于减少对输出长的结果的惩罚（因为翻译结果一般没有长度限制）。<br>关于集束宽 \(B\) 的取值，较大的 \(B\) 值意味着可能更好的结果和巨大的计算成本；而较小的 \(B\) 值代表较小的计算成本和可能表现较差的结果。对于产品来说，\(B\) 可以取 \(10\) 。</p>
<h2 id="集束搜索的误差分析"><a href="#集束搜索的误差分析" class="headerlink" title="集束搜索的误差分析"></a>集束搜索的误差分析</h2><p>集束搜索是一种启发式搜索算法，不总是输出可能性最大的句子。当结合 <code>Seq2Seq</code> 模型和集束搜索算法所构建的系统出错（没有输出最佳翻译结果）时，我们通过误差分析来分析错误出现在 <code>RNN</code> 模型还是集束搜索算法中。<br>例如对于下面的人工翻译和模型输出结果：$$\text{Human: Jane visits Africa in September. (\(y ^*\))}$$$$\text{Algorithm: Jane visits Africa last September. (\(\widehat y\))}$$将翻译中没有太大差别的前三个单词作为解码器前三个时间步的输入，得到第四个时间步的条件概率 \(P(y ^*|x)\)、\(P(\widehat y|x)\) 比较大小分析：</p>
<ul>
<li>若 \(P(y ^*|x) &gt; P(\widehat y|x)\) 说明集束搜索算法出错；</li>
<li>若 \(P(y ^*|x) \leq P(\widehat y|x)\) 说明 <code>RNN</code> 模型效果不好。</li>
</ul>
<p>建立一个如下图所示的表格，记录对每一个错误的分析，有助于判断错误出现在 <code>RNN</code> 模型还是集束搜索算法中。如果错误出现在集束搜索算法中，可以考虑增大集束宽 \(B\) ；否则，需要进一步分析，看是需要正则化、更多数据或是尝试一个不同的网络结构。<img src="/images/deeplearning_ai_125.png" alt="误差分析"></p>
<h2 id="Belu-得分"><a href="#Belu-得分" class="headerlink" title="Belu 得分"></a>Belu 得分</h2><blockquote>
<p><code>Belu</code> 得分用来评估机器翻译的好坏，机器翻译的结果越接近人工翻译，则得分越高。</p>
</blockquote>
<p>开始衡量机器翻译输出质量方法之一是观察输出结果中每一个词是否出现在参考中（人工翻译中），但此方法容易出现错误，如下图左边结果（精度为 \(1\) ，实际结果很差）。改进的方法是将输出的每个单词在人工翻译结果中出现的次数作为分子（分子取最大的一个，因为可能有多个人工翻译参考），输出的单词数为分母，如下图所示：<img src="/images/deeplearning_ai_126.png" alt="belu得分例子">上述方法是以单个词为单位进行统计，称为<strong>一元组</strong> ( <code>unigram</code> )，也可以以成对的词（连续的两个词）为单位进行统计，称为<strong>二元组</strong> ( <code>bigram</code> ) ，对于二元组合，分母为每个二元组在输出中出现的次数之和，分子为二元组在人工翻译中出现的次数之和。以此类推，得到以 \(n\) 个单词为整体的 <strong>\(n\) 元组</strong> ( <code>n-gram</code> ) ，对应的 <code>Belu</code> 得分计算公式为：$$p _n = \dfrac {\sum _{\text {n-gram} \in \widehat y}\text {count} _{\text {clip}}(\text {n-gram})}{\sum _{\text {n-gram} \in \widehat y}\text {count}(\text {n-gram})}$$最后将得分组合（一元组，二元组等等）得到最终的 <code>Belu</code> 得分。公式如下：$$p _{\text {avg}} = \text {exp}(\dfrac {1}{N}\sum _{i=1} ^{N}\log p _i)$$上述公式存在一个问题就是机器翻译结果输出很短时，较容易得到更大的分值，因为输出的大部分词可能出现在人工翻译中，实际中对上述公式加一个调整项 \(BP\) ，用以惩罚较短的输出：$$\text {BP} = \begin {cases}1 &amp; , \text {MT length }\geq \text {BM length}\\ \text {exp}(1-\dfrac {\text {MT length}}{\text {BM length}}) &amp; , \text {MT length} &lt; \text {BM length} \end {cases}$$最终的 <code>Belu</code> 得分为：$$p _{\text {avg}} = \text {BP}\times\text {exp}(\dfrac {1}{N}\sum _{i=1} ^{N}\log p _i)$$<code>Bleu</code> 得分的贡献是提出了一个表现不错的<strong>单一实数评估指标</strong>，因此加快了整个机器翻译领域以及其他文本生成领域的进程。<br>相关论文：<br><a href="http://www.aclweb.org/anthology/P02-1040.pdf" target="_blank" rel="noopener">Papineni et. al., 2002. A method for automatic evaluation of machine translation</a></p>
<h2 id="注意力模型"><a href="#注意力模型" class="headerlink" title="注意力模型"></a>注意力模型</h2><p>对于一大段文字，人工翻译一般每次阅读并翻译一小部分。因为难以记忆，很难每次将一大段文字一口气翻译完。同理，用 <code>Seq2Seq</code> 模型建立的翻译系统，对于长句子，<code>Blue</code> 得分会随着输入序列长度的增加而降低。实际上，我们也并不希望神经网络每次去“记忆”很长一段文字，而是想让它像人工翻译一样工作。因此，注意力模型 ( <code>Attention Model</code>) 被提出，如下图：<img src="/images/deeplearning_ai_127.png" alt="注意力模型">图中低层是一个双向 <code>RNN</code> 网络，该网络每个时间步的激活值包含前向传播和反向传播激活值：$$a ^{\langle t’\rangle}=(\overrightarrow a ^{\langle t’\rangle}, \overleftarrow a ^{\langle t’\rangle})$$顶层是一个多对多的 <code>RNN</code> 网络，每个时间步 \(t\) 的输入包括该网络前个时间步的激活值 \(s ^{\langle t-1\rangle}\) 、输出 \(y ^{\langle t-1\rangle}\) 和低层网络中多个时间步的激活 \(c\)（上下文），其中 \(c\) 计算如下：$$c ^{\langle t\rangle} = \sum _t’ \alpha ^{\langle t, t’\rangle}a ^{\langle t’\rangle}$$其中 \(\alpha ^{\langle t, t’\rangle}\) 表示 \(y ^{\langle t\rangle}\) 对 \(a ^{\langle t’\rangle}\) 应该保持的注意力权重，需要满足：$$\sum _t’ \alpha ^{\langle t, t’\rangle}=1$$可以使用 <code>Softmax</code> 函数确保上式成立，有：$$\alpha ^{\langle t, t’\rangle} = \dfrac {\text {exp}(e ^{\langle t, t’\rangle})}{\sum _{t’=1} ^{T _x}\text {exp}(e ^{\langle t, t’\rangle})}$$对于 \(e ^{\langle t, t’\rangle}\) 通过神经网络学习得到。输入是 \(s ^{\langle t-1\rangle}\) 和 \(a ^{\langle t’\rangle}\) 如下图所示：<img src="/images/deeplearning_ai_128.png" alt="注意力模型学习参数">注意力模型缺点是时间复杂度为 \(O(n ^3)\) 。<br>相关论文：<br><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et. al., 2014. Neural machine translation by jointly learning to align and translate</a><br><a href="https://arxiv.org/pdf/1502.03044.pdf" target="_blank" rel="noopener">Xu et. al., 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></p>
<h2 id="语音识别"><a href="#语音识别" class="headerlink" title="语音识别"></a>语音识别</h2><blockquote>
<p>语音识别任务中，输入是一段以时间为横轴的音频片段，输出是文本。</p>
</blockquote>
<p>音频数据的常见预处理步骤，生成一个声谱图 ( <code>a spectrogram</code> ) 。同样地，横轴是时间，纵轴是声音的频率 ( <code>frequencies</code> ) ，而图中不同的颜色，显示了声波能量的大小 ( <code>the amount of energy</code> ) ，也就是在不同的时间和频率上这些声音有多大。<img src="/images/deeplearning_ai_130.png" alt="语音信号预处理">语音识别系统可以用注意力模型来构建，如下图所示：<img src="/images/deeplearning_ai_129.png" alt="语音识别注意力模型">用 <code>CTC ( Connectionist Temporal Classification )</code> 损失函数来做语音识别的效果也不错。语音识别中输入时间步一般比输出大的多，<code>CTC</code> 损失允许下图所示的输出：<img src="/images/deeplearning_ai_131.png" alt="CTC损失结构"><code>CTC</code> 损失函数的一个基本规则是将空白符 ( <code>blank</code> ) 之间的重复的字符折叠起来，再将空白符去掉，得到最终的输出文本。<br>相关论文：<br><a href="http://people.idsia.ch/~santiago/papers/icml2006.pdf" target="_blank" rel="noopener">Graves et al., 2006. Connectionist Temporal Classification: Labeling unsegmented sequence data with recurrent neural networks</a></p>
<h2 id="触发字检测"><a href="#触发字检测" class="headerlink" title="触发字检测"></a>触发字检测</h2><p><strong>触发词检测</strong> ( <code>Trigger Word Detection</code> ) 常用于各种智能设备，通过约定的触发词可以语音唤醒设备。<br>使用 <code>RNN</code> 来实现触发词检测时，可以将触发词对应的序列的标签设置为 \(1\) ，而将其他的标签设置为 \(0\) 。<img src="/images/deeplearning_ai_132.png" alt="触发字检测"></p>
<h1 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h1><p>至此，吴恩达老师的深度学习五大讲视频学习笔记已经全部完成。<br><strong>特别感谢</strong>：</p>
<ul>
<li><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">网易云课程吴恩达老师的视频教程</a></li>
<li><a href="http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/" target="_blank" rel="noopener">吴恩达《深度学习》系列课程笔记</a></li>
<li><a href="http://www.ai-start.com/" target="_blank" rel="noopener">黄海广的笔记PDF</a></li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04-卷积神经网络/" rel="next" title="04.卷积神经网络">
                <i class="fa fa-chevron-left"></i> 04.卷积神经网络
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/浅谈机器学习中独立成分分析-ICA/" rel="prev" title="浅谈机器学习中独立成分分析-ICA">
                浅谈机器学习中独立成分分析-ICA <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/wukong.png"
                alt="穆义" />
            
              <p class="site-author-name" itemprop="name">穆义</p>
              <p class="site-description motion-element" itemprop="description">既已无岸，不必回头，唯有向前，踏碎云霄，放肆桀骜</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#循环序列模型"><span class="nav-number">1.</span> <span class="nav-text">循环序列模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#序列模型的应用"><span class="nav-number">1.1.</span> <span class="nav-text">序列模型的应用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数学符号"><span class="nav-number">1.2.</span> <span class="nav-text">数学符号</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#循环神经网络模型"><span class="nav-number">1.3.</span> <span class="nav-text">循环神经网络模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播"><span class="nav-number">1.4.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#不同类型的循环神经网络"><span class="nav-number">1.5.</span> <span class="nav-text">不同类型的循环神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#语言模型和序列生成"><span class="nav-number">1.6.</span> <span class="nav-text">语言模型和序列生成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#采样"><span class="nav-number">1.7.</span> <span class="nav-text">采样</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN-的梯度消失"><span class="nav-number">1.8.</span> <span class="nav-text">RNN 的梯度消失</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GRU（门控循环单元）"><span class="nav-number">1.9.</span> <span class="nav-text">GRU（门控循环单元）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM-Long-Short-Term-Memory"><span class="nav-number">1.10.</span> <span class="nav-text">LSTM (Long Short Term Memory)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#双向循环神经网络"><span class="nav-number">1.11.</span> <span class="nav-text">双向循环神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#深度循环神经网络-DRNN"><span class="nav-number">1.12.</span> <span class="nav-text">深度循环神经网络( DRNN )</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#自然语言处理和词嵌入"><span class="nav-number">2.</span> <span class="nav-text">自然语言处理和词嵌入</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#词嵌入"><span class="nav-number">2.1.</span> <span class="nav-text">词嵌入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#词嵌入和迁移学习"><span class="nav-number">2.2.</span> <span class="nav-text">词嵌入和迁移学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#词嵌入和类比推理"><span class="nav-number">2.3.</span> <span class="nav-text">词嵌入和类比推理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#嵌入矩阵"><span class="nav-number">2.4.</span> <span class="nav-text">嵌入矩阵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习词嵌入"><span class="nav-number">2.5.</span> <span class="nav-text">学习词嵌入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Word2Vec"><span class="nav-number">2.6.</span> <span class="nav-text">Word2Vec</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#负采样"><span class="nav-number">2.7.</span> <span class="nav-text">负采样</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GloVe-词向量"><span class="nav-number">2.8.</span> <span class="nav-text">GloVe 词向量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#情感分类"><span class="nav-number">2.9.</span> <span class="nav-text">情感分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#词嵌入除偏"><span class="nav-number">2.10.</span> <span class="nav-text">词嵌入除偏</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#序列模型和注意力机制"><span class="nav-number">3.</span> <span class="nav-text">序列模型和注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Seq2Seq-模型"><span class="nav-number">3.1.</span> <span class="nav-text">Seq2Seq 模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#选择最可能的句子"><span class="nav-number">3.2.</span> <span class="nav-text">选择最可能的句子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#集束搜索"><span class="nav-number">3.3.</span> <span class="nav-text">集束搜索</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#改进集束搜索"><span class="nav-number">3.4.</span> <span class="nav-text">改进集束搜索</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#集束搜索的误差分析"><span class="nav-number">3.5.</span> <span class="nav-text">集束搜索的误差分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Belu-得分"><span class="nav-number">3.6.</span> <span class="nav-text">Belu 得分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#注意力模型"><span class="nav-number">3.7.</span> <span class="nav-text">注意力模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#语音识别"><span class="nav-number">3.8.</span> <span class="nav-text">语音识别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#触发字检测"><span class="nav-number">3.9.</span> <span class="nav-text">触发字检测</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#致谢"><span class="nav-number">4.</span> <span class="nav-text">致谢</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">穆义</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  

  

  

</body>
</html>
