<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Times New Roman:300,300italic,400,400italic,700,700italic|18:300,300italic,400,400italic,700,700italic|Courier New:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hello, World" />










<meta name="description" content="循环序列模型序列模型的应用下图是序列模型的部分应用场景：在深度学习中，序列模型的代表是循环神经网络（ Recurrent NeuralNetwork, RNN ） 数学符号对于某一个序列数据 \(x\) （输入时间序列数据）相关符号约定如下：  \(x ^{\langle t\rangle}\) ：表示这个数据中的第 \(t\) 的元素； \(y ^{\langle t\rangle}\)">
<meta property="og:type" content="article">
<meta property="og:title" content="05.序列模型">
<meta property="og:url" content="https://muyi110.github.io/2018/05-序列模型/index.html">
<meta property="og:site_name" content="MuYi&#39;s Blog">
<meta property="og:description" content="循环序列模型序列模型的应用下图是序列模型的部分应用场景：在深度学习中，序列模型的代表是循环神经网络（ Recurrent NeuralNetwork, RNN ） 数学符号对于某一个序列数据 \(x\) （输入时间序列数据）相关符号约定如下：  \(x ^{\langle t\rangle}\) ：表示这个数据中的第 \(t\) 的元素； \(y ^{\langle t\rangle}\)">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_100.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_101.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_102.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_103.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_104.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_105.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_106.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_107.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_108.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_109.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_110.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_111.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_112.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_113.png">
<meta property="og:updated_time" content="2018-08-20T08:55:34.545Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="05.序列模型">
<meta name="twitter:description" content="循环序列模型序列模型的应用下图是序列模型的部分应用场景：在深度学习中，序列模型的代表是循环神经网络（ Recurrent NeuralNetwork, RNN ） 数学符号对于某一个序列数据 \(x\) （输入时间序列数据）相关符号约定如下：  \(x ^{\langle t\rangle}\) ：表示这个数据中的第 \(t\) 的元素； \(y ^{\langle t\rangle}\)">
<meta name="twitter:image" content="https://muyi110.github.io/images/deeplearning_ai_100.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://muyi110.github.io/2018/05-序列模型/"/>





  <title>05.序列模型 | MuYi's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">MuYi's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">此博客创建于2018-07-10</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-reprinted_article">
          <a href="/reprinted-article/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heart"></i> <br />
            
            转载文章
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://muyi110.github.io/2018/05-序列模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="穆义">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/wukong.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MuYi's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">05.序列模型</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-19T20:34:37+08:00">
                2018-08-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习课程-Andrew-Ng-学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习课程(Andrew Ng)学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h1 id="循环序列模型"><a href="#循环序列模型" class="headerlink" title="循环序列模型"></a>循环序列模型</h1><h2 id="序列模型的应用"><a href="#序列模型的应用" class="headerlink" title="序列模型的应用"></a>序列模型的应用</h2><p>下图是<strong>序列模型</strong>的部分应用场景：<img src="/images/deeplearning_ai_100.png" alt="序列模型应用场景">在深度学习中，序列模型的代表是<strong>循环神经网络</strong>（ <code>Recurrent NeuralNetwork, RNN</code> ）</p>
<h2 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h2><p>对于某一个序列数据 \(x\) （输入时间序列数据）相关符号约定如下：</p>
<ul>
<li>\(x ^{\langle t\rangle}\) ：表示这个数据中的第 \(t\) 的元素；</li>
<li>\(y ^{\langle t\rangle}\) ：表示这个数据中的对应的第 \(t\) 的标签；</li>
<li>\(T _x\) ：输入的数据长度；</li>
<li>\(T _y\) ：输出的数据长度；</li>
<li>\(x ^{\left (i\right )\langle t\rangle}\)：第 \(i\) 个输入序列数据（第 \(i\) 个样本）的第 \(t\) 个元素；</li>
<li>\(y ^{\left (i\right )\langle t\rangle}\)：第 \(i\) 个输出（第 \(i\) 个样本对应的输出）的第 \(t\) 个标签；</li>
<li>\(T _x ^{(i)}\) ：第 \(i\) 个输入序列数据（第 \(i\) 个样本）的长度；</li>
<li>\(T _y ^{(i)}\) ：第 \(i\) 个输出（第 \(i\) 个样本）的长度。</li>
</ul>
<p>例如输入是一段话， \(x ^{\langle t\rangle}\) 的表示方式为：首先建立一个<strong>词汇表</strong>（或者称为<strong>字典</strong>），根据要表示单词在词汇表中的位置，采用 <code>one-hot</code> 编码表示 \(x ^{\langle t\rangle}\) 对应的单词。例如单词 <code>zulu</code> 在词汇表的最后一位，则 \(x ^{\langle T _x\rangle}\) 表示为：$$\begin {bmatrix} 0\\0\\\vdots\\1\end {bmatrix}$$</p>
<ul>
<li>杂谈： <code>one-hot</code> 编码的缺点：<br>每个单词被表示为完全独立的个体， 因此单词间的相似度无法体现。例如单词 <code>hotel</code> 和 <code>motel</code> 意思相近，但与 <code>cat</code> 不同，但：$$(x ^{\langle \text {hotel}\rangle}) ^T x ^{\langle \text {motel}\rangle} = (x ^{\langle \text {hotel}\rangle}) ^T x ^{\langle \text {cat}\rangle} = 0$$</li>
</ul>
<h2 id="循环神经网络模型"><a href="#循环神经网络模型" class="headerlink" title="循环神经网络模型"></a>循环神经网络模型</h2><p>对于序列数据，如果使用标准的神经网络则存在如下问题：</p>
<ul>
<li>对于不同的例子，输入和输出可能有不同的长度，因此输入层和输出层的神经元个数无法确定；</li>
<li>从文本的不同位置学习到的特征无法共享；</li>
<li>模型参数多，计算量大。</li>
</ul>
<p><strong>循环神经网络</strong>（ <code>Recurrent Neural Network, RNN</code> ）则没有上述问题。基本的 <code>RNN</code> 网络结构如下图所示：<img src="/images/deeplearning_ai_101.png" alt="基本RNN结构">当元素 \(x ^{\langle t\rangle}\) <strong>输入相对应的时间步的隐藏层的同时</strong>，该隐藏层同时接收上一时间步的隐藏层的激活值 \(a ^{\langle t-1\rangle}\) 。图中的 \(a ^ {\langle 0\rangle}\) 一般初始化为零向量，每个时间步对应输出一个预测值 \(\widehat y ^{\langle t\rangle}\) 。<br><code>RNN</code> 网络从左到右扫描输入数据，<strong>每个时间步对应参数共享</strong>。输入、激活、输出对应的参数分别为：\(W _{ax}\)、\(W _{aa}\)、\(W _{ya}\)（<strong>注：图中是 \(W _{ay}\) 最好改过来和视频一致</strong>）。<br>下图是一个基本的 <code>RNN</code> 网络的单元：<img src="/images/deeplearning_ai_102.png" alt="基本RNN单元">前向传播的计算公式为：$$a ^{\langle t\rangle}=g _1 (W _{aa}a ^{\langle t-1\rangle}+W _{ax}x ^{\langle t\rangle}+b _a)$$$$\widehat y ^{\langle t\rangle}= g _2 (W _{ya}a ^{\langle t\rangle}+b _y)$$$$a ^{\langle 0\rangle}=\overrightarrow 0$$激活函数 \(g _1\) 一般选择 <code>tanh</code> 函数，偶尔也选择 <code>Relu</code> 函数。激活函数 \(g _2\) 选择 <code>Sigmoid</code> 或者 <code>Softmax</code> 函数，取决于具体的应用（多分类还是二分类）。<br>将上述的前向传播公式符号简化如下：$$W _a =[W _{ax}, W _{aa}]$$$$a ^{\langle t\rangle} = g _1 (W _a [a ^{\langle t-1\rangle}, x ^{\langle t\rangle}]+ b _a)$$$$\widehat y ^{\langle t\rangle} = g _2 (W _y a ^{\langle t\rangle}+ b _y)$$将 \(W _{aa}\) 和 \(W _{ax}\) <strong>水平并列</strong>为一个矩阵 \(W _a\) 将 \(a ^{\langle t-1\rangle}\) 和 \(x ^{\langle t\rangle}\) <strong>堆叠</strong>为一个矩阵。<br>下图是 <code>RNN</code> 网络的前向传播示意图：<img src="/images/deeplearning_ai_103.png" alt="RNN前向传播"></p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>为了计算反向传播，需要定义一个损失函数。一个元素的损失函数（一个时间步）定义为<strong>交叉损失函数</strong>，如下：$$L ^{\langle t\rangle}(\widehat y ^{\langle t\rangle}, y ^{\langle t\rangle}) = -y ^{\langle t\rangle}\log \widehat y ^{\langle t\rangle} - (1- y ^{\langle t\rangle})\log (1- \widehat y ^{\langle t\rangle})$$进而得到整个序列的损失函数如下：$$J = \sum _{t=1} ^{T _x}L ^{\langle t\rangle}(\widehat y ^{\langle t\rangle}, y ^{\langle t\rangle})$$基本<code>RNN</code> 网络的反向传播公式如下图所示：<img src="/images/deeplearning_ai_104.png" alt="RNN反向传播"></p>
<h2 id="不同类型的循环神经网络"><a href="#不同类型的循环神经网络" class="headerlink" title="不同类型的循环神经网络"></a>不同类型的循环神经网络</h2><p>某些应用场景中，输入和输出的长度可能不一样，根据输入和输出的序列长度，可以将循环神经网络划分为如下几种结构：<img src="/images/deeplearning_ai_105.png" alt="RNN的几种结构">目前介绍的模型只是使用序列之前的信息来预测当前的值，没有使用后文的信息。可以引入<strong>双向循环神经网络</strong>（ <code>Bidirectional RNN, BRNN</code> ）解决。</p>
<h2 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h2><p><strong>语言模型会告诉特点句子出现的概率是多少</strong>，并以此为依据作出准确判断。建立语言模型所采用的训练集是一个大型的<strong>语料库</strong>，指众多句子组成的文本。第一步需要将输入的某个数据 \(x\) <strong>标记化</strong>，即建立字典，采用 <code>one-hot</code> 编码。<br>将标记化后的训练集用于训练 <code>RNN</code> 网络，如下图所示：<img src="/images/deeplearning_ai_106.png" alt="RNN训练">在第一个时间步中，输入 \(a ^{\langle 0\rangle}\) 和 \(x ^{\langle 1\rangle}\) 其都是零向量，\(\widehat y ^{\langle 1\rangle}\)是通过 <code>Softmax</code> 预测出的字典中每个单词作为该句子第一个单词出现的概率；<br>在第二的时间步中，输入 \(x ^{\langle 2\rangle}\) 和 上一层的激活值 \(a ^{\langle 1\rangle}\) ，其中 \(x ^{\langle 2\rangle}\) 是训练样本标签中的第一个单词 \(y ^{\langle 1\rangle}\)（即单词 <code>cats</code> ），输出的 \(y ^{\langle 2\rangle}\) 是通过 <code>Softmax</code> 函数预测的以单词 <code>cats</code> 为条件出现字典中其他每个词的条件概率，以此类推得到整个句子出现的概率。<br>训练网络定义的损失函数为：$$L(\widehat y ^{\langle t\rangle}, y ^{\langle t\rangle}) = -\sum _{i}y _i ^{\langle t\rangle}\log \widehat y _i ^{\langle t\rangle}$$总体损失函数为：$$L = \sum _{t}L ^{\langle t\rangle}(\widehat y ^{\langle t\rangle}, y ^{\langle t\rangle})$$将训练好的网络用于新的句子，例如一个新句子只有三个单词 \(y ^{\langle 1\rangle}\) 、\(y ^{\langle 2\rangle}\) 、\(y ^{\langle 3\rangle}\) 如要按公式：$$P(y ^{\langle 1\rangle}, y ^{\langle 2\rangle}, y ^{\langle 3\rangle})= P(y ^{\langle 1\rangle})P(y ^{\langle 2\rangle}|y ^{\langle 1\rangle})P(y ^{\langle 3\rangle}|y ^{\langle 2\rangle}, y ^{\langle 1\rangle})$$得到整个句子的概率。</p>
<h2 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h2><p>当训练好一个语言模型后，可以通过非正式方法<strong>采样</strong>新的序列来了解模型学到的什么。<br><img src="/images/deeplearning_ai_107.png" alt="采样">在第一个时间步中，输入 \(a ^{\langle 0\rangle}\) 和 \(x ^{\langle 1\rangle }\) 其都是零向量，输出预测出字典中每个词作为第一个词的概率，根据 <code>Softmax</code> 分布进行随机采样（ <code>np.random.choice</code> ），将采样得到的 \(\widehat y ^{\langle 1\rangle}\) 作为第二个时间步的输入 \(x ^{\langle 2\rangle }\) 。以此类推，直到采样到句子结尾标志（例如 <code>EOS</code> 等）或采样达到特定的时间步长，最后该模型可以生成一些句子。<br>实际中也可以考虑建立基于字符的语言模型（上述是基于词汇的语言模型），基于字符的语言模型得到的序列过长，计算成本高。</p>
<h2 id="RNN-的梯度消失"><a href="#RNN-的梯度消失" class="headerlink" title="RNN 的梯度消失"></a>RNN 的梯度消失</h2><p>$$\text {The cat, which already ate a bunch of food, was full.}$$$$\text {The cats, which already ate a bunch of food, were full.}$$对于上面的两句话，后面谓语动词单复数形式由前面主语名词的单复数形式决定。但<strong>基本的 <code>RNN</code>网络不擅长获取这种长期依赖关系</strong>。其原因主要是由于梯度消失，在反向传播时，后面层的输出误差很难影响前面层的计算，网络很难调整前面层的计算。反向传播如下图所示：<img src="/images/deeplearning_ai_108.png" alt="反向传播">在反向传播过程中，也有可能出现梯度爆炸问题（较容易发现，参数会快速膨胀到数值溢出，可能显示 <code>NaN</code> 。可以采用<strong>梯度修剪</strong>解决。观察梯度值，如果大于某个阈值，则缩放梯度向量保证其不会太大。相比之下，梯度消失问题更难解决，目前<strong>GRU 和 LSTM 可以作为缓解梯度消失问题的解决方案</strong>。</p>
<h2 id="GRU（门控循环单元）"><a href="#GRU（门控循环单元）" class="headerlink" title="GRU（门控循环单元）"></a>GRU（门控循环单元）</h2><p>对于句子$$\text {The cat, which already ate a bunch of food, was full.}$$当从左到右读的时候，<code>GRU</code> 有一个新的变量称为 <code>c</code> ，表示<strong>记忆细胞</strong>（<code>Memory Cell</code> ），其提供记忆的能力，例如记住前面的主语是单数函数复数。在时间 <code>t</code> 记忆细胞的值 \(c ^{\langle t\rangle}\) 等于激活值 \(a ^{\langle t\rangle}\) 。\(\widehat c ^{\langle t\rangle}\) 表示下一个 \(c\) 的候选值。\(\Gamma _u\) 表示<strong>更新门</strong>，用于决定什么时候更新记忆细胞的值。具体公式如下：$$\widehat c ^{\langle t\rangle}= \tanh (W _c [c ^{\langle t-1\rangle}, x ^{\langle t\rangle}] +b _c )$$$$\Gamma _u = \sigma (W _u [c ^{\langle t-1\rangle}, x ^{\langle t\rangle}] + b _u )$$$$c ^{\langle t\rangle} = \Gamma _u \times \widehat c ^{\langle t\rangle} + (1-\Gamma _u)\times c ^{\langle t-1\rangle}$$$$a ^{\langle t\rangle}= c ^{\langle t\rangle}$$上述公式是简化过的 <code>GRU</code> 单元，实际中完整的 <code>GRU</code> 单元还有一个新的<strong>相关门</strong> \(\Gamma _r\) ，表示 \(\widehat c ^{\langle t\rangle}\) 和 \(c ^{\langle t\rangle}\) 间的相关性，完整的公式如下：$$\widehat c ^{\langle t\rangle}= \tanh (W _c [\Gamma _r c ^{\langle t-1\rangle}, x ^{\langle t\rangle}] +b _c )$$$$\Gamma _u = \sigma (W _u [c ^{\langle t-1\rangle}, x ^{\langle t\rangle}] + b _u )$$$$\Gamma _r = \sigma (W _r [c ^{\langle t-1\rangle}, x ^{\langle t\rangle}] + b _r )$$$$c ^{\langle t\rangle} = \Gamma _u \times \widehat c ^{\langle t\rangle} + (1-\Gamma _u)\times c ^{\langle t-1\rangle}$$$$a ^{\langle t\rangle}= c ^{\langle t\rangle}$$为了便于理解，给出下图（下图中符号和上面公式表示不同）：<img src="/images/deeplearning_ai_109.png" alt="GRU结构"><br>相关博客：<br><a href="https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be" target="_blank" rel="noopener">Understanding GRU networks</a><br>相关论文：<br><a href="https://arxiv.org/pdf/1409.1259.pdf" target="_blank" rel="noopener">Cho et al., 2014. On the properties of neural machine translation: Encoder-decoder approaches</a><br><a href="https://arxiv.org/pdf/1412.3555.pdf" target="_blank" rel="noopener">Chung et al., 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a></p>
<h2 id="LSTM-Long-Short-Term-Memory"><a href="#LSTM-Long-Short-Term-Memory" class="headerlink" title="LSTM (Long Short Term Memory)"></a>LSTM (Long Short Term Memory)</h2><p><code>LSTM</code> 比 <code>GRU</code> 更加灵活，其引入了<strong>遗忘门</strong> \(\Gamma _f\) 和<strong>输出门</strong> \(\Gamma _o\) ，其一个单元结构如下图所示：<img src="/images/deeplearning_ai_110.png" alt="LSTM单元结构"><code>LSTM</code> 网络如下图所示：<img src="/images/deeplearning_ai_111.png" alt="LSTM网络">实际使用中，几个门值不仅取决于 \(a ^{\langle t-1\rangle}\) 和 \(x ^{\langle t\rangle}\) ，有时候可以偷窥上个记忆细胞的输入值 \(c ^{\langle t-1\rangle}\) 此被称为<strong>窥视连接</strong>。<br>\(c ^{\langle 0\rangle}\) 通常初始化为零向量。<br>相关博客：<br><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></p>
<h2 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h2><p>单向的循环神经网络在某一时刻的预测结果只能使用之前输入的序列信息。双向循环神经网络（<code>Bidirectional RNN，BRNN</code> ）可以在序列的任意位置使用之前和之后的数据。其工作原理是增加一个反向循环层，如下图所示：<img src="/images/deeplearning_ai_112.png" alt="BRNN结构">根据上图有：$$y ^{\langle t\rangle} = g(W _y [\overrightarrow a ^{\langle t\rangle}, \overlefttarrow a ^{\langle t\rangle}]+ b_y )$$其中基本单元不仅可以是标准的 <code>RNN</code> 模块，也可以是 <code>LSTM</code> 或 <code>GRU</code> 模块。缺点是需要完整的序列数据，才能预测任意位置的结果。例如构建语音识别系统，需要等待用户说完并获取整个语音表达，才能处理这段语音并进一步做语音识别。因此，实际应用会有更加复杂的模块。</p>
<h2 id="深度循环神经网络-DRNN"><a href="#深度循环神经网络-DRNN" class="headerlink" title="深度循环神经网络( DRNN )"></a>深度循环神经网络( DRNN )</h2><p>深度循环神经网络（<code>Deep RNN</code>)的结构如下：<img src="/images/deeplearning_ai_113.png" alt="DRNN结构">以 \(a ^{[2]\langle 3\rangle}\) 为例，有：$$a ^{[2]\langle 3\rangle} = g(W _a ^{[2]}[a ^{[2]\langle 2\rangle}, a ^{[1]\langle 3\rangle}]+ b _a ^{[2]})$$</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04-卷积神经网络/" rel="next" title="04.卷积神经网络">
                <i class="fa fa-chevron-left"></i> 04.卷积神经网络
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/wukong.png"
                alt="穆义" />
            
              <p class="site-author-name" itemprop="name">穆义</p>
              <p class="site-description motion-element" itemprop="description">既已无岸，不必回头，唯有向前，踏碎云霄，放肆桀骜</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#循环序列模型"><span class="nav-number">1.</span> <span class="nav-text">循环序列模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#序列模型的应用"><span class="nav-number">1.1.</span> <span class="nav-text">序列模型的应用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数学符号"><span class="nav-number">1.2.</span> <span class="nav-text">数学符号</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#循环神经网络模型"><span class="nav-number">1.3.</span> <span class="nav-text">循环神经网络模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播"><span class="nav-number">1.4.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#不同类型的循环神经网络"><span class="nav-number">1.5.</span> <span class="nav-text">不同类型的循环神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#语言模型和序列生成"><span class="nav-number">1.6.</span> <span class="nav-text">语言模型和序列生成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#采样"><span class="nav-number">1.7.</span> <span class="nav-text">采样</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN-的梯度消失"><span class="nav-number">1.8.</span> <span class="nav-text">RNN 的梯度消失</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GRU（门控循环单元）"><span class="nav-number">1.9.</span> <span class="nav-text">GRU（门控循环单元）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM-Long-Short-Term-Memory"><span class="nav-number">1.10.</span> <span class="nav-text">LSTM (Long Short Term Memory)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#双向循环神经网络"><span class="nav-number">1.11.</span> <span class="nav-text">双向循环神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#深度循环神经网络-DRNN"><span class="nav-number">1.12.</span> <span class="nav-text">深度循环神经网络( DRNN )</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">穆义</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  

  

  

</body>
</html>
