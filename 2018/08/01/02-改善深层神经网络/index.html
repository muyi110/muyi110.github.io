<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Times New Roman:300,300italic,400,400italic,700,700italic|18:300,300italic,400,400italic,700,700italic|Courier New:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hello, World" />










<meta name="description" content="深度学习的实用层面数据集划分：训练/验证/测试 应用深度学习/机器学习是一个不断迭代的过程。  对于一个实际问题的样本数据，在建立模型的过程中，数据集会被划分为如下部分：  训练集：对模型进行训练，学习模型的参数； 验证集：用于交叉验证，选择最好的模型； 测试集：对学习到的模型进行测试，获取模型运行的无偏估计。  对小数据情况（如：100、1000、10000）数据集通常按照如下比例划分：">
<meta property="og:type" content="article">
<meta property="og:title" content="02.改善深层神经网络">
<meta property="og:url" content="https://muyi110.github.io/2018/08/01/02-改善深层神经网络/index.html">
<meta property="og:site_name" content="MuYi&#39;s Blog">
<meta property="og:description" content="深度学习的实用层面数据集划分：训练/验证/测试 应用深度学习/机器学习是一个不断迭代的过程。  对于一个实际问题的样本数据，在建立模型的过程中，数据集会被划分为如下部分：  训练集：对模型进行训练，学习模型的参数； 验证集：用于交叉验证，选择最好的模型； 测试集：对学习到的模型进行测试，获取模型运行的无偏估计。  对小数据情况（如：100、1000、10000）数据集通常按照如下比例划分：">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_028.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_029.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_030.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_031.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_032.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_033.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_034.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_035.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_036.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_037.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_038.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_039.png">
<meta property="og:updated_time" content="2018-08-03T01:47:13.798Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="02.改善深层神经网络">
<meta name="twitter:description" content="深度学习的实用层面数据集划分：训练/验证/测试 应用深度学习/机器学习是一个不断迭代的过程。  对于一个实际问题的样本数据，在建立模型的过程中，数据集会被划分为如下部分：  训练集：对模型进行训练，学习模型的参数； 验证集：用于交叉验证，选择最好的模型； 测试集：对学习到的模型进行测试，获取模型运行的无偏估计。  对小数据情况（如：100、1000、10000）数据集通常按照如下比例划分：">
<meta name="twitter:image" content="https://muyi110.github.io/images/deeplearning_ai_028.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://muyi110.github.io/2018/08/01/02-改善深层神经网络/"/>





  <title>02.改善深层神经网络 | MuYi's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">MuYi's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">此博客创建于2018-07-10</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-reprinted_article">
          <a href="/reprinted-article/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heart"></i> <br />
            
            转载文章
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://muyi110.github.io/2018/08/01/02-改善深层神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="穆义">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/wukong.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MuYi's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">02.改善深层神经网络</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-01T11:33:38+08:00">
                2018-08-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习课程-Andrew-Ng-学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习课程(Andrew Ng)学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h1 id="深度学习的实用层面"><a href="#深度学习的实用层面" class="headerlink" title="深度学习的实用层面"></a>深度学习的实用层面</h1><h2 id="数据集划分：训练-验证-测试"><a href="#数据集划分：训练-验证-测试" class="headerlink" title="数据集划分：训练/验证/测试"></a>数据集划分：训练/验证/测试</h2><blockquote>
<p>应用深度学习/机器学习是一个不断迭代的过程。</p>
</blockquote>
<p>对于一个实际问题的样本数据，在建立模型的过程中，数据集会被划分为如下部分：</p>
<ul>
<li>训练集：对模型进行<strong>训练</strong>，学习模型的参数；</li>
<li>验证集：用于<strong>交叉验证，选择最好的模型</strong>；</li>
<li>测试集：对学习到的模型进行测试，获取模型运行的无偏估计。</li>
</ul>
<p>对<strong>小数据情况</strong>（如：100、1000、10000）数据集通常按照如下比例划分：</p>
<ul>
<li>无验证集：70% / 30%；</li>
<li>有验证集：60% / 20% / 20%。</li>
</ul>
<p>对<strong>大数据情况</strong>，数据规模可能是百万级别的，验证集和测试集所占的比重会趋向于更小。验证集的目的是选择最好的模型，验证集只要足够大到验证大约2-10种模型，不需要20%数据集，例如在百万级别数据中，选1万数据作为验证集就行。大数据集的的划分可以按照如下划分（建议）：</p>
<ul>
<li>100万数据：98% / 1% / 1%；</li>
<li>更多数据：99.5% / 0.25% / 0.25%。</li>
</ul>
<p><strong>建议</strong>：要确保<strong>验证集</strong>和<strong>测试集</strong>的数据来自同一分布。</p>
<h2 id="偏差-方差"><a href="#偏差-方差" class="headerlink" title="偏差/方差"></a>偏差/方差</h2><p><img src="/images/deeplearning_ai_028.png" alt="偏差和方差">在深度学习（或机器学习）中，偏差和方差的解释如下：</p>
<ul>
<li>偏差：可通过<strong>训练误差(train set error)</strong>反映，当<strong>训练误差大时</strong>，说明当前模型<strong>偏差高</strong>，模型存在<strong>欠拟合</strong>；</li>
<li>方差：可通过<strong>验证误差(dev set error)</strong>反映，当<strong>验证误差大时</strong>，说明当前模型<strong>方差高</strong>，模型存在<strong>过拟合</strong>。</li>
</ul>
<p>好的模型应该是<strong>偏差和方差都小</strong>。<br><strong>存在高偏差</strong>：</p>
<ul>
<li>训练更大的网络（添加隐藏层或隐藏单元数目或更大的神经网络结构）；</li>
<li>增加训练时间（不一定有效）。</li>
</ul>
<p><strong>存在高方差</strong>：</p>
<ul>
<li>增加样本数据；</li>
<li>添加<strong>正则化(regularization)</strong>；</li>
<li>寻找更加合适的网络结构。</li>
</ul>
<h2 id="正则化-regularization"><a href="#正则化-regularization" class="headerlink" title="正则化(regularization)"></a>正则化(regularization)</h2><blockquote>
<p>深度学习可能存在<strong>过拟合问题–高方差</strong>，有两个解决方法，一个是<strong>正则化</strong>，一个是<strong>准备更多数据</strong>。<strong>正则化</strong>是在成本函数中添加一个正则化项，限制模型的复杂度。</p>
</blockquote>
<h3 id="Logistic回归中正则化"><a href="#Logistic回归中正则化" class="headerlink" title="Logistic回归中正则化"></a>Logistic回归中正则化</h3><p>对于<code>Logistic regression</code>，加入<code>L2</code>正则化（也称<code>L2</code>范数）的代价函数：$$J(w,b)=\dfrac{1}{m}\sum_{i=1}^{m}L(\widehat {y}^{(i)},y^{(i)})+\dfrac{\lambda}{2m}||w||_2$$其中<code>L2</code>正则化：$$\dfrac{\lambda}{2m}||w||_2 =\dfrac{\lambda}{2m}\sum _{j=1}^{n_x}w_j^2 =\dfrac{\lambda}{2m}w^T w$$有时也使用<code>L1</code>正则化：$$\dfrac{\lambda}{2m}||w||_1 =\dfrac{\lambda}{2m}\sum _{j=1}^{n_x}|w_j|$$上式中\(\lambda\)是<strong>正则化因子，是超参数（通常使用验证集或交叉验证集来配置这个参数）。</strong><br>如果用<code>L1</code>正则化，参数\(w\)最终会是<strong>稀疏</strong>的，就是说\(w\)中有很多\(0\)。在训练网络时，越来越倾向于使用<code>L2</code>正则化。<br><strong>注意</strong>：<code>lambda</code>在<code>Python</code>中属于保留字段，所有在编程时候，使用<code>lambd</code>代替正则化因子。</p>
<h3 id="神经网络中的正则化"><a href="#神经网络中的正则化" class="headerlink" title="神经网络中的正则化"></a>神经网络中的正则化</h3><p><strong>避免过拟合的标准的方式添加<code>L2</code>正则化项</strong>，需修改代价函数：$$J=-\dfrac{1}{m}\sum _{i=1}^{m}\Big ( y^{(i)}\log(a^{[L]\left (i\right )})+(1-y^{(i)})\log(1-a^{[L]\left (i\right )})\Big )$$为$$J _{regularized} =\underbrace {-\dfrac{1}{m} \sum _{i = 1}^{m} \Big (y^{(i)}\log(a^{[L]\left (i\right )}) + (1-y^{(i)})\log(1- a^{[L]\left (i\right )}) \Big )} _\text {cross-entropy cost} + \underbrace {\dfrac{\lambda}{2m} \sum _{l=1}^{L}\sum _{k=1}^{n^{[l-1]}}\sum _{j=1}^{n^{[l]}} W _{k,j}^{[l] 2}} _\text {L2 regularization cost}$$<br><strong>权重衰减(Weight decay)</strong><br><strong>对于神经网络，加入正则化项后，梯度变为（反向传播用到）：</strong>$$dW^{[l]}=\dfrac{\partial L}{\partial {W^{[l]}}}+\dfrac{\lambda}{m}W^{[l]}$$得到梯度更新公式：$$W^{[l]}=W^{[l]}-\alpha dW^{[l]}$$可得：$$W^{[l]}=W^{[l]}-\alpha \Big [\dfrac{\partial L}{\partial {W^{[l]}}}+\dfrac{\lambda}{m}W^{[l]}\Big ]$$$$  =W^{[l]}-\alpha \dfrac{\lambda}{m}W^{[l]}-\alpha \dfrac{\partial L}{\partial {W^{[l]}}}$$$$ =(1-\dfrac{\alpha \lambda}{m})W^{[l]}-\alpha \dfrac{\partial L}{\partial {W^{[l]}}}$$其中，由于\(1-\dfrac{\alpha \lambda}{m} &lt; 1\)，会给原来的\(W^{[l]}\)一个衰减系数，因此<code>L2</code>正则化称为<strong>权重衰减(Weight Decay)</strong>。</p>
<h2 id="正则化减小过拟合的原因"><a href="#正则化减小过拟合的原因" class="headerlink" title="正则化减小过拟合的原因"></a>正则化减小过拟合的原因</h2><p><strong>直观解释</strong><br><img src="/images/deeplearning_ai_029.png" alt="正则化减小过拟合">如果正则化\(\lambda\)设置的足够大，权重矩阵\(W\)会被设置接近\(0\)，直观理解是把隐藏单元的权重设置为\(0\)，于是基本上消除了这些隐藏单元的影响。会简化神经网络为小的网络，使得网络从过拟合的状态更接近左图的高偏差状态。<br>另一种解释<img src="/images/deeplearning_ai_030.png" alt="正则化减小过拟合的另一种解释">假设使用的激活函数是\(g(z)=\tanh(z)\)(<code>sigmoid</code>同理)，加入正则化项后，当\(\lambda\)增大时，使得\(W^{[l]}\)减小，\(Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\)会变小，根据上图可知，当\(z\)较小时，函数\(\tanh(z)\)近似线性，每层的激活函数近似线性使得整个网络进行线性，不会发生过拟合。</p>
<h2 id="dropout-正则化"><a href="#dropout-正则化" class="headerlink" title="dropout 正则化"></a>dropout 正则化</h2><p><img src="/images/deeplearning_ai_031.png" alt="dropout"><code>dropout</code>(随机失活)在每次迭代中随机关闭一些神经元，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。</p>
<p><div align="center"><video width="620" height="440" src="/images/dropout1.mp4" type="video/mp4" controls><br></video></div> </p>
<p><caption><center> Drop-out on the second hidden layer.</center></caption><br>At each iteration, you shut down (set to zero) each neuron of a layer with probability<code>1-keep_prob</code> or keep it with probability<code>keep_prob</code>(50% here). The dropped neurons don’t contribute to the training in both the forward and backward propagations of the iteration.</p>
<p><div align="center"><video width="620" height="440" src="/images/dropout2.mp4" type="video/mp4" controls><br></video></div> </p>
<p><caption><center> Drop-out on the first and third hidden layers. <br> \(1^{st}\) layer: we shut down on average 40% of the neurons.  \(3^{rd}\) layer: we shut down on average 20% of the neurons.</center></caption></p>
<blockquote>
<p>When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time.</p>
</blockquote>
<p><strong>反向随机失活(Inverted dropout)</strong><br><code>Inverted dropout</code>是最常用的实现<code>dropout</code>的方法。对第<code>1</code>层实现<code>dropout</code>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = <span class="number">0.8</span> <span class="comment">#设置神经元保留概率</span></span><br><span class="line">d1 = np.random.rand(a1.shape[<span class="number">0</span>], a1.shape[<span class="number">1</span>]) &lt; keep_prob</span><br><span class="line">a1 = np.multiply(a1, d1)</span><br><span class="line">a1 /= keep_prob</span><br><span class="line"><span class="comment"># 注：d1是一个布尔型数组，python会自动把True和False翻译为1和0</span></span><br></pre></td></tr></table></figure></p>
<p>在最后一步执行<code>a1 /= keep_prob</code>是因为\(a^{[1]}\)减少比例为<code>keep_prob</code>，也就是说\(a^{[1]}\)中有比例为<code>keep_prob</code>的元素归零，为了不影响下一层\(z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}\)的期望值，需要\(W^{[2]}a^{[1]}/keep\_prob\)。<br><strong>注意</strong>：在测试阶段不要使用dropout，因为那样会使预测结果变的随机。</p>
<h2 id="理解-dropout"><a href="#理解-dropout" class="headerlink" title="理解 dropout"></a>理解 dropout</h2><p>加入了<code>dropout</code>后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。通过传播过程，<code>dropout</code>将产生和<code>L2</code>正则化相同的收缩权重的效果。<br>对于不同的层，设置的<code>keep_prob</code>也不同。一般来说，神经元较少的层，会设<code>keep_prob</code>为<code>1.0</code>，而神经元多的层则会设置比较小的<code>keep_prob</code>。<br><code>dropout</code>的一大缺点是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用<code>dropout</code>时，先将<code>keep_prob</code>全部设置为<code>1.0</code>后运行代码，确保<code>J(w,b)</code>函数单调递减，再打开 <code>dropout</code>。</p>
<h2 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h2><ul>
<li><strong>数据扩增(Data Augmentation)</strong>：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。</li>
<li><strong>早停止法(Early Stopping)</strong>：将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，在两者开始发生较大偏差时及时停止迭代，避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。<img src="/images/deeplearning_ai_032.png" alt="早停止法"></li>
</ul>
<h2 id="归一化输入-Normalizing-inputs"><a href="#归一化输入-Normalizing-inputs" class="headerlink" title="归一化输入(Normalizing inputs)"></a>归一化输入(Normalizing inputs)</h2><p><img src="/images/deeplearning_ai_033.png" alt="归一化输入">包含两步：零均值化和归一化。公式如下：$$x=\dfrac{x-\mu }{\sigma }$$其中$$\mu = \dfrac{1}{m}\sum _{i=1}^{m}x^{(i)}$$$$\sigma = \sqrt {\dfrac{1}{m}\sum _{i=1}^{m}x^{(i) 2}}$$经过归一化后每个特征的方差都变为\(1\)。<strong>注意，测试集和训练集用相同的参数\(\mu \)和\(\sigma \)进行数据归一化</strong>。<br><strong>使用归一化的原因</strong><br><img src="/images/deeplearning_ai_034.png" alt="归一化原因">使用归一化后代价函数看起来更加对称。如果不使用归一化，需要设置一个非常小的学习率，因为初始值在某些位置时，梯度下降法可能需要更多次的迭代；如果归一化后，不论从哪个位置开始梯度下降法，可以更快找到最小值，可以在梯度下降中使用较大的步长。</p>
<h2 id="梯度消失和梯度爆炸-Vanishing-Exploding-gradients"><a href="#梯度消失和梯度爆炸-Vanishing-Exploding-gradients" class="headerlink" title="梯度消失和梯度爆炸(Vanishing / Exploding gradients)"></a>梯度消失和梯度爆炸(Vanishing / Exploding gradients)</h2><p>在梯度函数上出现的以指数级递增或者递减的情况分别称为<strong>梯度爆炸或者梯度消失</strong>。<br>假定\(g(z)=z, b^{[l]}=0\)，对应的输出为：$$\widehat y = W^{[L]}W^{[L-1]}\ldots W^{[2]}W^{[1]}$$</p>
<ul>
<li>当\(W^{[l]}\)值大于\(1\)时，激活函数值以指数级递增；</li>
<li>当\(W^{[l]}\)值小于\(1\)时，激活函数值以指数级递减。</li>
</ul>
<p><strong>对于导数同理</strong>。在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练难度上升，当梯度很小时，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。<br><strong>神经网络权重初始化</strong><br>对于出现的梯度消失或梯度爆炸问题，有一个不完整的解决方案（但有效）是<strong>谨慎选择随机初始化参数</strong>。<br>根据（只有一个神经元的网络，\(b=0\) ）：$$z=w_1 x_1 + w_2 x_2 +\ldots w_n x_n$$为预防\(z\)过大或过小，当\(n\)越大，则\(w_i\)应该越小。合理的方法是设置\(w_i =\dfrac{1}{n}\)其中\(n\)表示神经元的输入特征量（对于只有一个神经元的网络）。<br>对于有多层的神经网络，设置某层\(W^{[l]}\)的权重为:$$W^{[l]} = np.random.randn(W^{[l]}.shape) * np.sqrt(\dfrac {1}{n^{[l-1]}})$$其中\(n^{[l-1]}\)是第\(l-1\)层的神经元数量。<br><strong>如果某一层 \(l\) 的激活函数是 Relu 则初始化使用 np.sqrt( \(\dfrac{2}{n^{[l-1]}}\) )效果会更好；当某一层 \(l\) 的激活函数是 tanh 则初始化使用 np.sqrt( \(\dfrac{1}{n^{[l-1]}}\) )，有时也使用 np.sqrt( \(\dfrac{2}{n^{[l-1]}+ n^{[l]}}\) )</strong>。</p>
<h2 id="梯度检验-Gradient-checking"><a href="#梯度检验-Gradient-checking" class="headerlink" title="梯度检验(Gradient checking)"></a>梯度检验(Gradient checking)</h2><p>导数或梯度的定义如下（双边误差求导）：$$\dfrac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \dfrac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}$$进行梯度检验时需要连接参数，将参数\(W^{[1]}, b^{[1]}, \ldots , W^{[L]}, b^{[L]}\)全部连接，形成一个向量\(\theta \)，如下：$$J(W^{[1]}, b^{[1]}, \ldots , W^{[L]}, b^{[L]})=J(\theta )$$同时对\(dW^{[1]}, db^{[1]}, \ldots , dW^{[L]}, db^{[L]}\)执行同样的操作得到向量\(d\theta \)，其与\(\theta \)有相同的维度。<img src="/images/deeplearning_ai_035.png" alt="梯度检验">求一个梯度的逼近值：$$d\theta _\text {approx}[i]=\dfrac{J(\theta _1 ,\theta _2 ,\ldots ,\theta _i +\varepsilon ,\ldots )-J(\theta _1 ,\theta _2 ,\ldots ,\theta _i -\varepsilon ,\ldots )}{2\theta}$$应该：$$\approx d\theta [i] =\dfrac{\partial J}{\partial \theta _i }$$因此，利用梯度检验值：$$\dfrac{||d\theta _\text {approx} -d\theta || _2}{||d\theta _\text {approx}|| _2 +||d\theta || _2}$$其中：$$||x || _2 =\sum _{i=1}^{m}|x _i | ^2$$表示<code>L2</code>范数。<br>如果梯度检验值和\(\varepsilon \)的值相近，说明神经网络的实施是正确的，否则要去检查代码是否存在<code>bug</code>。</p>
<h2 id="神经网络使用梯度检验注意事项"><a href="#神经网络使用梯度检验注意事项" class="headerlink" title="神经网络使用梯度检验注意事项"></a>神经网络使用梯度检验注意事项</h2><ol>
<li>不要在训练中使用梯度检验，它只用于调试（<code>debug</code>）。使用完毕关闭梯度检验的功能；</li>
<li>如果算法的梯度检验失败，要检查所有项，并试着找出<code>bug</code>，即确定哪个\(d\theta _\text {approx}[i]\)与<code>dθ</code>的值相差比较大；</li>
<li>当成本函数包含正则项时，也需要带上正则项进行检验；</li>
<li>梯度检验不能与<code>dropout</code>同时使用。因为每次迭代过程中，<code>dropout</code>会随机消除隐藏层单元的不同子集，难以计算<code>dropout</code>在梯度下降上的成本函数<code>J</code>。建议关闭  <code>dropout</code>，用梯度检验进行双重检查，确定在没有<code>dropout</code>的情况下算法正确，然后打开<code>dropout</code>；</li>
</ol>
<h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><blockquote>
<p>机器学习是一个经验过程，伴随大量的迭代过程，优化算法可以帮助快速训练模型。</p>
</blockquote>
<h2 id="Mini-batch-梯度下降-Mini-batch-gradient-descent"><a href="#Mini-batch-梯度下降-Mini-batch-gradient-descent" class="headerlink" title="Mini-batch 梯度下降(Mini-batch gradient descent)"></a>Mini-batch 梯度下降(Mini-batch gradient descent)</h2><p><strong>batch 梯度下降法</strong>（批梯度下降法）：同时处理整个训练集，当训练集很大时，处理速度会变慢。<br><strong>Mini-batch 梯度下降</strong>（小批量梯度下降法）：每次同时处理单个<code>mini-batch</code>(训练数据集的一部分)，其他与<code>batch</code>梯度下降法一致。<br>使用<code>batch</code>梯度下降法，对整个训练集的一次遍历只能做一次梯度下降；使用<code>mini-batch</code>梯度下降法对整个训练集的一次遍历（称为一个<code>epoch</code>）可以做<code>mini-batch</code>个数个梯度下降。之后可多次遍历数据集，直到收敛到一个合适的精度。<br>考虑一个极端情况，当<code>mini-batch</code>个数为\(1\)（一个<code>mini-batch</code>只包含一个样本），对应的算法称为随机梯度下降法(<code>Stochastic Gradient Descent--SGD</code>)。对比于<code>batch</code>下降法的区别主要是：每一次的梯度计算仅仅针对一个训练样本，而不是整个训练集( What changes is that you would be computing gradients on just one training example at a time, rather than on the whole training set )。下面的程序说明了<code>SGD</code>和<code>GD</code>算法的差异：</p>
<ul>
<li><p><strong>(Batch) Gradient Descent</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="comment"># Forward propagation</span></span><br><span class="line">    a, caches = forward_propagation(X, parameters)</span><br><span class="line">    <span class="comment"># Compute cost.</span></span><br><span class="line">    cost = compute_cost(a, Y)</span><br><span class="line">    <span class="comment"># Backward propagation.</span></span><br><span class="line">    grads = backward_propagation(a, caches, parameters)</span><br><span class="line">    <span class="comment"># Update parameters.</span></span><br><span class="line">    parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Stochastic Gradient Descent</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, m):</span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(a, Y[:,j])</span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = backward_propagation(a, caches, parameters)</span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>
<p>  In Stochastic Gradient Descent, you use only 1 training example before updating the gradients.</p>
</li>
</ul>
<p>当训练集很大时，<code>SGD</code>算法更快。<code>But the parameters will &quot;oscillate&quot; toward the minimum rather than converge smoothly.</code>下图是说明：<img src="/images/deeplearning_ai_036.png" alt="SGD与GD"><code>+</code>表示代价函数的最小值。<code>SGD</code>达到收敛之前会有许多振荡，比起<code>GD</code>每一步的计算更快，因为每一次<code>SGD</code>只使用一个样本。<br><strong>注意</strong>：实现<code>SGD</code>算法需要\(3\)个<code>for</code>循环：</p>
<ol>
<li>Over the number of iterations</li>
<li>Over the \(m\) training examples</li>
<li>Over the layers (to update all parameters, from \((W^{[1]},b^{[1]})\) to \((W^{[L]},b^{[L]}))\)</li>
</ol>
<p>实际中，使用<code>mini-batch</code>梯度下降法（每个<code>mini-batch</code>使用训练样本个数为中间值，也就是既不使用整个训练集，也不使用单个样本，介于二者之间）会得到更快的结果。下图是说明：<img src="/images/deeplearning_ai_037.png" alt="SGD与mini-batch GD">在<code>SGD</code>，<code>GD</code>和<code>mini-batch GD</code>三者之间的主要差异是：<code>the number of examples you use to perform one update step.</code>通常来说当训练集很大时，<code>mini-batch</code>梯度下降法的效果更好。</p>
<h3 id="mini-batch-梯度下降法中-mini-batches的实现"><a href="#mini-batch-梯度下降法中-mini-batches的实现" class="headerlink" title="mini-batch 梯度下降法中 mini-batches的实现"></a>mini-batch 梯度下降法中 mini-batches的实现</h3><p>实现<code>mini-batches</code>主要有两步：<strong>将数据集打乱</strong>和<strong>按照既定大小划分数据集</strong>。</p>
<ul>
<li><strong>将数据集打乱(Shuffle)</strong>：如下图所示：<img src="/images/deeplearning_ai_038.png" alt="数据集打乱"><code>X</code>和<code>Y</code>的每一列表示一个训练样本。<code>X</code>和<code>Y</code>的随机打乱要<strong>同步</strong>，也就是说随机打乱后<code>X</code>的\(i^{th}\)列对应于<code>Y</code>中\(i^{th}\)个标签(<code>label</code>)（和打乱之前相对应）。这一步确保数据集被随机划分为不同的<code>mini-batches</code>。</li>
<li><strong>按照既定大小划分数据集(Partition)</strong>：将随机打乱后的数据集<code>X</code>和<code>Y</code>划分为大小为<code>mini_batch_size</code>的<code>mini-batches</code>。实际中训练样本集可能并不能保证每一个<code>mini_batches</code>的大小都是<code>mini-batch_size</code>，最后一个<code>mini_batch</code>可能更小，不过不用担心， 下图是说明：<img src="/images/deeplearning_ai_039.png" alt="数据集划分"></li>
</ul>
<h3 id="mini-batch-大小的选择"><a href="#mini-batch-大小的选择" class="headerlink" title="mini-batch 大小的选择"></a>mini-batch 大小的选择</h3><ul>
<li>当训练样本集比较小，如\(m &lt; 2000\)时，选择<code>batch</code>梯度下降法；</li>
<li>当训练样本集比较大时，选择<code>mini-batch</code>梯度下降法，一般<code>mini-batch</code>的大小为\(2\)的幂次方，典型的大小有\(2^6, 2^7, 2^8, 2^9\)；</li>
<li>注意<code>mini-batch</code>的大小要符合<code>CPU/GPU</code>的内存。</li>
</ul>
<h2 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h2><p><strong>指数加权平均(Exponentially Weight Average)</strong>的公式如下：$$S_t =\begin {cases}0 ,t=0 \\ \beta S_{t-1}+(1-\beta)Y_t ,t&gt;1 \end {cases}$$其中\(Y_t\)为\(t\)下的实际值，\(S_t\)为\(t\)下的加权平均后的值，\(\beta \)为权重值。<strong>\(\beta \)越大相当于利用之前的数据越多</strong>（下面是解释）。</p>
<h3 id="理解指数加权平均"><a href="#理解指数加权平均" class="headerlink" title="理解指数加权平均"></a>理解指数加权平均</h3><p>当\beta 为\(0.9\)时，有：$$v _{100}=0.9v _{99}+0.1\theta _{100}$$$$v _{99}=0.9v _{98}+0.1\theta _{99}$$$$v _{98}=0.9v _{97}+0.1\theta _{98}$$$$\ldots$$展开得：$$v _{100}=0.1\theta _{100}+0.1<em>0.9\theta _{99}+0.1</em> 0.9^2\theta _{98}+\ldots$$其中\(\theta _{i}\)表示第\(i\)个实际数据，且\(\theta \)前的系数之和为\(1\)（或近似为\(1\)）。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://pengzhendong.cn/2018/06/06/Optimization-algorithms/#more" target="_blank" rel="noopener">Randy’s Notes</a><br><a href="https://en.wikipedia.org/wiki/Moving_average" target="_blank" rel="noopener">Exponential moving average</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/27/01-神经网络和深度学习/" rel="next" title="01.神经网络和深度学习">
                <i class="fa fa-chevron-left"></i> 01.神经网络和深度学习
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/wukong.png"
                alt="穆义" />
            
              <p class="site-author-name" itemprop="name">穆义</p>
              <p class="site-description motion-element" itemprop="description">既已无岸，不必回头，唯有向前，踏碎云霄，放肆桀骜</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#深度学习的实用层面"><span class="nav-number">1.</span> <span class="nav-text">深度学习的实用层面</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据集划分：训练-验证-测试"><span class="nav-number">1.1.</span> <span class="nav-text">数据集划分：训练/验证/测试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#偏差-方差"><span class="nav-number">1.2.</span> <span class="nav-text">偏差/方差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化-regularization"><span class="nav-number">1.3.</span> <span class="nav-text">正则化(regularization)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic回归中正则化"><span class="nav-number">1.3.1.</span> <span class="nav-text">Logistic回归中正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络中的正则化"><span class="nav-number">1.3.2.</span> <span class="nav-text">神经网络中的正则化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化减小过拟合的原因"><span class="nav-number">1.4.</span> <span class="nav-text">正则化减小过拟合的原因</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dropout-正则化"><span class="nav-number">1.5.</span> <span class="nav-text">dropout 正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#理解-dropout"><span class="nav-number">1.6.</span> <span class="nav-text">理解 dropout</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他正则化方法"><span class="nav-number">1.7.</span> <span class="nav-text">其他正则化方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#归一化输入-Normalizing-inputs"><span class="nav-number">1.8.</span> <span class="nav-text">归一化输入(Normalizing inputs)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度消失和梯度爆炸-Vanishing-Exploding-gradients"><span class="nav-number">1.9.</span> <span class="nav-text">梯度消失和梯度爆炸(Vanishing / Exploding gradients)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度检验-Gradient-checking"><span class="nav-number">1.10.</span> <span class="nav-text">梯度检验(Gradient checking)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络使用梯度检验注意事项"><span class="nav-number">1.11.</span> <span class="nav-text">神经网络使用梯度检验注意事项</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#优化算法"><span class="nav-number">2.</span> <span class="nav-text">优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Mini-batch-梯度下降-Mini-batch-gradient-descent"><span class="nav-number">2.1.</span> <span class="nav-text">Mini-batch 梯度下降(Mini-batch gradient descent)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#mini-batch-梯度下降法中-mini-batches的实现"><span class="nav-number">2.1.1.</span> <span class="nav-text">mini-batch 梯度下降法中 mini-batches的实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mini-batch-大小的选择"><span class="nav-number">2.1.2.</span> <span class="nav-text">mini-batch 大小的选择</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#指数加权平均"><span class="nav-number">2.2.</span> <span class="nav-text">指数加权平均</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#理解指数加权平均"><span class="nav-number">2.2.1.</span> <span class="nav-text">理解指数加权平均</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考"><span class="nav-number">3.</span> <span class="nav-text">参考</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">穆义</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
