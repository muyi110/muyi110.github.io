<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Times New Roman:300,300italic,400,400italic,700,700italic|18:300,300italic,400,400italic,700,700italic|Courier New:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hello, World" />










<meta name="description" content="深度学习的实用层面数据集划分：训练/验证/测试 应用深度学习/机器学习是一个不断迭代的过程。  对于一个实际问题的样本数据，在建立模型的过程中，数据集会被划分为如下部分：  训练集：对模型进行训练，学习模型的参数； 验证集：用于交叉验证，选择最好的模型； 测试集：对学习到的模型进行测试，获取模型运行的无偏估计。  对小数据情况（如：100、1000、10000）数据集通常按照如下比例划分：">
<meta property="og:type" content="article">
<meta property="og:title" content="02.改善深层神经网络">
<meta property="og:url" content="https://muyi110.github.io/2018/08/01/02-改善深层神经网络/index.html">
<meta property="og:site_name" content="MuYi&#39;s Blog">
<meta property="og:description" content="深度学习的实用层面数据集划分：训练/验证/测试 应用深度学习/机器学习是一个不断迭代的过程。  对于一个实际问题的样本数据，在建立模型的过程中，数据集会被划分为如下部分：  训练集：对模型进行训练，学习模型的参数； 验证集：用于交叉验证，选择最好的模型； 测试集：对学习到的模型进行测试，获取模型运行的无偏估计。  对小数据情况（如：100、1000、10000）数据集通常按照如下比例划分：">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_028.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_029.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_030.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_031.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_032.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_033.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_034.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_035.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_036.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_037.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_038.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_039.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_040.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_041.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_042.png">
<meta property="og:updated_time" content="2018-08-07T01:53:39.421Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="02.改善深层神经网络">
<meta name="twitter:description" content="深度学习的实用层面数据集划分：训练/验证/测试 应用深度学习/机器学习是一个不断迭代的过程。  对于一个实际问题的样本数据，在建立模型的过程中，数据集会被划分为如下部分：  训练集：对模型进行训练，学习模型的参数； 验证集：用于交叉验证，选择最好的模型； 测试集：对学习到的模型进行测试，获取模型运行的无偏估计。  对小数据情况（如：100、1000、10000）数据集通常按照如下比例划分：">
<meta name="twitter:image" content="https://muyi110.github.io/images/deeplearning_ai_028.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://muyi110.github.io/2018/08/01/02-改善深层神经网络/"/>





  <title>02.改善深层神经网络 | MuYi's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">MuYi's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">此博客创建于2018-07-10</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-reprinted_article">
          <a href="/reprinted-article/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heart"></i> <br />
            
            转载文章
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://muyi110.github.io/2018/08/01/02-改善深层神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="穆义">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/wukong.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MuYi's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">02.改善深层神经网络</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-01T11:33:38+08:00">
                2018-08-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习课程-Andrew-Ng-学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习课程(Andrew Ng)学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h1 id="深度学习的实用层面"><a href="#深度学习的实用层面" class="headerlink" title="深度学习的实用层面"></a>深度学习的实用层面</h1><h2 id="数据集划分：训练-验证-测试"><a href="#数据集划分：训练-验证-测试" class="headerlink" title="数据集划分：训练/验证/测试"></a>数据集划分：训练/验证/测试</h2><blockquote>
<p>应用深度学习/机器学习是一个不断迭代的过程。</p>
</blockquote>
<p>对于一个实际问题的样本数据，在建立模型的过程中，数据集会被划分为如下部分：</p>
<ul>
<li>训练集：对模型进行<strong>训练</strong>，学习模型的参数；</li>
<li>验证集：用于<strong>交叉验证，选择最好的模型</strong>；</li>
<li>测试集：对学习到的模型进行测试，获取模型运行的无偏估计。</li>
</ul>
<p>对<strong>小数据情况</strong>（如：100、1000、10000）数据集通常按照如下比例划分：</p>
<ul>
<li>无验证集：70% / 30%；</li>
<li>有验证集：60% / 20% / 20%。</li>
</ul>
<p>对<strong>大数据情况</strong>，数据规模可能是百万级别的，验证集和测试集所占的比重会趋向于更小。验证集的目的是选择最好的模型，验证集只要足够大到验证大约2-10种模型，不需要20%数据集，例如在百万级别数据中，选1万数据作为验证集就行。大数据集的的划分可以按照如下划分（建议）：</p>
<ul>
<li>100万数据：98% / 1% / 1%；</li>
<li>更多数据：99.5% / 0.25% / 0.25%。</li>
</ul>
<p><strong>建议</strong>：要确保<strong>验证集</strong>和<strong>测试集</strong>的数据来自同一分布。</p>
<h2 id="偏差-方差"><a href="#偏差-方差" class="headerlink" title="偏差/方差"></a>偏差/方差</h2><p><img src="/images/deeplearning_ai_028.png" alt="偏差和方差">在深度学习（或机器学习）中，偏差和方差的解释如下：</p>
<ul>
<li>偏差：可通过<strong>训练误差(train set error)</strong>反映，当<strong>训练误差大时</strong>，说明当前模型<strong>偏差高</strong>，模型存在<strong>欠拟合</strong>；</li>
<li>方差：可通过<strong>验证误差(dev set error)</strong>反映，当<strong>验证误差大时</strong>，说明当前模型<strong>方差高</strong>，模型存在<strong>过拟合</strong>。</li>
</ul>
<p>好的模型应该是<strong>偏差和方差都小</strong>。<br><strong>存在高偏差</strong>：</p>
<ul>
<li>训练更大的网络（添加隐藏层或隐藏单元数目或更大的神经网络结构）；</li>
<li>增加训练时间（不一定有效）。</li>
</ul>
<p><strong>存在高方差</strong>：</p>
<ul>
<li>增加样本数据；</li>
<li>添加<strong>正则化(regularization)</strong>；</li>
<li>寻找更加合适的网络结构。</li>
</ul>
<h2 id="正则化-regularization"><a href="#正则化-regularization" class="headerlink" title="正则化(regularization)"></a>正则化(regularization)</h2><blockquote>
<p>深度学习可能存在<strong>过拟合问题–高方差</strong>，有两个解决方法，一个是<strong>正则化</strong>，一个是<strong>准备更多数据</strong>。<strong>正则化</strong>是在成本函数中添加一个正则化项，限制模型的复杂度。</p>
</blockquote>
<h3 id="Logistic回归中正则化"><a href="#Logistic回归中正则化" class="headerlink" title="Logistic回归中正则化"></a>Logistic回归中正则化</h3><p>对于<code>Logistic regression</code>，加入<code>L2</code>正则化（也称<code>L2</code>范数）的代价函数：$$J(w,b)=\dfrac{1}{m}\sum_{i=1}^{m}L(\widehat {y}^{(i)},y^{(i)})+\dfrac{\lambda}{2m}||w||_2$$其中<code>L2</code>正则化：$$\dfrac{\lambda}{2m}||w||_2 =\dfrac{\lambda}{2m}\sum _{j=1}^{n_x}w_j^2 =\dfrac{\lambda}{2m}w^T w$$有时也使用<code>L1</code>正则化：$$\dfrac{\lambda}{2m}||w||_1 =\dfrac{\lambda}{2m}\sum _{j=1}^{n_x}|w_j|$$上式中\(\lambda\)是<strong>正则化因子，是超参数（通常使用验证集或交叉验证集来配置这个参数）。</strong><br>如果用<code>L1</code>正则化，参数\(w\)最终会是<strong>稀疏</strong>的，就是说\(w\)中有很多\(0\)。在训练网络时，越来越倾向于使用<code>L2</code>正则化。<br><strong>注意</strong>：<code>lambda</code>在<code>Python</code>中属于保留字段，所有在编程时候，使用<code>lambd</code>代替正则化因子。</p>
<h3 id="神经网络中的正则化"><a href="#神经网络中的正则化" class="headerlink" title="神经网络中的正则化"></a>神经网络中的正则化</h3><p><strong>避免过拟合的标准的方式添加<code>L2</code>正则化项</strong>，需修改代价函数：$$J=-\dfrac{1}{m}\sum _{i=1}^{m}\Big ( y^{(i)}\log(a^{[L]\left (i\right )})+(1-y^{(i)})\log(1-a^{[L]\left (i\right )})\Big )$$为$$J _{regularized} =\underbrace {-\dfrac{1}{m} \sum _{i = 1}^{m} \Big (y^{(i)}\log(a^{[L]\left (i\right )}) + (1-y^{(i)})\log(1- a^{[L]\left (i\right )}) \Big )} _\text {cross-entropy cost} + \underbrace {\dfrac{\lambda}{2m} \sum _{l=1}^{L}\sum _{k=1}^{n^{[l-1]}}\sum _{j=1}^{n^{[l]}} W _{k,j}^{[l] 2}} _\text {L2 regularization cost}$$<br><strong>权重衰减(Weight decay)</strong><br><strong>对于神经网络，加入正则化项后，梯度变为（反向传播用到）：</strong>$$dW^{[l]}=\dfrac{\partial L}{\partial {W^{[l]}}}+\dfrac{\lambda}{m}W^{[l]}$$得到梯度更新公式：$$W^{[l]}=W^{[l]}-\alpha dW^{[l]}$$可得：$$W^{[l]}=W^{[l]}-\alpha \Big [\dfrac{\partial L}{\partial {W^{[l]}}}+\dfrac{\lambda}{m}W^{[l]}\Big ]$$$$  =W^{[l]}-\alpha \dfrac{\lambda}{m}W^{[l]}-\alpha \dfrac{\partial L}{\partial {W^{[l]}}}$$$$ =(1-\dfrac{\alpha \lambda}{m})W^{[l]}-\alpha \dfrac{\partial L}{\partial {W^{[l]}}}$$其中，由于\(1-\dfrac{\alpha \lambda}{m} &lt; 1\)，会给原来的\(W^{[l]}\)一个衰减系数，因此<code>L2</code>正则化称为<strong>权重衰减(Weight Decay)</strong>。</p>
<h2 id="正则化减小过拟合的原因"><a href="#正则化减小过拟合的原因" class="headerlink" title="正则化减小过拟合的原因"></a>正则化减小过拟合的原因</h2><p><strong>直观解释</strong><br><img src="/images/deeplearning_ai_029.png" alt="正则化减小过拟合">如果正则化\(\lambda\)设置的足够大，权重矩阵\(W\)会被设置接近\(0\)，直观理解是把隐藏单元的权重设置为\(0\)，于是基本上消除了这些隐藏单元的影响。会简化神经网络为小的网络，使得网络从过拟合的状态更接近左图的高偏差状态。<br>另一种解释<img src="/images/deeplearning_ai_030.png" alt="正则化减小过拟合的另一种解释">假设使用的激活函数是\(g(z)=\tanh(z)\)(<code>sigmoid</code>同理)，加入正则化项后，当\(\lambda\)增大时，使得\(W^{[l]}\)减小，\(Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\)会变小，根据上图可知，当\(z\)较小时，函数\(\tanh(z)\)近似线性，每层的激活函数近似线性使得整个网络进行线性，不会发生过拟合。</p>
<h2 id="dropout-正则化"><a href="#dropout-正则化" class="headerlink" title="dropout 正则化"></a>dropout 正则化</h2><p><img src="/images/deeplearning_ai_031.png" alt="dropout"><code>dropout</code>(随机失活)在每次迭代中随机关闭一些神经元，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。</p>
<p><div align="center"><video width="620" height="440" src="/images/dropout1.mp4" type="video/mp4" controls><br></video></div> </p>
<p><caption><center> Drop-out on the second hidden layer.</center></caption><br>At each iteration, you shut down (set to zero) each neuron of a layer with probability<code>1-keep_prob</code> or keep it with probability<code>keep_prob</code>(50% here). The dropped neurons don’t contribute to the training in both the forward and backward propagations of the iteration.</p>
<p><div align="center"><video width="620" height="440" src="/images/dropout2.mp4" type="video/mp4" controls><br></video></div> </p>
<p><caption><center> Drop-out on the first and third hidden layers. <br> \(1^{st}\) layer: we shut down on average 40% of the neurons.  \(3^{rd}\) layer: we shut down on average 20% of the neurons.</center></caption></p>
<blockquote>
<p>When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time.</p>
</blockquote>
<p><strong>反向随机失活(Inverted dropout)</strong><br><code>Inverted dropout</code>是最常用的实现<code>dropout</code>的方法。对第<code>1</code>层实现<code>dropout</code>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = <span class="number">0.8</span> <span class="comment">#设置神经元保留概率</span></span><br><span class="line">d1 = np.random.rand(a1.shape[<span class="number">0</span>], a1.shape[<span class="number">1</span>]) &lt; keep_prob</span><br><span class="line">a1 = np.multiply(a1, d1)</span><br><span class="line">a1 /= keep_prob</span><br><span class="line"><span class="comment"># 注：d1是一个布尔型数组，python会自动把True和False翻译为1和0</span></span><br></pre></td></tr></table></figure></p>
<p>在最后一步执行<code>a1 /= keep_prob</code>是因为\(a^{[1]}\)减少比例为<code>keep_prob</code>，也就是说\(a^{[1]}\)中有比例为<code>keep_prob</code>的元素归零，为了不影响下一层\(z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}\)的期望值，需要\(W^{[2]}a^{[1]}/keep\_prob\)。<br><strong>注意</strong>：在测试阶段不要使用dropout，因为那样会使预测结果变的随机。</p>
<h2 id="理解-dropout"><a href="#理解-dropout" class="headerlink" title="理解 dropout"></a>理解 dropout</h2><p>加入了<code>dropout</code>后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。通过传播过程，<code>dropout</code>将产生和<code>L2</code>正则化相同的收缩权重的效果。<br>对于不同的层，设置的<code>keep_prob</code>也不同。一般来说，神经元较少的层，会设<code>keep_prob</code>为<code>1.0</code>，而神经元多的层则会设置比较小的<code>keep_prob</code>。<br><code>dropout</code>的一大缺点是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用<code>dropout</code>时，先将<code>keep_prob</code>全部设置为<code>1.0</code>后运行代码，确保<code>J(w,b)</code>函数单调递减，再打开 <code>dropout</code>。</p>
<h2 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h2><ul>
<li><strong>数据扩增(Data Augmentation)</strong>：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。</li>
<li><strong>早停止法(Early Stopping)</strong>：将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，在两者开始发生较大偏差时及时停止迭代，避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。<img src="/images/deeplearning_ai_032.png" alt="早停止法"></li>
</ul>
<h2 id="归一化输入-Normalizing-inputs"><a href="#归一化输入-Normalizing-inputs" class="headerlink" title="归一化输入(Normalizing inputs)"></a>归一化输入(Normalizing inputs)</h2><p><img src="/images/deeplearning_ai_033.png" alt="归一化输入">包含两步：零均值化和归一化。公式如下：$$x=\dfrac{x-\mu }{\sigma }$$其中$$\mu = \dfrac{1}{m}\sum _{i=1}^{m}x^{(i)}$$$$\sigma = \sqrt {\dfrac{1}{m}\sum _{i=1}^{m}x^{(i) 2}}$$经过归一化后每个特征的方差都变为\(1\)。<strong>注意，测试集和训练集用相同的参数\(\mu \)和\(\sigma \)进行数据归一化</strong>。<br><strong>使用归一化的原因</strong><br><img src="/images/deeplearning_ai_034.png" alt="归一化原因">使用归一化后代价函数看起来更加对称。如果不使用归一化，需要设置一个非常小的学习率，因为初始值在某些位置时，梯度下降法可能需要更多次的迭代；如果归一化后，不论从哪个位置开始梯度下降法，可以更快找到最小值，可以在梯度下降中使用较大的步长。</p>
<h2 id="梯度消失和梯度爆炸-Vanishing-Exploding-gradients"><a href="#梯度消失和梯度爆炸-Vanishing-Exploding-gradients" class="headerlink" title="梯度消失和梯度爆炸(Vanishing / Exploding gradients)"></a>梯度消失和梯度爆炸(Vanishing / Exploding gradients)</h2><p>在梯度函数上出现的以指数级递增或者递减的情况分别称为<strong>梯度爆炸或者梯度消失</strong>。<br>假定\(g(z)=z, b^{[l]}=0\)，对应的输出为：$$\widehat y = W^{[L]}W^{[L-1]}\ldots W^{[2]}W^{[1]}$$</p>
<ul>
<li>当\(W^{[l]}\)值大于\(1\)时，激活函数值以指数级递增；</li>
<li>当\(W^{[l]}\)值小于\(1\)时，激活函数值以指数级递减。</li>
</ul>
<p><strong>对于导数同理</strong>。在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练难度上升，当梯度很小时，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。<br><strong>神经网络权重初始化</strong><br>对于出现的梯度消失或梯度爆炸问题，有一个不完整的解决方案（但有效）是<strong>谨慎选择随机初始化参数</strong>。<br>根据（只有一个神经元的网络，\(b=0\) ）：$$z=w_1 x_1 + w_2 x_2 +\ldots w_n x_n$$为预防\(z\)过大或过小，当\(n\)越大，则\(w_i\)应该越小。合理的方法是设置\(w_i =\dfrac{1}{n}\)其中\(n\)表示神经元的输入特征量（对于只有一个神经元的网络）。<br>对于有多层的神经网络，设置某层\(W^{[l]}\)的权重为:$$W^{[l]} = np.random.randn(W^{[l]}.shape) * np.sqrt(\dfrac {1}{n^{[l-1]}})$$其中\(n^{[l-1]}\)是第\(l-1\)层的神经元数量。<br><strong>如果某一层 \(l\) 的激活函数是 Relu 则初始化使用 np.sqrt( \(\dfrac{2}{n^{[l-1]}}\) )效果会更好；当某一层 \(l\) 的激活函数是 tanh 则初始化使用 np.sqrt( \(\dfrac{1}{n^{[l-1]}}\) )，有时也使用 np.sqrt( \(\dfrac{2}{n^{[l-1]}+ n^{[l]}}\) )</strong>。</p>
<h2 id="梯度检验-Gradient-checking"><a href="#梯度检验-Gradient-checking" class="headerlink" title="梯度检验(Gradient checking)"></a>梯度检验(Gradient checking)</h2><p>导数或梯度的定义如下（双边误差求导）：$$\dfrac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \dfrac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}$$进行梯度检验时需要连接参数，将参数\(W^{[1]}, b^{[1]}, \ldots , W^{[L]}, b^{[L]}\)全部连接，形成一个向量\(\theta \)，如下：$$J(W^{[1]}, b^{[1]}, \ldots , W^{[L]}, b^{[L]})=J(\theta )$$同时对\(dW^{[1]}, db^{[1]}, \ldots , dW^{[L]}, db^{[L]}\)执行同样的操作得到向量\(d\theta \)，其与\(\theta \)有相同的维度。<img src="/images/deeplearning_ai_035.png" alt="梯度检验">求一个梯度的逼近值：$$d\theta _\text {approx}[i]=\dfrac{J(\theta _1 ,\theta _2 ,\ldots ,\theta _i +\varepsilon ,\ldots )-J(\theta _1 ,\theta _2 ,\ldots ,\theta _i -\varepsilon ,\ldots )}{2\varepsilon}$$应该：$$\approx d\theta [i] =\dfrac{\partial J}{\partial \theta _i }$$因此，利用梯度检验值：$$\dfrac{||d\theta _\text {approx} -d\theta || _2}{||d\theta _\text {approx}|| _2 +||d\theta || _2}$$其中：$$||x || _2 =(\sum _{i=1}^{m}|x _i | ^2)^{\dfrac {1}{2}}$$表示<code>L2</code>范数。<br>如果梯度检验值和\(\varepsilon \)的值相近，说明神经网络的实施是正确的，否则要去检查代码是否存在<code>bug</code>。</p>
<h2 id="神经网络使用梯度检验注意事项"><a href="#神经网络使用梯度检验注意事项" class="headerlink" title="神经网络使用梯度检验注意事项"></a>神经网络使用梯度检验注意事项</h2><ol>
<li>不要在训练中使用梯度检验，它只用于调试（<code>debug</code>）。使用完毕关闭梯度检验的功能；</li>
<li>如果算法的梯度检验失败，要检查所有项，并试着找出<code>bug</code>，即确定哪个\(d\theta _\text {approx}[i]\)与<code>dθ</code>的值相差比较大；</li>
<li>当成本函数包含正则项时，也需要带上正则项进行检验；</li>
<li>梯度检验不能与<code>dropout</code>同时使用。因为每次迭代过程中，<code>dropout</code>会随机消除隐藏层单元的不同子集，难以计算<code>dropout</code>在梯度下降上的成本函数<code>J</code>。建议关闭  <code>dropout</code>，用梯度检验进行双重检查，确定在没有<code>dropout</code>的情况下算法正确，然后打开<code>dropout</code>；</li>
</ol>
<h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><blockquote>
<p>机器学习是一个经验过程，伴随大量的迭代过程，优化算法可以帮助快速训练模型。</p>
</blockquote>
<h2 id="Mini-batch-梯度下降-Mini-batch-gradient-descent"><a href="#Mini-batch-梯度下降-Mini-batch-gradient-descent" class="headerlink" title="Mini-batch 梯度下降(Mini-batch gradient descent)"></a>Mini-batch 梯度下降(Mini-batch gradient descent)</h2><p><strong>batch 梯度下降法</strong>（批梯度下降法）：同时处理整个训练集，当训练集很大时，处理速度会变慢。<br><strong>Mini-batch 梯度下降</strong>（小批量梯度下降法）：每次同时处理单个<code>mini-batch</code>(训练数据集的一部分)，其他与<code>batch</code>梯度下降法一致。<br>使用<code>batch</code>梯度下降法，对整个训练集的一次遍历只能做一次梯度下降；使用<code>mini-batch</code>梯度下降法对整个训练集的一次遍历（称为一个<code>epoch</code>）可以做<code>mini-batch</code>个数个梯度下降。之后可多次遍历数据集，直到收敛到一个合适的精度。<br>考虑一个极端情况，当<code>mini-batch</code>个数为\(1\)（一个<code>mini-batch</code>只包含一个样本），对应的算法称为随机梯度下降法(<code>Stochastic Gradient Descent--SGD</code>)。对比于<code>batch</code>下降法的区别主要是：每一次的梯度计算仅仅针对一个训练样本，而不是整个训练集( What changes is that you would be computing gradients on just one training example at a time, rather than on the whole training set )。下面的程序说明了<code>SGD</code>和<code>GD</code>算法的差异：</p>
<ul>
<li><p><strong>(Batch) Gradient Descent</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="comment"># Forward propagation</span></span><br><span class="line">    a, caches = forward_propagation(X, parameters)</span><br><span class="line">    <span class="comment"># Compute cost.</span></span><br><span class="line">    cost = compute_cost(a, Y)</span><br><span class="line">    <span class="comment"># Backward propagation.</span></span><br><span class="line">    grads = backward_propagation(a, caches, parameters)</span><br><span class="line">    <span class="comment"># Update parameters.</span></span><br><span class="line">    parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Stochastic Gradient Descent</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, m):</span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(a, Y[:,j])</span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = backward_propagation(a, caches, parameters)</span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>
<p>  In Stochastic Gradient Descent, you use only 1 training example before updating the gradients.</p>
</li>
</ul>
<p>当训练集很大时，<code>SGD</code>算法更快。<code>But the parameters will &quot;oscillate&quot; toward the minimum rather than converge smoothly.</code>下图是说明：<img src="/images/deeplearning_ai_036.png" alt="SGD与GD"><code>+</code>表示代价函数的最小值。<code>SGD</code>达到收敛之前会有许多振荡，比起<code>GD</code>每一步的计算更快，因为每一次<code>SGD</code>只使用一个样本。<br><strong>注意</strong>：实现<code>SGD</code>算法需要\(3\)个<code>for</code>循环：</p>
<ol>
<li>Over the number of iterations</li>
<li>Over the \(m\) training examples</li>
<li>Over the layers (to update all parameters, from \((W^{[1]},b^{[1]})\) to \((W^{[L]},b^{[L]}))\)</li>
</ol>
<p>实际中，使用<code>mini-batch</code>梯度下降法（每个<code>mini-batch</code>使用训练样本个数为中间值，也就是既不使用整个训练集，也不使用单个样本，介于二者之间）会得到更快的结果。下图是说明：<img src="/images/deeplearning_ai_037.png" alt="SGD与mini-batch GD">在<code>SGD</code>，<code>GD</code>和<code>mini-batch GD</code>三者之间的主要差异是：<code>the number of examples you use to perform one update step.</code>通常来说当训练集很大时，<code>mini-batch</code>梯度下降法的效果更好。</p>
<h3 id="mini-batch-梯度下降法中-mini-batches的实现"><a href="#mini-batch-梯度下降法中-mini-batches的实现" class="headerlink" title="mini-batch 梯度下降法中 mini-batches的实现"></a>mini-batch 梯度下降法中 mini-batches的实现</h3><p>实现<code>mini-batches</code>主要有两步：<strong>将数据集打乱</strong>和<strong>按照既定大小划分数据集</strong>。</p>
<ul>
<li><strong>将数据集打乱(Shuffle)</strong>：如下图所示：<img src="/images/deeplearning_ai_038.png" alt="数据集打乱"><code>X</code>和<code>Y</code>的每一列表示一个训练样本。<code>X</code>和<code>Y</code>的随机打乱要<strong>同步</strong>，也就是说随机打乱后<code>X</code>的\(i^{th}\)列对应于<code>Y</code>中\(i^{th}\)个标签(<code>label</code>)（和打乱之前相对应）。这一步确保数据集被随机划分为不同的<code>mini-batches</code>。</li>
<li><strong>按照既定大小划分数据集(Partition)</strong>：将随机打乱后的数据集<code>X</code>和<code>Y</code>划分为大小为<code>mini_batch_size</code>的<code>mini-batches</code>。实际中训练样本集可能并不能保证每一个<code>mini_batches</code>的大小都是<code>mini-batch_size</code>，最后一个<code>mini_batch</code>可能更小，不过不用担心， 下图是说明：<img src="/images/deeplearning_ai_039.png" alt="数据集划分"></li>
</ul>
<h3 id="mini-batch-大小的选择"><a href="#mini-batch-大小的选择" class="headerlink" title="mini-batch 大小的选择"></a>mini-batch 大小的选择</h3><ul>
<li>当训练样本集比较小，如\(m &lt; 2000\)时，选择<code>batch</code>梯度下降法；</li>
<li>当训练样本集比较大时，选择<code>mini-batch</code>梯度下降法，一般<code>mini-batch</code>的大小为\(2\)的幂次方，典型的大小有\(2^6, 2^7, 2^8, 2^9\)；</li>
<li>注意<code>mini-batch</code>的大小要符合<code>CPU/GPU</code>的内存。</li>
</ul>
<p>建议：编程实现 <code>mini-batch</code> 算法时，每一代( <code>epoch</code> )迭代都应该重新打乱数据集，再划分 <code>mini-batch</code> 。具体讨论参考<a href="https://stats.stackexchange.com/questions/235844/should-training-samples-randomly-drawn-for-mini-batch-training-neural-nets-be-dr" target="_blank" rel="noopener">这儿</a><a href="https://stats.stackexchange.com/questions/242004/why-do-neural-network-researchers-care-about-epochs" target="_blank" rel="noopener">和这儿</a>。</p>
<h2 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h2><p><strong>指数加权平均(Exponentially Weight Average)</strong>的公式如下：$$S_t =\begin {cases}0 ,t=0 \\ \beta S_{t-1}+(1-\beta)Y_t ,t&gt;1 \end {cases}$$其中\(Y_t\)为\(t\)下的实际值，\(S_t\)为\(t\)下的加权平均后的值，\(\beta \)为权重值。<strong>\(\beta \)越大相当于利用之前的数据越多</strong>（下面是解释）。</p>
<h3 id="理解指数加权平均-EMA"><a href="#理解指数加权平均-EMA" class="headerlink" title="理解指数加权平均(EMA)"></a>理解指数加权平均(EMA)</h3><p>当\(\beta \)为\(0.9\)时，有：$$v _{100}=0.9v _{99}+0.1\theta _{100}$$$$v _{99}=0.9v _{98}+0.1\theta _{99}$$$$v _{98}=0.9v _{97}+0.1\theta _{98}$$$$\ldots$$展开得：$$v _{100}=0.1\theta _{100}+ 0.1\times 0.9\theta _{99}+ 0.1\times 0.9^2\theta _{98}+\ldots$$其中\(\theta _{i}\)表示第\(i\)个实际数据，且\(\theta \)前的系数之和为\(1\)（或近似为\(1\)）。越旧的数据权值越小，如何给\(\beta \)一个直观的感觉呢？当权值\(\beta ^n\)小于\(\dfrac {1}{e}\)就可以说只关注了前\(n\)个数据。<br>根据公式：$$\lim _{\beta \to 0}(1-\beta )^{\dfrac {1}{\beta }}=\dfrac {1}{e}\approx 0.368$$可将\(\dfrac {1}{1-\beta}\)看成对过去多少个数据进行加权平均（近似）。</p>
<ul>
<li><strong>杂谈：如何理解当权值\(\beta ^n\)小于\(\dfrac {1}{e}\)就可以说只关注了前\(n\)个数据？</strong><blockquote>
<p>根据指数加权<code>(EMA)</code>的公式可得到如下公式：$$EMS _\text {today}=\alpha [p _1 + (1-\alpha )p _2 +(1-\alpha )^{2}p _3 + \ldots]$$其中\(p _1\)是\(\text {value} _\text {today}\)，\(p _2\)是\(\text {value} _\text {yesterday}\)，以此类推。<br>进而可以得到：$$EMS _\text {today}=\dfrac {p _1 + (1-\alpha )p _2 +(1-\alpha )^{2}p _3 + \ldots}{1+(1-\alpha )+(1-\alpha )^2 +\ldots}$$考虑：$$\dfrac {1}{\alpha}=1+(1-\alpha )+(1-\alpha )^2 +\ldots$$下面考虑\(k\)天以后被忽略的权重所占的比重。<br>被<strong>忽略</strong>的权重之和为：$$\alpha [(1-\alpha )^{k} +(1-\alpha )^{k+1} +(1-\alpha )^{k+2} +\ldots ]$$$$=\alpha (1-\alpha )^{k}[1+(1-\alpha )+(1-\alpha )^2 +\ldots ]$$被忽略的权重之和所占的比例为：$$\begin{align} &amp; \dfrac {\text {weight omitted by stopping after k terms}}{\text {total weight}}\\ ={} &amp; \dfrac {\alpha [(1-\alpha )^{k} +(1-\alpha )^{k+1} +(1-\alpha )^{k+2} +\ldots ]}{\alpha [1+(1-\alpha )+(1-\alpha )^2 +\ldots ]}\\ ={} &amp; (1-\alpha )^k \end{align}$$例如，如果我们要获取\(1-\dfrac {1}{e}\)的权重（也就是说用到的数据所占的权重占\((1-\dfrac {1}{e}\)），需要设置：$$(1-\alpha )^k =\dfrac {1}{e}$$解得$$k=\dfrac {\ln {(e^{-1})}}{\ln {(1-\alpha )}}\approx \dfrac {-1}{-\alpha}=\dfrac {1}{\alpha}$$其中分母\(\ln {(1-\alpha)}\)用泰勒公式展开获得近似。<br>(注：这个杂谈，各位看官只当看个笑话就行，我自己都没信服，如果有好的解释，麻烦联系我并告知，将不胜感激）</p>
</blockquote>
</li>
</ul>
<p>指数加权平均优点：只需要一行代码（不断覆盖旧值），占用内存少，效率高。</p>
<h3 id="指数加权平均偏差修正"><a href="#指数加权平均偏差修正" class="headerlink" title="指数加权平均偏差修正"></a>指数加权平均偏差修正</h3><p>通常有：$$v _0 = 0$$$$v _1 =0.98 v _0 + 0.02\theta _1$$因此，可知\(v _1\)仅为第一个数据的\(0.02\)（或者\(1-\beta \)），显示偏差较大。修改公式为：$$v _t =\dfrac{\beta v _{t-1}+(1-\beta )\theta _1}{1-\beta ^t}$$当\(t\)增大时，\(\beta ^t \to 0\)，所示当\(t\)较大时，修正偏差公式基本没作用，但在前期会帮助更好预测偏差。实际中，一般会忽略前期偏差的影响。</p>
<h2 id="动量梯度下降法-Gradient-Descent-with-Momentum"><a href="#动量梯度下降法-Gradient-Descent-with-Momentum" class="headerlink" title="动量梯度下降法(Gradient Descent with Momentum)"></a>动量梯度下降法(Gradient Descent with Momentum)</h2><blockquote>
<p>动量梯度下降法(Gradient Descent with Momentum)，运行速度几乎总是快于标准的梯度下降法，基本思想是计算梯度的指数加权平均，并利用该梯度更新权重。</p>
</blockquote>
<p>动量梯度下降法的更新公式为(\(l =1, \ldots ,L)\)：$$ \begin{cases}v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \\W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}} \\ v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \\b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}} \end{cases}$$<img src="/images/deeplearning_ai_040.png" alt="动量梯度下降一">进行一般的梯度下降将会得到图中的蓝色曲线，存在上下波动，减缓了梯度下降的速度，只能用一个较小的学习率进行迭代。如果用较大的学习率，结果可能会像紫色曲线一样偏离函数的范围。<br>动量梯度下降法，通过累加过去的梯度值来减少抵达最小值路径上的波动，加速了收敛，在横轴方向下降得更快，从而得到图中红色的曲线。<strong>当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。</strong><img src="/images/deeplearning_ai_041.png" alt="动量梯度下降二"></p>
<h3 id="如何选择超参数-beta"><a href="#如何选择超参数-beta" class="headerlink" title="如何选择超参数\(\beta \)"></a>如何选择超参数\(\beta \)</h3><ul>
<li>The larger the momentum \(\beta \) is, the smoother the update because the more we take the past gradients into account. But if \(\beta \) is too big, it could also smooth out the updates too much. </li>
<li>Common values for \(\beta \) range from \(0.8\) to \(0.999\). If you don’t feel inclined to tune this, \(\beta =0.9\) is often a reasonable default.</li>
<li>Tuning the optimal \(\beta \) for your model might need trying several values to see what works best in term of reducing the value of the cost function J.</li>
</ul>
<h2 id="RMSProp-算法"><a href="#RMSProp-算法" class="headerlink" title="RMSProp 算法"></a>RMSProp 算法</h2><p><code>RMSProp</code> 算法的更新公式为：$$s_{dW}=\beta s_{dW}+ (1-\beta )dW ^2$$$$s_{db}=\beta s_{db}+ (1-\beta )db ^2$$$$W=W-\alpha \dfrac {dW}{\sqrt {s_{dW}}+\varepsilon }$$$$b=b-\alpha \dfrac {db}{\sqrt {s_{db}}+\varepsilon }$$其中\(\varepsilon \)是一个实际操作（编程）添加的较小的数（例如\(10^{-8}\)为了防止分母太小导致数值不稳定。（上式中\(dW ^2\)操作是<strong>逐元素</strong>操作）</p>
<h2 id="Adam-算法"><a href="#Adam-算法" class="headerlink" title="Adam 算法"></a>Adam 算法</h2><blockquote>
<p>Adam 优化算法( Adaptive Moment Estimation )基本上是将 <code>Momentum</code> 和 <code>RMSProp</code> 算法结合在一起。在神经网络中是最有效的优化算法之一。</p>
</blockquote>
<p><code>Adam</code>算法工作流程如下：</p>
<ol>
<li>It calculates an exponentially weighted average of past gradients, and stores it in variables \(v\) (before bias correction) and \(v^{\text {corrected}}\) (with bias correction). </li>
<li>It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables \(s\) (before bias correction) and \(s^{\text {corrected}}\) (with bias correction).</li>
<li>It updates parameters in a direction based on combining information from “1” and “2”.</li>
</ol>
<p>更新公式如下( \(l=1, \ldots, L)\)：$$\begin{align} \begin{cases}v _{dW ^{[l]}}=\beta _{1}v _{dW ^{[l]}}+(1-\beta _{1})\dfrac{\partial {J}}{\partial {W^{[l]}}} \\ v _{dW^{[l]}} ^{\text {corrected}} = \dfrac {v _{dW ^{[l]}}}{1 - (\beta _{1}) ^t } \\ s _{dW ^{[l]}} = \beta _{2} s _{dW ^{[l]}} + (1 - \beta _{2}) (\dfrac{\partial {J} }{\partial {W ^{[l]}} }) ^2 \\ s _{dW ^{[l]}} ^{\text {corrected}} = \dfrac {s _{dW ^{[l]}}}{1 - (\beta _{2}) ^t} \\ W ^{[l]} = W ^{[l]} - \alpha \dfrac {v _{dW ^{[l]}} ^{\text {corrected}} }{\sqrt {s _{dW ^{[l]}} ^{\text {corrected}} }+ \varepsilon }\end{cases}&amp; &amp;\begin{cases}v _{db ^{[l]}}=\beta _{1}v _{db ^{[l]}}+(1-\beta _{1})\dfrac{\partial {J}}{\partial {b^{[l]}}} \\ v _{db^{[l]}} ^{\text {corrected}} = \dfrac {v _{db ^{[l]}}}{1 - (\beta _{1}) ^t } \\ s _{db ^{[l]}} = \beta _{2} s _{db ^{[l]}} + (1 - \beta _{2}) (\dfrac{\partial {J} }{\partial {b ^{[l]}} }) ^2 \\ s _{db ^{[l]}} ^{\text {corrected}} = \dfrac {s _{db ^{[l]}}}{1 - (\beta _{2}) ^t} \\ b ^{[l]} = b ^{[l]} - \alpha \dfrac {v _{db ^{[l]}} ^{\text {corrected}} }{\sqrt {s _{db ^{[l]}} ^{\text {corrected}} }+ \varepsilon }\end{cases}\end{align}$$其中 \(t\) 表示 <code>Adam</code> 算法 <code>the number of steps</code>。初始化设置如下：$$v _{dW ^{[l]}} =0, s _{dW ^{[l]}} =0, v _{db ^{[l]}} =0, s _{db ^{[l]}} =0$$</p>
<h3 id="超参数选择"><a href="#超参数选择" class="headerlink" title="超参数选择"></a>超参数选择</h3><ul>
<li>\(\beta _1\)常用默认值 \(0.9\)；</li>
<li>\(\beta _2\)常用默认值 \(0.999\)；</li>
<li>\(\varepsilon \)不重要，不会影响算法表现，<code>Adam</code> 算法的作者建议为 \(10^{-8}\)。</li>
</ul>
<h2 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h2><p>如果设置一个固定的学习率 \(\alpha \) ，在最小值点附近，由于不同的 <code>batch</code> 中存在一定的噪声，因此不会精确收敛，而是始终在最小值周围一个较大的范围内波动。<br>而如果随着时间慢慢减少学习率 \(\alpha \) ，在初期 \(\alpha \) 较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小 \(\alpha \) ，即减小步长，有助于算法的收敛，更容易接近最优解。<br>最常用的学习率衰减方法：$$\alpha =\dfrac {1}{1+ \text {decay_rate * epoch_num}} \times {\alpha _0}$$其中，<code>decay_rate</code> 为衰减率（超参数），<code>epoch_num</code>为遍历数据集的代数。还有其他的衰减方法，如指数衰减：$$\alpha =0.95^{\text {epoch_num}}\times \alpha _0$$其他方法可以参考视频教程。<br>注：实际中学习率衰减并不首先考虑。</p>
<h1 id="超参数调试、Batch-归一化和编程框架"><a href="#超参数调试、Batch-归一化和编程框架" class="headerlink" title="超参数调试、Batch 归一化和编程框架"></a>超参数调试、Batch 归一化和编程框架</h1><h2 id="超参数调试"><a href="#超参数调试" class="headerlink" title="超参数调试"></a>超参数调试</h2><h3 id="超参数重要性排序如下（参考）"><a href="#超参数重要性排序如下（参考）" class="headerlink" title="超参数重要性排序如下（参考）"></a>超参数重要性排序如下（参考）</h3><ul>
<li><strong>最重要</strong>：学习率 \(\alpha \)；</li>
<li><strong>次重要</strong>：动量梯度下降法(Gradient Descent with Momentum)中的 \(\beta \)，常设置为 \(0.9\)；各隐藏层神经元数 <code>hidden units</code>；<code>mini-batch size</code>。</li>
<li><strong>再次重要</strong>：<code>Adam</code>优化算法超参数 \(\beta _1 \)，\(\beta _2 \)，\(\varepsilon \)，常设为 \(0.9\)、\(0.999\)、\(10 ^{-8}\)；神经网络层数 <code>layers</code>；学习衰减率 <code>decay-rate</code>。</li>
</ul>
<h3 id="调参技巧（建议）"><a href="#调参技巧（建议）" class="headerlink" title="调参技巧（建议）"></a>调参技巧（建议）</h3><ul>
<li><strong>随机选择点</strong>：在超参数组成的参数空间随机选择点进行实验；</li>
<li>由粗糙到精细：先粗略确定效果不错的小区域，再在小区域上精细实验。</li>
</ul>
<h3 id="给超参数选择合适的范围"><a href="#给超参数选择合适的范围" class="headerlink" title="给超参数选择合适的范围"></a>给超参数选择合适的范围</h3><ul>
<li><p>对于学习率 \(\alpha \)，用<strong>对数标尺(log)</strong>而不是线性标尺。例如 \(\alpha \)的范围是 \(0.0001\) 到 \(1\)，利用对数标尺，在 <code>python</code> 中实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r = <span class="number">-4</span> * np.random.rand()</span><br><span class="line">a = np.power(<span class="number">10</span>, r)</span><br></pre></td></tr></table></figure>
</li>
<li><p>对于 \(\beta\)（动量梯度下降法参数），可以考虑随机选取 \(1-\beta \)值。因为当 \(\beta \) 接近 \(1\) 时，即使 \(\beta \)有微小的变化，对结果的影响较大（当 \(\beta \) 取 \(0.9\) 相当于在 \(10\) 个值取平均，当 \(\beta \) 取 \(0.999\) 相当于在 \(1000\) 个值中取平均，\(\beta \)从 \(0.9\) 增加到 \(0.9005\) 对 \(\dfrac {1}{1-\beta }\) 基本没影响，但 \(\beta \) 从 \(0.999\) 增加到 \(0.9995\) 对 \(\dfrac {1}{1-\beta }\) 影响较大）</p>
</li>
</ul>
<h3 id="超参数调试实践"><a href="#超参数调试实践" class="headerlink" title="超参数调试实践"></a>超参数调试实践</h3><ul>
<li>考虑数据变化或其他因素，建议每隔几个月重新评估超参数；</li>
<li>更具计算资源，训练模型方式：<code>panda</code> (熊猫方式）与 <code>caviar</code> (鱼子酱方式)。</li>
</ul>
<h2 id="Batch-归一化-Batch-Normalization"><a href="#Batch-归一化-Batch-Normalization" class="headerlink" title="Batch 归一化(Batch Normalization)"></a>Batch 归一化(Batch Normalization)</h2><blockquote>
<p>Batch 归一化 (Batch Normalization，简称 BN) 会使参数搜索问题变得容易，使神经网络对参数的选择更加稳定，超参数范围会更庞大，工作效果也很好，会使训练更容易。</p>
</blockquote>
<p>之前的讨论中，对输入特征 <code>X</code> 进行了归一化处理。同样的思想可以用于处理隐藏层的激活值 \(a ^{[l]}\) ，从而可以加速参数 \(W ^{[l+1]}\) 和 \(b ^{[l+1]}\) 的训练。在实践中，经常选择归一化 \(Z ^{[l]}\)。公式如下：$$\mu =\dfrac {1}{m}\sum _{i=1} ^{m}z ^{(i)}$$$$\sigma ^{2} =\dfrac {1}{m}\sum _{i=1} ^{m}(z _i -\mu ) ^2$$$$z _{\text {norm}} ^{(i)} =\dfrac {z ^{(i)}-\mu }{\sqrt {\sigma ^2 +\varepsilon }}$$其中 \(m\) 为一个 <code>mini-batch</code> 包含的样本数，\(\varepsilon \) 是为了防止分母为 \(0\)，常设为 \(10 ^{-8} \)。<br>归一化后，使得输入 \(Z ^{[l]}\) （针对一个 <code>mini-batch</code> ）变为均值为 \(0\) ，方差为 \(1\) 。<strong>有时可能不想隐藏单元总是均值为 \(0\) ，方差为 \(1\)</strong>，因此可以计算：$$\widetilde {z} ^{(i)} =\gamma z _{\text {norm}} ^{(i)} +\beta$$其中 \(\gamma \) 和 \(\beta \) 是模型的参数，需要学习。可以像更新神经网络的权重一样，用梯度下降法更新 \(\gamma \) 和 \(\beta \) 的值。<br>通过对 \(\gamma \) 和 \(\beta \) 的合理设置，可让 \(\widetilde {z} ^{(i)} \) 的均值和方差为任意值。在隐藏层，用 \(\widetilde {z} ^{(i)}\) 代替 \(z ^{(i)}\) 。<br>注：设置 \(\gamma \) 和 \(\beta \) 的原因是，如果各隐藏层的输入均值在靠近 \(0\) 的区域，即处于激活函数的线性区域，不利于训练非线性神经网络，从而得到效果较差的模型。</p>
<h3 id="Batch-归一化-Batch-Normalization-用于神经网络"><a href="#Batch-归一化-Batch-Normalization-用于神经网络" class="headerlink" title="Batch 归一化(Batch Normalization)用于神经网络"></a>Batch 归一化(Batch Normalization)用于神经网络</h3><p><img src="/images/deeplearning_ai_042.png" alt="BN用于神经网络"><code>Batch Normalization</code> 经常使用在 <code>mini-batch</code> 上。在<code>Batch Normalization</code> 归一化时，参数中的 \(b\) 实际没有起到作用（归一化包括减去均值，常量不起作用），实际操作可以将参数 \(b\) 设置为 \(0\) 。<br>可以用梯度下降法、<code>Adam</code> 算法、<code>RMSProp</code> 算法、动量下降法等对参数 \(W ^{[l]}\)，\(\beta ^{[l]}\)，\(\gamma ^{[l]}\)进行更新。</p>
<h3 id="BN有效的解释"><a href="#BN有效的解释" class="headerlink" title="BN有效的解释"></a>BN有效的解释</h3><p><code>Batch</code> 归一化(<code>Batch Normalization</code>)效果好的原因主要有以下：</p>
<ol>
<li>通过对隐藏层各神经元的输入做类似的归一化处理，提高神经网络训练速度；</li>
<li>使得前面层的权重变化对后面层的影响减弱，整体网络更加健壮。</li>
</ol>
<p>关于上面第二点的解释：<br>实际中，如果训练样本和测试样本的分布不同（称之为<code>Covariate Shift</code>）使用 <code>Batch Normalization</code> 可以减小 <code>Covariate Shift</code> 带来的影响，让模型变得更具有鲁棒性。因为即使输入值改变了（不同的分布），由于 <code>Batch Normalization</code> 的作用，使得均值和方差不变（由参数 \(\gamma \) 和 \(\beta \) 决定），限制了输入变化对网络的影响，让学习变得容易些。<code>Batch Normalization</code> 让各层参数 \(W\) 和 \(b\) 间的耦合性降低，各层相对独立。<br>另外，<code>Batch Normalization</code> 还有轻微正则化作用。因为在每个 <code>mini-batch</code> 而非整个数据集上计算均值和方差，只由这一小部分数据估计得出的均值和方差会有一些噪声。因此最终计算出的 \(\widetilde {z} ^{(i)}\) 也有一定噪声，类似于 <code>dropout</code>，这种噪声会使得神经元不再特别依赖于任何一个输入特征。<br><code>Batch Normalization</code> 可以和 <code>dropout</code> 一起使用，以获得更强大的正则化效果。不要将 <code>Batch Normalization</code> 作为正则化的手段，而是当作加速学习的方式。</p>
<h3 id="测试阶段的-Batch-归一化"><a href="#测试阶段的-Batch-归一化" class="headerlink" title="测试阶段的 Batch 归一化"></a>测试阶段的 Batch 归一化</h3><p>使用之前的<strong>指数加权平均</strong>的方法来预测测试过程单个样本的 \(\mu \) 和 \(\sigma ^2 \)。<br>具体就是：对于第 <code>l</code> 层隐藏层，考虑所有 <code>mini-batch</code> 在该隐藏层下的 \(\mu ^{[l]}\) 和 \(\sigma ^{2[l]} \) ，然后用指数加权平均的方式来预测当前单个样本的 \(\mu ^{[l]}\) 和 \(\sigma ^{2[l]} \) 。这样实现了对测试过程单个样本的均值和方差估计。</p>
<h2 id="Softmax-回归"><a href="#Softmax-回归" class="headerlink" title="Softmax 回归"></a>Softmax 回归</h2><p>可用于多分类问题。<a href="http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92" target="_blank" rel="noopener">Softmax</a></p>
<h2 id="编程框架-Tensorflow"><a href="#编程框架-Tensorflow" class="headerlink" title="编程框架 Tensorflow"></a>编程框架 Tensorflow</h2><p><a href="https://www.tensorflow.org/" target="_blank" rel="noopener">官方文档</a></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://pengzhendong.cn/2018/06/06/Optimization-algorithms/#more" target="_blank" rel="noopener">Randy’s Notes</a><br><a href="https://en.wikipedia.org/wiki/Moving_average" target="_blank" rel="noopener">Exponential moving average</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/27/01-神经网络和深度学习/" rel="next" title="01.神经网络和深度学习">
                <i class="fa fa-chevron-left"></i> 01.神经网络和深度学习
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/08/08/03-构建机器学习项目/" rel="prev" title="03.构建机器学习项目">
                03.构建机器学习项目 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/wukong.png"
                alt="穆义" />
            
              <p class="site-author-name" itemprop="name">穆义</p>
              <p class="site-description motion-element" itemprop="description">既已无岸，不必回头，唯有向前，踏碎云霄，放肆桀骜</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#深度学习的实用层面"><span class="nav-number">1.</span> <span class="nav-text">深度学习的实用层面</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据集划分：训练-验证-测试"><span class="nav-number">1.1.</span> <span class="nav-text">数据集划分：训练/验证/测试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#偏差-方差"><span class="nav-number">1.2.</span> <span class="nav-text">偏差/方差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化-regularization"><span class="nav-number">1.3.</span> <span class="nav-text">正则化(regularization)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic回归中正则化"><span class="nav-number">1.3.1.</span> <span class="nav-text">Logistic回归中正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络中的正则化"><span class="nav-number">1.3.2.</span> <span class="nav-text">神经网络中的正则化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化减小过拟合的原因"><span class="nav-number">1.4.</span> <span class="nav-text">正则化减小过拟合的原因</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dropout-正则化"><span class="nav-number">1.5.</span> <span class="nav-text">dropout 正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#理解-dropout"><span class="nav-number">1.6.</span> <span class="nav-text">理解 dropout</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他正则化方法"><span class="nav-number">1.7.</span> <span class="nav-text">其他正则化方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#归一化输入-Normalizing-inputs"><span class="nav-number">1.8.</span> <span class="nav-text">归一化输入(Normalizing inputs)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度消失和梯度爆炸-Vanishing-Exploding-gradients"><span class="nav-number">1.9.</span> <span class="nav-text">梯度消失和梯度爆炸(Vanishing / Exploding gradients)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度检验-Gradient-checking"><span class="nav-number">1.10.</span> <span class="nav-text">梯度检验(Gradient checking)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络使用梯度检验注意事项"><span class="nav-number">1.11.</span> <span class="nav-text">神经网络使用梯度检验注意事项</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#优化算法"><span class="nav-number">2.</span> <span class="nav-text">优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Mini-batch-梯度下降-Mini-batch-gradient-descent"><span class="nav-number">2.1.</span> <span class="nav-text">Mini-batch 梯度下降(Mini-batch gradient descent)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#mini-batch-梯度下降法中-mini-batches的实现"><span class="nav-number">2.1.1.</span> <span class="nav-text">mini-batch 梯度下降法中 mini-batches的实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mini-batch-大小的选择"><span class="nav-number">2.1.2.</span> <span class="nav-text">mini-batch 大小的选择</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#指数加权平均"><span class="nav-number">2.2.</span> <span class="nav-text">指数加权平均</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#理解指数加权平均-EMA"><span class="nav-number">2.2.1.</span> <span class="nav-text">理解指数加权平均(EMA)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指数加权平均偏差修正"><span class="nav-number">2.2.2.</span> <span class="nav-text">指数加权平均偏差修正</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#动量梯度下降法-Gradient-Descent-with-Momentum"><span class="nav-number">2.3.</span> <span class="nav-text">动量梯度下降法(Gradient Descent with Momentum)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#如何选择超参数-beta"><span class="nav-number">2.3.1.</span> <span class="nav-text">如何选择超参数\(\beta \)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RMSProp-算法"><span class="nav-number">2.4.</span> <span class="nav-text">RMSProp 算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adam-算法"><span class="nav-number">2.5.</span> <span class="nav-text">Adam 算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#超参数选择"><span class="nav-number">2.5.1.</span> <span class="nav-text">超参数选择</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习率衰减"><span class="nav-number">2.6.</span> <span class="nav-text">学习率衰减</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#超参数调试、Batch-归一化和编程框架"><span class="nav-number">3.</span> <span class="nav-text">超参数调试、Batch 归一化和编程框架</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#超参数调试"><span class="nav-number">3.1.</span> <span class="nav-text">超参数调试</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#超参数重要性排序如下（参考）"><span class="nav-number">3.1.1.</span> <span class="nav-text">超参数重要性排序如下（参考）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#调参技巧（建议）"><span class="nav-number">3.1.2.</span> <span class="nav-text">调参技巧（建议）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#给超参数选择合适的范围"><span class="nav-number">3.1.3.</span> <span class="nav-text">给超参数选择合适的范围</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#超参数调试实践"><span class="nav-number">3.1.4.</span> <span class="nav-text">超参数调试实践</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-归一化-Batch-Normalization"><span class="nav-number">3.2.</span> <span class="nav-text">Batch 归一化(Batch Normalization)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-归一化-Batch-Normalization-用于神经网络"><span class="nav-number">3.2.1.</span> <span class="nav-text">Batch 归一化(Batch Normalization)用于神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BN有效的解释"><span class="nav-number">3.2.2.</span> <span class="nav-text">BN有效的解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#测试阶段的-Batch-归一化"><span class="nav-number">3.2.3.</span> <span class="nav-text">测试阶段的 Batch 归一化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax-回归"><span class="nav-number">3.3.</span> <span class="nav-text">Softmax 回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#编程框架-Tensorflow"><span class="nav-number">3.4.</span> <span class="nav-text">编程框架 Tensorflow</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考"><span class="nav-number">4.</span> <span class="nav-text">参考</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">穆义</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
