<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Times New Roman:300,300italic,400,400italic,700,700italic|18:300,300italic,400,400italic,700,700italic|Courier New:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hello, World" />










<meta name="description" content="卷积神经网络基础计算机视觉计算机视觉 ( Computer Vision )是一个飞速发展的领域，其中深度学习技术起到重要作用。计算机视觉中有许多的应用，例如图片分类 ( Image Classification ) 、目标检测 ( Object detection ) 、图片风格转换 ( Neural Style Transfer ) 等等。应用计算机视觉时可能要面临的一个挑战是输入的数据">
<meta property="og:type" content="article">
<meta property="og:title" content="04.卷积神经网络">
<meta property="og:url" content="https://muyi110.github.io/2018/04-卷积神经网络/index.html">
<meta property="og:site_name" content="MuYi&#39;s Blog">
<meta property="og:description" content="卷积神经网络基础计算机视觉计算机视觉 ( Computer Vision )是一个飞速发展的领域，其中深度学习技术起到重要作用。计算机视觉中有许多的应用，例如图片分类 ( Image Classification ) 、目标检测 ( Object detection ) 、图片风格转换 ( Neural Style Transfer ) 等等。应用计算机视觉时可能要面临的一个挑战是输入的数据">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_052.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_053.png">
<meta property="og:image" content="https://muyi110.github.io/images/Convolution_schematic.gif">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_054.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_055.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_056.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_057.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_058.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_059.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_060.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_061.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_062.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_063.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_064.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_065.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_066.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_067.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_068.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_069.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_070.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_071.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_072.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_073.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_074.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_075.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_076.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_077.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_078.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_079.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_080.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_081.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_082.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_083.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_084.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_085.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_086.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_087.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_088.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_089.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_090.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_091.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_092.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_093.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_094.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_095.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_096.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_097.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_098.png">
<meta property="og:image" content="https://muyi110.github.io/images/deeplearning_ai_099.png">
<meta property="og:updated_time" content="2018-08-14T08:32:56.640Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="04.卷积神经网络">
<meta name="twitter:description" content="卷积神经网络基础计算机视觉计算机视觉 ( Computer Vision )是一个飞速发展的领域，其中深度学习技术起到重要作用。计算机视觉中有许多的应用，例如图片分类 ( Image Classification ) 、目标检测 ( Object detection ) 、图片风格转换 ( Neural Style Transfer ) 等等。应用计算机视觉时可能要面临的一个挑战是输入的数据">
<meta name="twitter:image" content="https://muyi110.github.io/images/deeplearning_ai_052.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://muyi110.github.io/2018/04-卷积神经网络/"/>





  <title>04.卷积神经网络 | MuYi's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">MuYi's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">此博客创建于2018-07-10</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-reprinted_article">
          <a href="/reprinted-article/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heart"></i> <br />
            
            转载文章
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://muyi110.github.io/2018/04-卷积神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="穆义">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/wukong.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MuYi's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">04.卷积神经网络</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-10T15:25:02+08:00">
                2018-08-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习课程-Andrew-Ng-学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习课程(Andrew Ng)学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h1 id="卷积神经网络基础"><a href="#卷积神经网络基础" class="headerlink" title="卷积神经网络基础"></a>卷积神经网络基础</h1><h2 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h2><p>计算机视觉 ( <code>Computer Vision</code> )是一个飞速发展的领域，其中深度学习技术起到重要作用。计算机视觉中有许多的应用，例如图片分类 ( <code>Image Classification</code> ) 、目标检测 ( <code>Object detection</code> ) 、图片风格转换 ( <code>Neural Style Transfer</code> ) 等等。<br>应用计算机视觉时可能要面临的一个挑战是<strong>输入的数据可能会非常大</strong>。例如输入一张 \(1000\times 1000\times 3 \) 的图片，则神经网络的输入维度将达到 300 万，使得神经网络的权重 \(W\) 的参数非常多，会带来两个后果：</p>
<ol>
<li>难以获得足够数据来防止神经网络发生过拟合；</li>
<li>需要很大的内存和计算代价。</li>
</ol>
<p>因此，需要利用<strong>卷积神经网络</strong> ( <code>Convolutional Neural Network, CNN</code> )。</p>
<h2 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h2><blockquote>
<p>卷积运算 (<code>Convolutional Operation</code>) 是卷积神经网络最基本的组成部分。（神经网络由浅层到深层，分别可以检测出图片的边缘特征、局部特征（例如眼睛、鼻子等）、到最后一层可由前面检测的特征识别整体面部轮廓。）</p>
</blockquote>
<p>图片常做的边缘检测有<strong>垂直边缘检测</strong>和<strong>水平边缘检测</strong>等，如下图所示。<img src="/images/deeplearning_ai_052.png" alt="边缘检测">图片的边缘检测可以通过与相应滤波器卷积来实现。例如在垂直边缘检测中，原始图片是 \(6\times 6 \) ，中间的矩阵称为<strong>滤波器</strong> (<code>filter</code>) ，大小为 \(3\times 3 \) ，卷积后的图片大小为 \(4\times 4\) ，如下图所示（以左上角和右下角为例，数值表示灰度）：<img src="/images/deeplearning_ai_053.png" alt="垂直边缘检测卷积运算">根据上图可知，卷积运算过程为从左到右，从上到下，每次在原始图中取与滤波器同等大小的一部分，每一部分中的值与滤波器中的值对应相乘（逐元素相乘）然后求和，将结果组成一个新的矩阵输出，为方便理解，下图是卷积运算实现动图：<img src="/images/Convolution_schematic.gif" alt="卷积运算实现动图">下图是垂直边缘检测的另一个例子，解释为什么下图中的滤波器可以做垂直边缘检测：<img src="/images/deeplearning_ai_054.png" alt="垂直边缘检测解释">将上图中最右边的矩阵看作输出图片，则经过卷积后，输出图片中间白色区域对应输入图片中间的垂直边缘（区分白色和灰色的垂直线）。</p>
<h2 id="更多边缘检测例子"><a href="#更多边缘检测例子" class="headerlink" title="更多边缘检测例子"></a>更多边缘检测例子</h2><p><img src="/images/deeplearning_ai_055.png" alt="边缘检测又例">如果将灰度图左右数值进行翻转，再与之前的滤波器进行卷积，得到结果有区别。实际中，反映了由明变暗和由暗变明的两种渐变方式。如果不在乎两者的区别，可以对输出取绝对值。<br>下图是垂直边缘检测和水平边缘检测滤波器：<img src="/images/deeplearning_ai_056.png" alt="垂直和水平边缘检测">还有其他的滤波器，如 <code>Sobel</code> 滤波器和 <code>Scharr</code> 滤波器，其增加了中间一行的权重，提高了结果的稳健性。<img src="/images/deeplearning_ai_057.png" alt="其它滤波器示例"><strong>注意</strong>：实际中，不一定去使用那些研究者选择的滤波器数字，<strong>可以将滤波器中的数字看成参数</strong>，之后可以通过反向传播去学习这些参数。</p>
<h2 id="填充-Padding"><a href="#填充-Padding" class="headerlink" title="填充 ( Padding )"></a>填充 ( Padding )</h2><p>假如输入的图片大小是 \(n\times n \) 滤波器的大小是 \(f\times f \) ，则卷积后的输出大小是 \((n-f+1)\times (n-f+1)\) 。这样会存在两个缺点：</p>
<ol>
<li>每次卷积运算后，输出的图形会缩小，例如从 \(6\times 6 \) 缩小到 \(4\times 4 \)，经过几次后可能变为 \(1\times 1 \) 的大小。</li>
<li>那些在角落或者边缘区域的像素在输出中采用较少，意味着丢掉了图像边缘位置的许多信息。</li>
</ol>
<p>为了解决这两个问题，需要在卷积操作之前对输入图像进行<strong>填充</strong>。就是在图像边缘再填充像素，<strong>通常填充 \(0\)</strong> 。如果用 <code>p</code> 表示每个方向填充的像素点的数量，下图的中 \(p=2 \) ：<img src="/images/deeplearning_ai_058.png" alt="填充事例">可知，填充后的图片大小是 \((n+2p)\times (n+2p)\) ，滤波器的大小是 \(f\times f \) ，则卷积后输出大小是 \((n+2p-f+1)\times (n+2p-f+1)\) 。<br>至于选择多少像素填充，有两种选择：</p>
<ul>
<li><strong>Valid 填充</strong>：不填充( \(p=0 \) )，输出结果是 \((n-f+1)\times (n-f+1)\) 。</li>
<li><strong>Same 填充</strong>：填充使得输入和卷积后的输出的大小一样，需要 \(p=\dfrac {f-1}{2}\) 。</li>
</ul>
<p><strong>注意</strong>：一般情况下，\(f\) 通常是奇数。</p>
<h2 id="卷积步长-Strided-convolutions"><a href="#卷积步长-Strided-convolutions" class="headerlink" title="卷积步长 ( Strided convolutions )"></a>卷积步长 ( Strided convolutions )</h2><blockquote>
<p>卷积步长是另一个构建卷积神经网络的基本操作。</p>
</blockquote>
<p><strong>步长表示滤波器在原始图像上水平或垂直方向上每次移动的距离。</strong>之前的步常默认为 \(1\) 。如果将步长设置为 \(2\) 则卷积过程如下图所示：<img src="/images/deeplearning_ai_059.png" alt="步长为2的卷积过程">设步长为 s ，填充像素量为 p ，输入图片大小为 \(n\times n \) ，滤波器大小为 \(f\times f \) ，则卷积后的输出大小为：$$\lfloor {\dfrac {n+2p-f}{s}+1}\rfloor \times \lfloor {\dfrac {n+2p-f}{s}+1}\rfloor$$公式中的符号表示取整符号。</p>
<ul>
<li>杂谈：<blockquote>
<p>在机器学习中谈论的<strong>卷积</strong>实际上被称为<strong>互相关 (cross-correlation)</strong>，不是数学意义上的卷积。数学上的卷积在做元素乘积求和之前，需要将滤波器沿水平和垂直轴翻转（相当于做镜像）。但按照机器学习惯例，通常不进行翻转操作。</p>
</blockquote>
</li>
</ul>
<h2 id="高维卷积"><a href="#高维卷积" class="headerlink" title="高维卷积"></a>高维卷积</h2><p><img src="/images/deeplearning_ai_060.png" alt="三通道卷积"><strong>如果对三通道的 <code>RGB</code> 的图像 (\(6\times 6 \times 3 \)) 进行卷积操作，则对应的滤波器也同样是三通道的 (\(3\times 3\times 3 \))卷积的结果输出是 (\(4\times 4 \times 1\))</strong>。实现过程：将每个单通道与对应的滤波器（单通道）进行卷积，然后将三个通道相加，将 \(27\) 个值求和作为一个输出值。<br>不同的通道对应的滤波器可以不同。例如，如果只检测 <code>R</code> 通道的垂直边缘，则 <code>G</code> 和 <code>B</code> 通道对应的滤波器值可以全部设置为 \(0\) 。<strong>当输入有特定的高度、宽度和通道数时，滤波器可以有不同的高、宽，但必须有相同的通道数。</strong><br>如果想同时检测垂直和水平边缘，或更多其他边缘（换句话说就是同时检测更多特征），可以增加滤波器个数。例如设置第一个滤波器检测垂直边缘，第二个滤波器检测水平边缘，最后将每个滤波器卷积的输出堆叠在一起，如下图所示：<img src="/images/deeplearning_ai_061.png" alt="多个滤波器">每个滤波器的输出对应一个特征。设输入的图像大小是 \(n\times n \times n_c\) (\(n_c \) 是通道数) ，滤波器大小是 \(f\times f \times n_c \) ，则卷积的输出是 \((n-f+1)\times (n-f+1)\times n’ _c \) ，其中 \(n’_c \) 为滤波器个数（默认步长为 \(1\)）。</p>
<h2 id="单层卷积网络-one-layer-of-a-convolutional-network"><a href="#单层卷积网络-one-layer-of-a-convolutional-network" class="headerlink" title="单层卷积网络 (one layer of a convolutional network)"></a>单层卷积网络 (one layer of a convolutional network)</h2><p><img src="/images/deeplearning_ai_062.png" alt="单层卷积网络">与之前的卷积过程相比较，卷积神经网络的单层结构多了<strong>激活函数</strong>和<strong>偏移量</strong>；而与标准神经网络：$$Z ^{[l]} = W ^{[l]}A ^{[l-1]}+ b$$$$A ^{[l]}= g ^{[l]}(Z ^{[l]})$$相比，滤波器的数值对应权重 \(W ^{[l]}\) ，卷积运算对应 \(W ^{[l]}\) 与 \(A ^{[l-1]}\) 的乘积运算，激活函数选择 <code>Relu</code> 函数。<br>下面视频说明了单层卷积神经网络工作过程：</p>
<p><div align="center"><video width="620" height="440" src="/images/conv_kiank.mp4" type="video/mp4" controls><br></video></div><br>对于一个 \(3\times 3 \times 3 \) 的滤波器，包括偏移量 \(b\) 在内共有 \(28\) 个参数，不论输入的图像多大，参数始终是 \(28\) 。即<strong>选定滤波器组后，参数的数目与输入的图片尺寸无关</strong>，因此对比标志神经网络，卷积神经网络的参数要少的多，这是卷积神经网络 (<code>CNN</code>) 的优点之一。<br><strong>符号总结</strong><br>设 \(l\) 层为卷积层，则：</p>
<ul>
<li>\(f ^{[l]}\) ：滤波器的高或宽；</li>
<li>\(p ^{[l]}\) ：填充的像素数量；</li>
<li>\(s ^{[l]}\) ：步长；</li>
<li>\(n _c ^{[l]}\) ：滤波器的个数；</li>
<li>输入维度：\(n _H ^{[l-1]}\times n _W ^{[l-1]}\times n _c ^{[l-1]}\) ，其中 \(n _H ^{[l-1]}\) 表示输入图像高度，\(n _W ^{[l-1]}\) 表示输入图像宽度。</li>
<li>输出维度：\(n _H ^{[l]}\times n _W ^{[l]}\times n _c ^{[l]}\) ，其中：$$n _H ^{[l]} = \lfloor {\dfrac {n _H ^{[l-1]}+2p ^{[l]}-f ^{[l]}}{s ^{[l]}}+1}\rfloor$$$$n _W ^{[l]} = \lfloor {\dfrac {n _W ^{[l-1]}+2p ^{[l]}-f ^{[l]}}{s ^{[l]}}+1}\rfloor$$</li>
<li>每个滤波器的维度是：\(f ^{[l]}\times f ^{[l]}\times n _c ^{[l-1]}\) ，其中 \(n _c ^{[l-1]}\) 为输入图像的通道数（也称为深度）；</li>
<li>权重维度：\(f ^{[l]}\times f ^{[l]}\times n _c ^{[l-1]}\times n _c ^{[l]}\) ；</li>
<li>偏置维度：\(1\times 1 \times 1 \times n _c ^{[l]}\)。</li>
</ul>
<p>注：不同的文献可能对高度、宽度、通道数的顺序表示不同。</p>
<h2 id="简单卷积网络示例"><a href="#简单卷积网络示例" class="headerlink" title="简单卷积网络示例"></a>简单卷积网络示例</h2><p>一个简单的 <code>CNN</code> 神经网络如下图所示：<img src="/images/deeplearning_ai_063.png" alt="简单CNN神经网络">图中 \(a ^{[3]}\) 的维度为 \(7\times 7 \times 40 \) ，将 \(1960\) 个特征拉伸为一列，作为最后一层的输入。<br>随着神经网络的深度的不断加深，输出的图像的高度 \(n _H ^{[l]}\) 和宽度 \(n _W ^{[l]}\) 不断减小，但 \(n _c ^{[l]}\)在不断增加。<br>一个典型的卷积神经网络通常有三种层：<strong>卷积层 (Convolution layer)、池化层 (Pooling layer) 、全连接层 (Fully Connected layer)</strong>。</p>
<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><blockquote>
<p><strong>池化层可用来缩减模型大小、提高计算速度、同时提高所提取特征的鲁棒性。</strong></p>
</blockquote>
<p>实际中，采用较多的一种池化过程叫<strong>最大化池化 (Max Pooling)</strong>。将输入拆分为不同的区域，输出的每个元素都是对应区域的最大值，如下图所示：<img src="/images/deeplearning_ai_064.png" alt="最大池化">池化过程类似于卷积过程，上图中的池化过程相当于使用了大小 \(f=2\) 的滤波器，且步长为 \(s=2\) 。卷积过程的公式同样也适用于池化过程。<strong>如果输入有多个通道，则分别对每个通道执行池化过程</strong>，因此输入的通道数和输出的通道数一样。</p>
<ul>
<li>杂谈：最大池化过程的直观理解<blockquote>
<p>元素值较大可能意味着池化过程之前的卷积过程提取到了某些特定的特征，池化过程中的最大化操作使得只要在一个区域内提取到某个特征，它都会保留在最大池化的输出中。</p>
</blockquote>
</li>
</ul>
<p>另一种池化过程是<strong>平均池化 (Average Pooling)</strong>。将从取某个区域最大值改为取这个区域的平均值作为输出（用的较少）。平均池化过程如下图所示：<img src="/images/deeplearning_ai_065.png" alt="平均池化"><strong>注意</strong>：池化过程有一组超参数（大小 \(f\) ，步长 \(s\) ，及选择最大池化还是平均池化）但<strong>没有参数需要学习</strong>。填充参数 \(p\) 很少用。<br>池化过程的输入维度为：$$n _H\times n _W\times n _c$$输出维度为：$$\lfloor {\dfrac {n _H -f}{s}+1}\rfloor \times \lfloor {\dfrac {n _W -f}{s}+1}\rfloor\times n _c$$</p>
<h2 id="卷积神经网络示例"><a href="#卷积神经网络示例" class="headerlink" title="卷积神经网络示例"></a>卷积神经网络示例</h2><p>下图是一个识别数字的卷积神经网络的结构（类似 <code>LeNet-5</code> 网络）：<img src="/images/deeplearning_ai_066.png" alt="识别数字CNN">建议：计算神经网络的层数时，通常只统计具有权重和参数的层，因此池化层和之前的卷积层记为一层。<br>图中的 <code>FC3</code> 和 <code>FC4</code> 为全连接层，与标准的神经网络一致。整个神经网络的参数如下图所示：<img src="/images/deeplearning_ai_067.png" alt="CNN 参数"></p>
<h2 id="使用卷积神经网络原因"><a href="#使用卷积神经网络原因" class="headerlink" title="使用卷积神经网络原因"></a>使用卷积神经网络原因</h2><p>和标准神经网络对比，对于大量的输入数据，<code>CNN</code> 有效减小了参数的数量，原因如下：</p>
<ul>
<li><strong>参数共享 (Parameter sharing)</strong>：特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。即在卷积过程中，不管输入有多大，一个特征探测器（滤波器）就能对整个输入的某一特征进行探测。</li>
<li><strong>稀疏连接 (Sparsity of connections)</strong>：在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值。</li>
</ul>
<p><strong>池化过程则在卷积后很好地聚合了特征，通过降维来减少运算量。</strong><br>由于 <code>CNN</code> 参数数量较小，所需的训练样本就相对较少，因此在一定程度上不容易发生过拟合现象。并且 <code>CNN</code> 比较擅长捕捉区域位置偏移（捕捉平移不变）。即进行物体检测时，不太受物体在图片中位置的影响，增加检测的准确性和系统的健壮性。</p>
<h1 id="深度卷积网络：实例探究"><a href="#深度卷积网络：实例探究" class="headerlink" title="深度卷积网络：实例探究"></a>深度卷积网络：实例探究</h1><p>本节涉及的神经网络实例包括：</p>
<ul>
<li>LeNet-5</li>
<li>AlexNet</li>
<li>VGG</li>
</ul>
<p>此外还涉及 Resnet (Residual Network, 残差网络) 和 Inception Neural Network 。</p>
<h2 id="经典网络"><a href="#经典网络" class="headerlink" title="经典网络"></a>经典网络</h2><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p><img src="/images/deeplearning_ai_068.png" alt="LeNet-5">该网络具有的特点如下：</p>
<ul>
<li><code>LeNet-5</code> 针对灰度图像训练，输入图像的通道数为 \(1\) ；</li>
<li>该网络模型包含参数少（此例中大约 \(6\) 万个），远少于现代神经网络的参数；</li>
<li>针对分类问题，现代版本中输出层使用 <code>Softmax</code> 函数（图中含有 \(84\) 个节点的层作为 <code>Softmax</code> 的输入；</li>
<li>该网络包含一种模式至今常用到，就是<strong>一个或多个卷积层后跟着一个池化层，然后又是若干个卷积层再接一个池化层，然后是全连接层，最后是输出</strong>；</li>
<li><code>LeNet-5</code> 网络被提出时，其池化层使用平均池化，各层的激活函数一般选择 <code>Sigmoid</code> 和 <code>tanh</code> 。现在，根据需要可以做出改进，例如使用最大池化，激活函数选择 <code>Relu</code> 函数。</li>
</ul>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p><img src="/images/deeplearning_ai_069.png" alt="AlexNet">该网络具有的特点如下：</p>
<ul>
<li><code>AlexNet</code> 模型与 <code>LeNet-5</code> 模型类似，但更加复杂（此例中包含大约 \(6000\) 万个参数。 <code>AlexNet</code> 模型使用了 <code>Relu</code> 激活函数；</li>
<li>当用于训练图像和数据集时， <code>AlexNet</code> 网络结构能够处理非常相似的基本构造块，这些模块往往包含大量的隐藏单元或数据。</li>
</ul>
<p>相关论文参考：<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">Krizhevsky et al.,2012. ImageNet classification with deep convolutional neural networks</a></p>
<h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><p><img src="/images/deeplearning_ai_070.png" alt="VGG">该网络具有的特点如下：</p>
<ul>
<li><code>VGG</code> 又称为 <code>VGG-16</code> ，其中数字 \(16\) 表示网络中包含 \(16\) 个卷积层和全连接层；</li>
<li>结构不复杂且规整（都是几个卷积层后面跟着可以压缩图像大小的池化层），卷积层的滤波器数量在每一步翻倍，或者说在每一组卷积层进行过滤器翻倍操作。</li>
<li>主要缺点是需要训练的特征数量非常大。</li>
</ul>
<p>相关论文参考：<a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">Simonvan &amp; Zisserman 2015. Very deep convolutional networks for large-scale image recognition</a></p>
<h2 id="残差网络-Residual-Networks-ResNets"><a href="#残差网络-Residual-Networks-ResNets" class="headerlink" title="残差网络 (Residual Networks, ResNets)"></a>残差网络 (Residual Networks, ResNets)</h2><blockquote>
<p>The main benefit of a very deep network is that it can represent very complex functions. It can also learn features at many different levels of abstraction, from edges (at the lower layers) to very complex features (at the deeper layers). However, using a deeper network doesn’t always help. A huge barrier to training them is vanishing gradients: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent unbearably slow. More specifically, during gradient descent, as you backprop from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and “explode” to take very large values). </p>
</blockquote>
<p>由于存在梯度消失和梯度爆炸问题，很深的网络往往很难训练。<strong>残差网络 (ResNets)</strong>可有效解决这个问题。<br><img src="/images/deeplearning_ai_071.png" alt="残差块">上图是<strong>残差块 (Residual block)</strong>。通过<strong>捷径</strong> ( <code>Short cut</code>，或者称为 <code>Skip connections</code> 跳远连接)将 \(a ^{[l]}\) 添加到第二个 <code>Relu</code> 中，直接建立 \(a ^{[l]}\) 和 \(a ^{[l+2]}\) 的连接。对应的表达式如下：$$z ^{[l+1]}=W ^{[l+1]}a ^{[l]}+b ^{[l+1]}$$$$a ^{[l+1]}=g(z ^{[l+1]})$$$$z ^{[l+2]}=W ^{[l+2]}a ^{[l+1]}+b ^{[l+2]}$$$$a ^{[l+2]}=g(z ^{[l+2]}+a ^{[l]})$$构建一个残差网络就是将许多残差块堆叠在一起，形成一个很深的网络，如下图所示：<img src="/images/deeplearning_ai_072.png" alt="残差网络">理论上，随着网络深度的加深，性能应该越来越好。但实际中，一个普通网络，随着神经网络层数增加，训练误差会先下降后上升。残差网络在训练集上会表现越来越好。如下图所示：<img src="/images/deeplearning_ai_073.png" alt="误差分析"><strong>残差网络有助于解决梯度消失和梯度爆炸问题</strong>,让我们在训练更深网络的同时，又能保证良好的性能。</p>
<h2 id="残差网络有效的原因"><a href="#残差网络有效的原因" class="headerlink" title="残差网络有效的原因"></a>残差网络有效的原因</h2><p>假设有一个大型神经网络，输入是 X ，输出是 \(a ^{[l]}\) 。给这个神经网络添加额外的两层，输出为 \(a ^{[l+2]}\) 。将这两层看层一个残差块，为便于说明，假设整个网络使用 <code>Relu</code> 激活函数。<img src="/images/deeplearning_ai_074.png" alt="解释残差网络">根据上图有：$$\begin {align} a ^{[l+2]} &amp;=g(z ^{[l+2]}+a ^{[l]})\\&amp;= g(W ^{[l+2]}a ^{[l+1]}+ b ^{[l+2]}+ a ^{[l]}) \end {align}$$当发生梯度消失时，有 \(W ^{[l+2]}\approx 0\) 方便说明令 \(b ^{[l+2]}\approx 0\) 则有：$$a ^{[l+2]}=g(a ^{[l]})=\text {Relu}(a ^{[l]})= a ^{[l]}$$说明增加的两层至少不会降低神经网络的性能（不论是把残差块添加到神经网络的中间还是末端位置）。如果增加的两层隐藏单元学习到了一些有用的信息，则可能会使网络的表现进一步提高。<br>如果 \(a ^{[l]}\) 和 \(a ^{[l+2]}\) 的维度不同，需要引入矩阵 \(W _s \) 与 \(a ^{[l]}\) 相乘，使得二者的维度相同。矩阵 \(W _s \) 可以是模型学习得到，或者是一个固定矩阵，使得 \(a ^{[l]}\) 截断或补 \(0\) 。下图是论文中的残差网络的一个典型结构：<img src="/images/deeplearning_ai_075.png" alt="残差网络用于图像识别">卷积层通常使用 <code>same</code> 卷积以保持维度相同。<br>相关论文：<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">He et al., 2015. Deep residual networks for image recognition</a></p>
<h2 id="1-times-1-卷积"><a href="#1-times-1-卷积" class="headerlink" title="\(1\times 1\) 卷积"></a>\(1\times 1\) 卷积</h2><p>\(1\times 1\) 卷积（或者称为 <code>Network in Network</code>）指滤波器的大小为 \(1\) 。当输入通道为 \(1\) 时，\(1\times 1\)卷积意味着乘积操作（似乎没啥用）。如下图所示：<img src="/images/deeplearning_ai_076.png" alt="1*1卷积">当输入通道数更多时，例如下图中的 \(32\) 个通道，<strong>\(1\times 1\) 卷积可以从根本上理解对这 \(32\) 个不同位置都应用一个全连接层。</strong>可用于降低或升高数据的维度（取决于滤波器的个数）。<img src="/images/deeplearning_ai_077.png" alt="多通道1*1卷积"><strong>池化层可以压缩输入的高度 (\(n _H\)) 和宽度 (\(n _W\))，而 \(1\times 1\) 卷积可以压缩输入的通道数 (\(n _c\))，取决于用多少个滤波器。</strong>例如下图中用 \(32\) 个大小为 \(1\times 1\times 192\) 的滤波器进行卷积，将输入数据通道数从 \(192\) 压缩为 \(32\) 。<img src="/images/deeplearning_ai_078.png" alt="1*1卷积压缩例子"></p>
<h2 id="Inception-网络"><a href="#Inception-网络" class="headerlink" title="Inception 网络"></a>Inception 网络</h2><blockquote>
<p><code>Inception</code> 网络不需要人为决定使用哪种滤波器 (\(1\times 1\) 还是 \(3\times 3\)等) 或者是否需要池化，而是由网络自行确定这些参数，可以给网络添加这些参数的所有可能值，然后将这些对应的输出连接起来，让网络自己学习它需要什么样的参数，采用哪些滤波器组合。</p>
</blockquote>
<p><img src="/images/deeplearning_ai_079.png" alt="Inception网络">为了使得输出组合时维度匹配，<code>Inception</code> 网络选择不同大小的滤波器进行 <code>same</code> 卷积。<br>在提升性能的同时， <code>Inception</code> 网络有较大的计算成本问题，下面以 \(5\times 5\) 大小的卷积为例说明：<img src="/images/deeplearning_ai_080.png" alt="Inception网络计算成本">图中有 \(32\) 个滤波器，每个滤波器大小为 \(5\times 5 \times 192\) ，输出大小为 \(28\times 28\times 32\) ,因此需要计算 \(28\times 28\times 32\) 个数字，对于每个数字，需要执行 \(5\times 5\times 192\) 次乘法，加法和乘法运算次数近似相等。因此，此层的计算量为 \(28\times 28\times 32\times 5\times 5\times 192 =1.2\) 亿。<br>为了解决计算量大的问题，引入 \(1\times 1\) 卷积（有时候称为瓶颈层），如下图：<img src="/images/deeplearning_ai_081.png" alt="有1*1卷积的Inception网络">同理可以计算出引入 \(1\times 1\) 卷积后，计算量变为 \(1024\) 万，大大降低了计算量。<br><strong>注</strong>：只要合理构建瓶颈层，既可以显著缩小表示层规模，又不会降低网络性能，从而节省计算。</p>
<h2 id="完整的-Inception-网络"><a href="#完整的-Inception-网络" class="headerlink" title="完整的 Inception 网络"></a>完整的 Inception 网络</h2><p>下图是引入 \(1\times 1\) 卷积后的 <code>Inception</code> 模块：<img src="/images/deeplearning_ai_082.png" alt="完整的Inception网络模块">使用 <code>same</code> 类型的 <code>padding</code> 来池化，使得输出的高度和宽度和输入保持一致。由于池化层不改变输入的通道数，因此需要加一个 \(1\times 1\) 卷积层将输入通道压缩。<br>多个 <code>Inception</code> 模块组成一个完整的 <code>Inception</code> 网络，如下图所示：<img src="/images/deeplearning_ai_083.png" alt="完整的Inception网络">图中圈起来的隐藏层用于 <code>softmax</code> 输出，他们确保了即便是隐藏单元和中间层也参与特征计算，在 <code>Inception</code> 网络中起到一种调整效果，并且可以防止网络发生过拟合。<br>相关论文：<a href="https://arxiv.org/pdf/1409.4842.pdf" target="_blank" rel="noopener">Szegedy et al., 2014, Going Deeper with Convolutions</a></p>
<h2 id="使用开源的实现方案"><a href="#使用开源的实现方案" class="headerlink" title="使用开源的实现方案"></a>使用开源的实现方案</h2><p>很多神经网络复杂细致，并充斥着参数调节的细节问题，因而很难仅通过阅读论文来重现他人的成果。想要搭建一个同样的神经网络，查看开源的实现方案会快很多。<br>某些网络通常都需要很长的时间来训练，而或许有人已经使用多个 <code>GPU</code>，通过庞大的数据集预先训练了这些网络，这样一来你就可以使用这些网络进行迁移学习。</p>
<h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><p><a href="https://muyi110.github.io/2018/08/08/03-%E6%9E%84%E5%BB%BA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/#more">迁移学习</a>在之前已有介绍。相比于从头训练权重，下载别人已经训练好的网络结构的权重，用其做预训练，然后转换到自己感兴趣的任务上，有助于加速开发。<br>对于已训练好的卷积神经网络，可以将所有层都看作是冻结的，只需要训练与你的 <code>Softmax</code> 层有关的参数即可。如下图所示：<img src="/images/deeplearning_ai_084.png" alt="迁移学习"><strong>冻结的层由于不需要改变和训练，可以看作一个固定函数。可以将这个固定函数存入硬盘，以便后续使用，而不必每次再使用训练集进行训练了。</strong><br>上述的做法适用于你只有一个较小的数据集。如果你有一个更大的数据集，应该冻结更少的层，然后训练后面的层。越多的数据意味着冻结越少的层，训练更多的层。如果有一个极大的数据集，你可以将开源的网络和它的权重整个当作初始化（代替随机初始化），然后训练整个网络。</p>
<h2 id="数据扩增"><a href="#数据扩增" class="headerlink" title="数据扩增"></a>数据扩增</h2><p>当数据量小时，数据扩增 ( <code>Data Augmentation</code>) 可能会有帮助。<strong>常用的数据扩增包括镜像翻转、随机裁剪、色彩转换。</strong>其中，色彩转换是对图片的 <code>RGB</code> 通道数值进行随意增加或者减少，改变图片色调。关于 <code>PCA</code> 颜色增强可以查阅相关文献或代码。<br>在构建大型神经网络的时候，<strong>数据扩增</strong>和<strong>模型训练</strong>可以<strong>由两个或多个不同的线程并行来实现。</strong></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在模型研究或者竞赛方面，有一些方法能够有助于提升神经网络模型的性能：</p>
<ul>
<li>集成（ <code>Ensembling</code> ）：独立地训练几个神经网络，并平均输出它们的输出；</li>
<li><code>Multi-crop at test time</code>：将数据扩增应用到测试集，对结果进行平均。</li>
</ul>
<p>由于这些方法计算和内存成本较大，一般不适用于构建实际的生产项目。</p>
<h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h1><h2 id="目标定位"><a href="#目标定位" class="headerlink" title="目标定位"></a>目标定位</h2><p>定位分类问题不仅需要识别判断出图片中物体的种类，还需要在图片中标记出其具体位置，用<strong>边框</strong> ( <code>Bounding Box</code> )将物体圈起来。定位分类问题通常只有一个较大的对象位于图片中间位置，需要对它进行识别和定位。在对象检测中，图片可能含有多个对象，甚至单张图片中会有多个不同的对象。<br>为了定位图片中的汽车位置，可以让神经网络多输出 \(4\) 个数字，标记为 \(b _x \) 、\(b _y \) 、\(b _h \) 、\(b _w \) 。将图片左上角标记为 \((0,0)\) 右下角标记为 \((1,1)\) 如下图所示：<img src="/images/deeplearning_ai_085.png" alt="目标定位"></p>
<blockquote>
<p>If you have 80 classes that you want YOLO to recognize, you can represent the class label c either as an integer from 1 to 80, or as an 80-dimensional vector (with 80 numbers) one component of which is 1 and the rest of which are 0. The video lectures had used the latter representation.</p>
</blockquote>
<p>图中 \(P _c \) 表示是否含有物体，若含有物体则 \(P _c =1\) 若不含物体则 \(P _c =0\) 。对应的 \(y\) 标签如下：$$\begin {align}P _c =1, y=\begin {bmatrix}1 \\ b _x \\ b _y \\ b _h \\ b _w \\ c \end {bmatrix}&amp;  &amp;P _c =0, y=\begin {bmatrix}0 \\ ? \\ ? \\ ? \\ ? \\ ? \end {bmatrix}\end {align}$$损失函数表示为 \(L(\widehat y , y)\) 若采用平方误差损失，则计算损失函数如下：</p>
<ol>
<li>当 \(P _c =1 \) 即 \(y _1 =1\)：\(L(\widehat y , y)=(\widehat {y} _1 - y _1) ^2 + \ldots + (\widehat {y} _n - y _n) ^2\) ；</li>
<li>当 \(P _c =0 \) 即 \(y _1 =0\)：\(L(\widehat {y} , y)=(\widehat {y} _1 - y _1) ^2\)。</li>
</ol>
<p>对于损失函数，通常做法是对边界框坐标应用平方差或类似方法，对 \(P _c =1\) 应用逻辑回归函数，甚至采用平方预测误差也是可以的。</p>
<h2 id="特征点检测"><a href="#特征点检测" class="headerlink" title="特征点检测"></a>特征点检测</h2><p>神经网络可以输出图片上的特征点来实现对目标特征的识别（类似于标识对象的中心点位置那样）。在标签中（预测标签和实际标签），特征点以多个二维坐标的形式表示。<br>检测人脸的特征点可以进行情绪分类与判断，或者应用 <code>AR</code> 等领域。也可以通过特征点检测人体的姿态。</p>
<h2 id="目标检测-1"><a href="#目标检测-1" class="headerlink" title="目标检测"></a>目标检测</h2><p>目标检测采用基于<strong>滑动窗口的目标检测 (Sliding Windows Detection)</strong>（因为一张图中可能有多个对象，一张图中有一个对象训练较容易，对于有多个对象，所以滑动窗口是比较好的方法）。该算法的步骤如下（以汽车检测算法为例）：</p>
<ol>
<li>创建一个标签数据集，即适当剪切汽车图片样本，使得汽车位于中间位置（整张图片几乎被汽车占据）；<img src="/images/deeplearning_ai_086.png" alt="汽车样本标签"></li>
<li>利用构建的标签数据集训练 <code>CNN</code> 网络；</li>
<li>选择大小合适窗口和适当的步幅，对测试图片从左到右，从上到下滑动遍历。每个窗口都输入到训练好的 <code>CNN</code> 神经网络进行预测。<img src="/images/deeplearning_ai_087.png" alt="滑动窗口预测"></li>
</ol>
<p>滑动窗口目标检测的<strong>缺点</strong>是计算成本高，因为每次滑动都要进行一次 <code>CNN</code> 网络计算。增大步幅可以减小计算，但可能会影响性能。</p>
<h2 id="卷积的滑动窗口实现"><a href="#卷积的滑动窗口实现" class="headerlink" title="卷积的滑动窗口实现"></a>卷积的滑动窗口实现</h2><p>相较于普通的滑动窗口目标检测（从较大的图片多次截取，分别输入网络计算），<strong>基于卷积的滑动窗口目标检测可以提高运行速度</strong>。为了实现基于卷积的滑动窗口目标检测，首先需要将全连接 (<code>FC</code>)层转化为卷积层，如下图所示：<img src="/images/deeplearning_ai_088.png" alt="FC转化为卷积层">对于全连接层，用 \(5\times 5\times 16\) 的滤波器，数量为 \(400\) 个来实现，输出维度为 \(1\times 1\times 400\) 。从数学上看，经过卷积的输出和全连接层一样，因为这 \(400\) 个节点中每一个都有一个 \(5\times 5\times 16\) 的滤波器，其每个值都是上一层这些 \(5\times 5\times 16\) 激活值经过某个线性函数的输出结果。对于下一个全连接层，用 \(1\times 1\times 400\) 的滤波器，数量为 \(400\) 来实现，输出维度为 \(1\times 1\times 400\)。<br>卷积滑动窗口的示例如下图所示：<img src="/images/deeplearning_ai_089.png" alt="卷积滑动窗口">如图，对于 \(16\times 16\times 3\) 的图片，将其<strong>整张输入给网络进行计算</strong>（不是把输入图形分割为四个子集，分别执行前向传播）。输出维度是 \(2\times 2\times 4\) （步长为 \(2\) ），其中 \(2\times 2\) 表示有 \(4\) 个窗口结果。对于更大的 \(28\times 28\times 3\) 图片，输出是 \(8\times 8\times 4\) ，对应有 \(64\) 个窗口结果。<br>卷积滑动窗口效率高的原因：<strong>各个窗口公共区域的计算可以共享，降低运算成本</strong>。但存在一个缺点就是边界框的位置可能不够准确。<br>相关论文：<a href="https://arxiv.org/pdf/1312.6229.pdf" target="_blank" rel="noopener">Sermanet et al., 2014. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</a></p>
<h2 id="边框预测"><a href="#边框预测" class="headerlink" title="边框预测"></a>边框预测</h2><blockquote>
<p>YOLO (“you only look once”) is a popular algoritm because it achieves high accuracy while also being able to run in real-time. This algorithm “only looks once” at the image in the sense that it requires only one forward propagation pass through the network to make predictions. After non-max suppression, it then outputs recognized objects together with the bounding boxes.</p>
</blockquote>
<p><strong>YOLO (You only look once)</strong>算法能得到更精确的边框。该算法的结构如下：<img src="/images/deeplearning_ai_090.png" alt="YOLO算法结构"><code>YOLO</code> 算法将原始图片划分为 \(n\times n\) 网格（上图 \(n=19\) ），并将图像分类和定位算法逐一应用在每个网格，每个网格都有标签：$$\begin {bmatrix}P _c\\b _x\\b _y\\b _n\\b _w\\c \end {bmatrix}$$<strong>若某个目标的中心点落在某个网格，则该网格负责检测该对象</strong>。<br>这个算法的优点在于神经网络可以输出精确的边界框，测试的时候，输入图像 <code>x</code> ，跑正向传播，得到输出 <code>y</code> 可以得到 \(n\times n\) 个网格哪个有对象，且对应网格的标号以及相应的边界框信息，只要每个网格中对象没超过 \(1\) 个，这个算法就没问题。<br>算法具有如下优点：</p>
<ul>
<li>与图像分类和目标定位算法类似，显示输出边框坐标和大小，不会受到滑动窗步幅大小的限制；</li>
<li>进行一次 <code>CNN</code> 正向计算（单次卷积运算），效率很高，可以达到实时识别。</li>
</ul>
<p>对于编码边框参数 \(b _x\)，\(b _y\)，\(b _h\)，\(b _w\)，<code>YOLO</code> 算法设 \(b _x\)，\(b _y\)，\(b _h\)，\(b _w\) 的值是<strong>相对于网格比例</strong>。则有 \(b _x\)，\(b _y\) 介于 \(0\) 和 \(1\)之间，\(b _h\)，\(b _w\) 值可以大于 \(1\) 。这里只是给出一个通用的表示方法。</p>
<h2 id="交并比"><a href="#交并比" class="headerlink" title="交并比"></a>交并比</h2><p><strong>交并比</strong> (<code>Intersection over union</code>) 是衡量定位精确度的一种方式。<img src="/images/deeplearning_ai_091.png" alt="交并比算法"><code>IoU</code> 的值介于 \(0\) 和 \(1\) 之间，且约接近 \(1\) 说明目标定位越准确。当 \(\text {IoU}\geq 0.5\) 时，一般认为预测边框是正确的，当然也可以设置一个更高的域值。</p>
<h2 id="非极大值抑制-Non-max-suppression"><a href="#非极大值抑制-Non-max-suppression" class="headerlink" title="非极大值抑制 (Non-max suppression)"></a>非极大值抑制 (Non-max suppression)</h2><blockquote>
<p>非最大值意味着只输出概率最大的分类结果，但抑制很接近，但不是最大的其他预测结果。</p>
</blockquote>
<p><code>YOLO</code> 算法理论上只有一个网格检测一个对象，但实际上，可能有多个网格检测同一个对象。非极大值抑制可以确保算法对每个对象只检测一次。<br>非极大值抑制的步骤如下：</p>
<ol>
<li>将包含目标中心的可行度 \(P _c\) 小于域值（例如 \(0.6\) ）的网格丢弃；</li>
<li>选择拥有最大 \(P _c\) 的网格；</li>
<li>分别计算该网格和其他网格的 <code>LoU</code> ，将 <code>LoU</code> 值超过阈值的网格丢弃；</li>
<li>重复 \(2-3\) 步，直到不存在未处理的网格。</li>
</ol>
<p>上述步骤适合于单类别目标检测，进行多个目标检测时，对于每个输出类别都分别做一次非极大值抑制。</p>
<h2 id="Anchor-Boxes"><a href="#Anchor-Boxes" class="headerlink" title="Anchor Boxes"></a>Anchor Boxes</h2><p>到目前为止，对象检测存在的一个问题是每个格子只检测一个对象，让一个格子检测多个对象，可以考虑 <code>Anchor Boxes</code> 。<br><img src="/images/deeplearning_ai_092.png" alt="Anchor Boxes例子">在上图示例中，我们希望同时检测人和汽车。因此，每个网格的的标签中含有两个 <code>Anchor Box</code>（当然可以设置更多的 <code>Anchor Box</code> ） 。输出的标签结果大小从 \(3\times 3\times 8\) 变为 \(3\times 3\times 16\)。若两个 \(P _c\) 都大于预设阈值，则说明检测到了两个目标。<br><strong>在单目标检测中，图像中的目标被分配给了包含该目标中点的那个网格；引入 <code>Anchor Box</code> 进行多目标检测时，图像中的目标则被分配到了包含该目标中点的那个网格以及具有最高 <code>IoU</code> 值的该网格的 <code>Anchor Box</code>。</strong><br><code>Anchor Boxes</code> 也有局限性，对于同一网格有三个及以上目标（但你有两个 <code>Anchor Box</code> ），或者两个目标的 <code>Anchor Box</code> 高度重合的情况处理不好。<br><code>Anchor Box</code> 的形状一般通过人工选取。高级一点的方法是用 <code>k-means</code> 将两类对象形状聚类，选择最具代表性的 <code>Anchor Box</code>。</p>
<h2 id="YOLO-算法"><a href="#YOLO-算法" class="headerlink" title="YOLO 算法"></a>YOLO 算法</h2><p>将上述介绍的组合在一起，具体参考视频讲解。<br>相关论文：<br><a href="https://arxiv.org/pdf/1506.02640.pdf" target="_blank" rel="noopener">You Only Look Once: Unified, Real-Time Object Detection</a><br><a href="https://arxiv.org/pdf/1612.08242.pdf" target="_blank" rel="noopener"> YOLO9000: Better, Faster, Stronger</a></p>
<h2 id="候选区域-Region-proposals"><a href="#候选区域-Region-proposals" class="headerlink" title="候选区域 (Region proposals)"></a>候选区域 (Region proposals)</h2><p>前面介绍的滑动窗口目标检测算法对一些明显没有目标的区域也进行了扫描，这降低了算法的运行效率。为了解决这个问题，<code>R-CNN（Region CNN，带区域的 CNN）</code> 被提出。通过对输入图片运行<strong>图像分割算法</strong>，在不同的色块上找出<strong>候选区域</strong>（ <code>Region Proposal</code> ），就只需要在这些区域上运行分类器。<img src="/images/deeplearning_ai_093.png" alt="R-CNN"><code>R-CNN</code> 的缺点是运行速度慢，所以有一系列后续研究工作改进。例如 <code>Fast R-CNN</code>（与基于卷积的滑动窗口实现相似，但得到候选区域的聚类步骤依然很慢）、<code>Faster R-CNN</code>（使用卷积对图片进行分割）。不过大多数时候还是比 <code>YOLO</code> 算法慢。<br><strong>相关论文</strong>：</p>
<ul>
<li>R-CNN: <a href="https://arxiv.org/pdf/1311.2524.pdf" target="_blank" rel="noopener">Girshik et al., 2013. Rich feature hierarchies for accurate object detection and semantic segmentation</a></li>
<li>Fast R-CNN: <a href="https://arxiv.org/pdf/1504.08083.pdf" target="_blank" rel="noopener">Girshik, 2015. Fast R-CNN</a></li>
<li>Faster R-CNN: <a href="https://arxiv.org/pdf/1506.01497v3.pdf" target="_blank" rel="noopener">Ren et al., 2016. Faster R-CNN: Towards real-time object detection with region proposal networks</a></li>
</ul>
<h1 id="特殊应用：人脸识别和神经风格转换"><a href="#特殊应用：人脸识别和神经风格转换" class="headerlink" title="特殊应用：人脸识别和神经风格转换"></a>特殊应用：人脸识别和神经风格转换</h1><h2 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h2><p><strong>人脸识别</strong> ( <code>Face recognition</code> ) 和<strong>人脸验证</strong> ( <code>Face verification</code> ) 的区别：</p>
<ul>
<li>人脸验证：<ul>
<li>Input image, name/ID</li>
<li>Output whether the input image is that of the claimed person</li>
</ul>
</li>
<li>人脸识别：<ul>
<li>Has a database of K persons</li>
<li>Get an input image</li>
<li>Output ID if the image is any of the K persons (or “not recognized” )</li>
</ul>
</li>
</ul>
<p>人脸识别问题要比人脸验证问题困难的多。</p>
<h2 id="One-shot-学习"><a href="#One-shot-学习" class="headerlink" title="One-shot 学习"></a>One-shot 学习</h2><p>人脸识别面临的一个挑战是需要解决<strong>一次学习</strong>问题，意味着在大多数人脸识别应用中，需要通过单单一张图片或者一个人脸样例能去识别这个人。在一次学习问题中，只能通过一个样本进行学习，也能认出同一个人。<strong>一次学习是迁移学习的一种。</strong><br>一个好的解决方法是学习 <code>Similarity</code> 函数，也就是说让神经网络学习一个用 <code>d</code> 表示的函数：$$d(\text {img1,img2}) = \text {degree of difference between images}$$以两张图片作为输入，输出两张图片的差异值。如果是同一个人，输出的差异值很小；如果是不同的人，输出的差异值很大。实际应用中可以设置一个阈值 \(\alpha\) 如果小于这个阈值则判断是同一个人，反之是不同的人。</p>
<h2 id="Siamese-网络"><a href="#Siamese-网络" class="headerlink" title="Siamese 网络"></a>Siamese 网络</h2><p>学习 <code>Similarity</code> 函数的一种方式是使用 <code>Siamese</code> 网络，如下图所示：<img src="/images/deeplearning_ai_094.png" alt="Siamese网络">将图片 \(x ^{(1)}\)，\(x ^{(2)}\) 分别输入两个相同的卷积网络中（实际上是一个网络，两个网络的参数完全一样），经过全连接层后得到特征向量 \(f(x ^{(1)})\)，\(f(x ^{(2)})\) 。进而将 \(x ^{(1)}\)，\(x ^{(2)}\) 的距离定义为这两幅图片的编码之差的范数：$$d(x ^{(1)},x ^{(2)})=||f(x ^{(1)})-f(x ^{(2)})|| _2 ^ 2$$<strong>需要做的就是学习参数（通过反向传播）</strong>，使得如果两个输入是同一个人，得到的两个编码的距离就小。如何定义实际的目标函数（代价函数），可以利用三元损失函数达到这个目的。<br>相关论文：<a href="http://www.cs.wayne.edu/~mdong/taigman_cvpr14.pdf" target="_blank" rel="noopener">Taigman et al., 2014, DeepFace closing the gap to human level performance</a></p>
<h2 id="Triplet-损失"><a href="#Triplet-损失" class="headerlink" title="Triplet 损失"></a>Triplet 损失</h2><p>要想通过神经网络的参数来得到优质的人脸图片编码，方法之一是<strong>定义三元组损失函数，然后应用梯度下降法</strong>。<br><code>Triplet</code> 一词来源于训练这个神经网络需要大量的包含 <code>Anchor</code> (靶目标）、<code>Positive</code> （正例）、<code>Negative</code> （反例）的图片组，其中 <code>Anchor</code> (靶目标）和 <code>Positive</code> （正例）是同一个人的人脸图像，如下图所示：<img src="/images/deeplearning_ai_095.png" alt="三元组">对于一个三元组，应该满足：$$||f(A)-f(P)|| _2 ^2 +\alpha \leq ||f(A)-f(N)|| _2 ^2$$其中 \(\alpha \) 被称为<strong>间隔</strong>( <code>margin</code> ) ，用于确保 \(f()\) 不会总输出 \(0\) （或者一个恒定的值）。<br><code>Triplet</code> 损失函数定义如下：$$L(A,P,N)=\text {max}(||f(A)-f(P)|| _2 ^2 -||f(A)-f(N)|| _2 ^2 +\alpha , 0)$$对于大小为 \(m\) 的训练集，代价函数为：$$J=\sum _{i=1} ^{m}L(A ^{(i)},P ^{(i)},N ^{(i)})$$通过梯度下降法来最小化代价函数。<br>选择三元组时，最好的做法是人为增加 <code>Anchor</code> 和 <code>Positive</code> 的区别，缩小 <code>Anchor</code> 和 <code>Negative</code> 的区别，促使模型去学习不同人脸之间的关键差异，而不是随机选择构建三元组。<br>为了构建三元组的数据集，需要同一个人的多张照片。如果只有一个人的一张照片，则根本无法训练这个系统。当然，训练完这个系统后，可以应用到一次学习问题上。<br>相关论文：<a href="https://arxiv.org/pdf/1503.03832.pdf" target="_blank" rel="noopener">Schroff et al., 2015, FaceNet: A unified embedding for face recognition and clustering</a></p>
<h2 id="面部验证和二分类"><a href="#面部验证和二分类" class="headerlink" title="面部验证和二分类"></a>面部验证和二分类</h2><p><code>Triplet</code> 损失是一个学习人脸识别卷积网络参数的好方法。还有其他的方法，例如将人脸识别当成一个二分类问题。<br><img src="/images/deeplearning_ai_096.png" alt="人脸识别二分类">输入一对图片，将 <code>Siamese</code> 网络产生的特征向量输入同一个 <code>sigmoid</code> 单元，输出 \(1\) 则表示识别为同一个人，输出 \(0\) 表示识别为不同的人。<br><code>sigmoid</code> 单元对应的表达式如下：$$\widehat y = \sigma \Big(\sum _{k=1} ^{K}w _i|f(x ^{(i)}) _k - f(x ^{(j)}) _k| +b\Big )$$上式中 \(w _k \) 和 \(b\) 是通过梯度下降法训练得到的参数，下标 \(k\) 表示特征向量中第 \(k\) 个特征值。<br>还有另外一种表示：$$\widehat y = \sigma \Big(\sum _{k=1} ^{K}w _i\dfrac {((f(x ^{(i)}) _k - f(x ^{(j)}) _k) ^2}{f(x ^{(i)}) _k + f(x ^{(j)}) _k} +b\Big )$$其中 \(\dfrac {(f(x ^{(i)}) _k - f(x ^{(j)}) _k) ^2}{f(x ^{(i)}) _k + f(x ^{(j)}) _k}\)被称为 \(\chi\) 相似度。<br>无论是对于使用 <code>Triplet</code> 损失函数的网络，还是二分类结构，为了减少计算量，可以提前计算好编码输出 <code>f(x)</code> 并保存。这样就不必存储原始图片，并且每次进行人脸识别时只需要计算测试图片的编码输出。</p>
<h2 id="神经风格迁移"><a href="#神经风格迁移" class="headerlink" title="神经风格迁移"></a>神经风格迁移</h2><p><strong>神经风格迁移</strong>是将参考风格图像的风格 “迁移” 到令一张图像中，生成具有其特色的图像。<img src="/images/deeplearning_ai_097.png" alt="神经风格迁移"></p>
<h2 id="深度卷积网络在学什么"><a href="#深度卷积网络在学什么" class="headerlink" title="深度卷积网络在学什么"></a>深度卷积网络在学什么</h2><p><img src="/images/deeplearning_ai_098.png" alt="可视化卷积网络">上图显示浅层的隐藏层通常检测出是原始图像的边缘，颜色，阴影等简单信息。随着层数的增加，隐藏单元能捕捉区域更大区域，学习的特征也更加复杂和具体。<br>相关论文：<a href="https://arxiv.org/pdf/1311.2901.pdf" target="_blank" rel="noopener">Zeiler and Fergus., 2013, Visualizing and understanding convolutional networks</a></p>
<h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>要构建一个神经风格迁移系统，需要为生成的图像定义一个代价函数。代价函数如下：$$J(G)=\alpha J _{\text {content}}(C, G)+\beta J _{\text {style}}(S, G)$$其中 \(J _{\text {content}}(C, G)\) 度量生成图片 \(G\) 和内容图片 \(C\) 的相似性，\(J _{\text {style}}(S, G)\) 度量生成图片 \(G\) 和风格图像 \(S\) 的相似性。利用超参数 \(\alpha\) 和 \(\beta\) 权衡两种代价度量。<br>神经风格迁移的算法步骤如下：</p>
<ul>
<li>随机生成图片 \(G\) 的所有像素点；</li>
<li>利用梯度下降法使得代价函数 \(J(G)\) 最小，不断修正 \(G\) 的像素点（\(G := G-\dfrac {\partial J(G)}{\partial G}\)）。（注意：这个步骤中更新的是图像 \(G\) 的像素）</li>
</ul>
<p>相关论文：<a href="https://arxiv.org/pdf/1508.06576v2.pdf" target="_blank" rel="noopener">Gatys al., 2015. A neural algorithm of artistic style</a></p>
<h2 id="内容代价函数"><a href="#内容代价函数" class="headerlink" title="内容代价函数"></a>内容代价函数</h2><p>内容代价函数 \(J _{\text {content}}(C, G)\) 计算过程如下：</p>
<ol>
<li>使用一个预训练好的 <code>CNN</code> （例如 <code>VGG</code> ）；</li>
<li>选择某一个隐藏层 <code>l</code> 来计算内容代价。<code>l</code> 太小使得生成图片像素非常接近你的内容图片，<code>l</code> 太大会使得内容图片里有狗，确保生成图片也有狗。实际中 <code>l</code> 通常选择中间层。</li>
<li>设 \(a ^{(C)[l]}\)、\(a ^{(G)[l]}\) 为 <code>C</code> 和 <code>G</code> 在 <code>l</code> 层的激活，则有：$$J _{\text {content}}(C, G)=\dfrac {1}{2}||(a ^{(C)[l]}-a ^{(G)[l]})|| ^2$$\(a ^{(C)[l]}\) 和 \(a ^{(G)[l]}\)越相似，则 \(J _{\text {content}}(C, G)\) 越小。</li>
</ol>
<h2 id="风格代价函数"><a href="#风格代价函数" class="headerlink" title="风格代价函数"></a>风格代价函数</h2><p>同一层的每个通道提取的特征不同（滤波器不同），例如有的通道提取垂直纹理特征，有的通道提取背景颜色。计算两个通道（同一层）间的相关性，即表示原始图片既包含了垂直纹理也包含了背景颜色的可能性大小。<br>通过 <code>CNN</code> ，<strong>风格被定义为同一个隐藏层不同通道之间激活值的相关系数</strong>，反映了原始图片特征之间的相互关系。<br>对于风格图像 <code>S</code> ，选择 <code>l</code> 层，则相关系数以一个 <code>gram</code> 矩阵形式表示：$$G _{kk’} ^{[l]\left (S\right )}=\sum _{i=1} ^{n _H ^{[l]}}\sum _{j=1} ^{n _W ^{[l]}}a _{ijk} ^{[l]\left (S\right )}a _{ijk’} ^{[l]\left (S\right )}$$其中 \(i\) 和 \(j\) 为 \(l\) 层的高度和宽度，\(k\) 和 \(k’\) 为选择的同一层的不同通道，范围为 \(1\) 到 \(n _C ^{[l]}\) ，\(a _{ijk} ^{[l]\left (S\right )}\) 为对应的激活值。同理对于生成图像 <code>G</code> 有：$$G _{kk’} ^{[l]\left (G\right )}=\sum _{i=1} ^{n _H ^{[l]}}\sum _{j=1} ^{n _W ^{[l]}}a _{ijk} ^{[l]\left (G\right )}a _{ijk’} ^{[l]\left (G\right )}$$则第 \(l\) 层的风格代价函数为：$$J _{\text {style}}^ {[l]}(S, G)=\dfrac {1}{(2n _H ^{[l]}n _W ^{[l]}n _C ^{[l]}) ^2}\sum _{k}\sum _{k’}(G _{kk’} ^{[l]\left (S\right )}-G _{kk’} ^{[l]\left (G\right )}) ^2$$对每一层都用风格代价函数，效果会更好，有：$$J _{\text {style}}(S, G)=\sum _{l}\lambda ^{[l]}J _{\text {style}}^ {[l]}(S, G)$$其中 \(\lambda \) 为设置不同层占的权重。</p>
<h2 id="一维到三维推广"><a href="#一维到三维推广" class="headerlink" title="一维到三维推广"></a>一维到三维推广</h2><p><img src="/images/deeplearning_ai_099.png" alt="卷积网络一维到三维的推广"><code>ECG</code> 数据（心电图）是由时间序列对应的每个瞬间的电压组成，是一维数据。一般来说我们会用 <code>RNN</code>（循环神经网络）来处理，不过如果用卷积处理，则有：</p>
<ul>
<li>输入时间序列维度：\(14\times 1\)</li>
<li>滤波器大小：\(5\times 1\) ，个数为 \(16\)</li>
<li>输出时间序列维度：\(10\times 16\)</li>
</ul>
<p>对于三维图片有：</p>
<ul>
<li>输入 <code>3D</code> 图像大小：\(14\times 14\times 14\times 1\)</li>
<li>滤波器大小：\(5\times 5\times 5\times 1\)，个数为 \(16\)</li>
<li>输出 <code>3D</code> 图片大小：\(10\times 10\times 10\times 16\)</li>
</ul>
<p>相关连接：<br><a href="https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf" target="_blank" rel="noopener">DeepFace: Closing the gap to human-level performance in face verification</a><br><a href="https://github.com/iwantooxxoox/Keras-OpenFace" target="_blank" rel="noopener">https://github.com/iwantooxxoox/Keras-OpenFace</a><br><a href="http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style" target="_blank" rel="noopener">TensorFlow Implementation of “A Neural Algorithm of Artistic Style”</a><br><a href="http://www.vlfeat.org/matconvnet/pretrained/" target="_blank" rel="noopener">MatConvNet</a><br><a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">Very deep convolutional networks for large-scale image recognition</a><br><a href="https://harishnarayanan.org/writing/artistic-style-transfer/" target="_blank" rel="noopener">Convolutional neural networks for artistic style transfer</a><br><a href="https://github.com/davidsandberg/facenet" target="_blank" rel="noopener">the official FaceNet github repository</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03-构建机器学习项目/" rel="next" title="03.构建机器学习项目">
                <i class="fa fa-chevron-left"></i> 03.构建机器学习项目
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/05-序列模型/" rel="prev" title="05.序列模型">
                05.序列模型 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/wukong.png"
                alt="穆义" />
            
              <p class="site-author-name" itemprop="name">穆义</p>
              <p class="site-description motion-element" itemprop="description">既已无岸，不必回头，唯有向前，踏碎云霄，放肆桀骜</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#卷积神经网络基础"><span class="nav-number">1.</span> <span class="nav-text">卷积神经网络基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#计算机视觉"><span class="nav-number">1.1.</span> <span class="nav-text">计算机视觉</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#边缘检测"><span class="nav-number">1.2.</span> <span class="nav-text">边缘检测</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#更多边缘检测例子"><span class="nav-number">1.3.</span> <span class="nav-text">更多边缘检测例子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#填充-Padding"><span class="nav-number">1.4.</span> <span class="nav-text">填充 ( Padding )</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积步长-Strided-convolutions"><span class="nav-number">1.5.</span> <span class="nav-text">卷积步长 ( Strided convolutions )</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#高维卷积"><span class="nav-number">1.6.</span> <span class="nav-text">高维卷积</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#单层卷积网络-one-layer-of-a-convolutional-network"><span class="nav-number">1.7.</span> <span class="nav-text">单层卷积网络 (one layer of a convolutional network)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#简单卷积网络示例"><span class="nav-number">1.8.</span> <span class="nav-text">简单卷积网络示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#池化层"><span class="nav-number">1.9.</span> <span class="nav-text">池化层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积神经网络示例"><span class="nav-number">1.10.</span> <span class="nav-text">卷积神经网络示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用卷积神经网络原因"><span class="nav-number">1.11.</span> <span class="nav-text">使用卷积神经网络原因</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#深度卷积网络：实例探究"><span class="nav-number">2.</span> <span class="nav-text">深度卷积网络：实例探究</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#经典网络"><span class="nav-number">2.1.</span> <span class="nav-text">经典网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LeNet-5"><span class="nav-number">2.1.1.</span> <span class="nav-text">LeNet-5</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AlexNet"><span class="nav-number">2.1.2.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VGG"><span class="nav-number">2.1.3.</span> <span class="nav-text">VGG</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#残差网络-Residual-Networks-ResNets"><span class="nav-number">2.2.</span> <span class="nav-text">残差网络 (Residual Networks, ResNets)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#残差网络有效的原因"><span class="nav-number">2.3.</span> <span class="nav-text">残差网络有效的原因</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-times-1-卷积"><span class="nav-number">2.4.</span> <span class="nav-text">\(1\times 1\) 卷积</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inception-网络"><span class="nav-number">2.5.</span> <span class="nav-text">Inception 网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#完整的-Inception-网络"><span class="nav-number">2.6.</span> <span class="nav-text">完整的 Inception 网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用开源的实现方案"><span class="nav-number">2.7.</span> <span class="nav-text">使用开源的实现方案</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#迁移学习"><span class="nav-number">2.8.</span> <span class="nav-text">迁移学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据扩增"><span class="nav-number">2.9.</span> <span class="nav-text">数据扩增</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">2.10.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#目标检测"><span class="nav-number">3.</span> <span class="nav-text">目标检测</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#目标定位"><span class="nav-number">3.1.</span> <span class="nav-text">目标定位</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征点检测"><span class="nav-number">3.2.</span> <span class="nav-text">特征点检测</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#目标检测-1"><span class="nav-number">3.3.</span> <span class="nav-text">目标检测</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积的滑动窗口实现"><span class="nav-number">3.4.</span> <span class="nav-text">卷积的滑动窗口实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#边框预测"><span class="nav-number">3.5.</span> <span class="nav-text">边框预测</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#交并比"><span class="nav-number">3.6.</span> <span class="nav-text">交并比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#非极大值抑制-Non-max-suppression"><span class="nav-number">3.7.</span> <span class="nav-text">非极大值抑制 (Non-max suppression)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Anchor-Boxes"><span class="nav-number">3.8.</span> <span class="nav-text">Anchor Boxes</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLO-算法"><span class="nav-number">3.9.</span> <span class="nav-text">YOLO 算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#候选区域-Region-proposals"><span class="nav-number">3.10.</span> <span class="nav-text">候选区域 (Region proposals)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#特殊应用：人脸识别和神经风格转换"><span class="nav-number">4.</span> <span class="nav-text">特殊应用：人脸识别和神经风格转换</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#人脸识别"><span class="nav-number">4.1.</span> <span class="nav-text">人脸识别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#One-shot-学习"><span class="nav-number">4.2.</span> <span class="nav-text">One-shot 学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Siamese-网络"><span class="nav-number">4.3.</span> <span class="nav-text">Siamese 网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Triplet-损失"><span class="nav-number">4.4.</span> <span class="nav-text">Triplet 损失</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#面部验证和二分类"><span class="nav-number">4.5.</span> <span class="nav-text">面部验证和二分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经风格迁移"><span class="nav-number">4.6.</span> <span class="nav-text">神经风格迁移</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#深度卷积网络在学什么"><span class="nav-number">4.7.</span> <span class="nav-text">深度卷积网络在学什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代价函数"><span class="nav-number">4.8.</span> <span class="nav-text">代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#内容代价函数"><span class="nav-number">4.9.</span> <span class="nav-text">内容代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#风格代价函数"><span class="nav-number">4.10.</span> <span class="nav-text">风格代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一维到三维推广"><span class="nav-number">4.11.</span> <span class="nav-text">一维到三维推广</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">穆义</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  

  

  

</body>
</html>
