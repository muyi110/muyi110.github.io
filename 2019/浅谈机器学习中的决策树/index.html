<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Times New Roman:300,300italic,400,400italic,700,700italic|18:300,300italic,400,400italic,700,700italic|Courier New:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hello, World" />










<meta name="description" content="决策树 决策树(decision tree)是一种基本的分类与回归方法。主要优点是模型具有可读性、分类速度快。决策树学习包括三个步骤：特征选择、决策树生成、决策树修剪分类决策树模型是一种描述对实例进行分类的树形结构，决策树由节点和有向边组成。节点有两种类型：内部节点和叶节点，内部节点表示一个特征或者属性、外部节点表示一个类  用决策树分类从根节点开始，对实例的某一特征进行分类，根据测试结果将">
<meta property="og:type" content="article">
<meta property="og:title" content="浅谈机器学习中的决策树">
<meta property="og:url" content="https://muyi110.github.io/2019/浅谈机器学习中的决策树/index.html">
<meta property="og:site_name" content="MuYi&#39;s Blog">
<meta property="og:description" content="决策树 决策树(decision tree)是一种基本的分类与回归方法。主要优点是模型具有可读性、分类速度快。决策树学习包括三个步骤：特征选择、决策树生成、决策树修剪分类决策树模型是一种描述对实例进行分类的树形结构，决策树由节点和有向边组成。节点有两种类型：内部节点和叶节点，内部节点表示一个特征或者属性、外部节点表示一个类  用决策树分类从根节点开始，对实例的某一特征进行分类，根据测试结果将">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://muyi110.github.io/images/cart_0.png">
<meta property="og:updated_time" content="2019-08-02T14:18:45.032Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="浅谈机器学习中的决策树">
<meta name="twitter:description" content="决策树 决策树(decision tree)是一种基本的分类与回归方法。主要优点是模型具有可读性、分类速度快。决策树学习包括三个步骤：特征选择、决策树生成、决策树修剪分类决策树模型是一种描述对实例进行分类的树形结构，决策树由节点和有向边组成。节点有两种类型：内部节点和叶节点，内部节点表示一个特征或者属性、外部节点表示一个类  用决策树分类从根节点开始，对实例的某一特征进行分类，根据测试结果将">
<meta name="twitter:image" content="https://muyi110.github.io/images/cart_0.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://muyi110.github.io/2019/浅谈机器学习中的决策树/"/>





  <title>浅谈机器学习中的决策树 | MuYi's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">MuYi's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">此博客创建于2018-07-10</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-reprinted_article">
          <a href="/reprinted-article/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heart"></i> <br />
            
            转载文章
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://muyi110.github.io/2019/浅谈机器学习中的决策树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="穆义">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/wukong.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MuYi's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">浅谈机器学习中的决策树</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-01T11:49:25+08:00">
                2019-08-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><blockquote>
<p>决策树(decision tree)是一种基本的分类与回归方法。主要优点是模型具有可读性、分类速度快。决策树学习包括三个步骤：特征选择、决策树生成、决策树修剪<br>分类决策树模型是一种描述对实例进行分类的树形结构，决策树由节点和有向边组成。节点有两种类型：内部节点和叶节点，内部节点表示一个特征或者属性、外部节点表示一个类</p>
</blockquote>
<p>用决策树分类从根节点开始，对实例的某一特征进行分类，根据测试结果将实例分配到子节点，如此递归对测试实例进行分配，直到到达叶节点，将实例分配到叶节点的类中</p>
<h2 id="决策树学习"><a href="#决策树学习" class="headerlink" title="决策树学习"></a>决策树学习</h2><p>决策树学习的本质是从训练数据集中归纳出一组分类规则。与训练数据集不矛盾的决策树（对训练数据集能正确分类）可能有多个，也可能一个都没有，需要<strong>找一个与训练数据集矛盾较小的决策树，同时具有很好的泛化能力</strong>。如果特征数量较多，可以在决策树学习开始的时候，<strong>对特征进行选择</strong>，保留对训练数据有足够分类能力的特征<br><strong>决策树的生成对应模型的局部选择，决策树的剪枝对应模型的全局选择，决策树的生成只考虑局部最优，决策树的剪枝则考虑全局最优</strong></p>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><blockquote>
<p>特征选择在于选取对训练数据具有分类能力的特征，以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果差别不大，则称这个特征没有分类能力，经验上扔掉这样的特征对决策树学习的精度影响不大。特征选择的准则是<strong>信息增益</strong>或者<strong>信息增益比</strong></p>
</blockquote>
<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>基于信息增益准则的特征选择算法：对训练数据集\(D\)计算每个特征的信息增益，比较大小，选择信息增益最大的特征</p>
<blockquote>
<p>熵的定义如下$$H(X) = -\sum_{i=1}^{n}p_i\log p_i$$其中\(P(X=x_i)=p_i, \quad i = 1, 2, \dots, n\)，若\(p_i=0\)则定义\(0\log 0 = 0\)。熵只依赖于\(X\)的分布，与\(X\)的取值无关，所以可将\(X\)的熵记作$$H(p)=-\sum_{i=1}^{n}p_i\log p_i$$<strong>熵越大，随机变量的不确定性就越大</strong><br>例：当随机变量只取两个值\(0\)和\(1\)时，则熵为\(H(p)=-p\log_2 p - (1-p)\log_2 (1-p)\)当\(p=0.5\)时，熵取最大值，随机变量的不确定性最大<br>条件熵的定义如下$$H(Y|X)=\sum_{i=1}^{n}p_i H(Y|X=x_i)$$其中\(p_i = P(X=x_i)\)，表示给定\(X\)条件下\(Y\)的条件概率分布的熵对\(X\)的数学期望</p>
</blockquote>
<p>信息增益表示得知特征\(X\)的信息使得类别\(Y\)的信息不确定性减少的程度，信息增益定义如下</p>
<blockquote>
<p>特征\(A\)对训练数据集\(D\)的信息增益\(g(D, A)\)定义为数据集\(D\)的经验熵\(H(D)\)与特征\(A\)给定条件下\(D\)的条件熵\(H(D|A)\)之差$$g(D, A)=H(D)-H(D|A)$$熵\(H(Y)\)与条件熵\(H(D|A)\)之差称为互信息(mutual information)</p>
</blockquote>
<p>信息增益<strong>算法</strong>如下<br>输入：数据集\(D\)和特征\(A\)<br>输出：特征\(A\)对数据集\(D\)的信息增益\(g(D, A)\)</p>
<ol>
<li>计算数据集\(D\)的经验熵$$H(D)=-\sum_{k=1}^{K}\dfrac{|C_k|}{|D|}\log_2 \dfrac{|C_k|}{|D|}$$其中\(|C_k|\)表示属于类别\(k\)的样本个数，\(D\)表示样本个数</li>
<li>计算特征\(A\)对数据集\(D\)的条件熵\(H(D|A)\)$$H(D|A)=\sum_{i=1}^{n}\dfrac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^{n}\dfrac{|D_i|}{|D|}\sum_{k=1}^{K}\dfrac{|D_{ik}|}{|D_i|}\log_2 \dfrac{|D_{ik}|}{|D_i|}$$其中\(n\)表示特征取值个数，\(|D_i|\)表示具有特征\(i\)取值样本个数，\(|D_{ik}|\)表示具有特征\(i\)取值的样本中属于类别\(k\)的样本个数</li>
<li>计算信息增益$$g(D, A)=H(D)-H(D|A)$$</li>
</ol>
<h3 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h3><p>以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征问题，使用信息增益比对其进行校正</p>
<blockquote>
<p>解释：样本总个数是一定的，当某一特征取值较多时候，则分配到每一个特征取值的样本变少，而计算条件熵$$H(D|A)=\sum_{i=1}^{n}\dfrac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^{n}\dfrac{|D_i|}{|D|}\sum_{k=1}^{K}\dfrac{|D_{ik}|}{|D_i|}\log_2 \dfrac{|D_{ik}|}{|D_i|}$$需要计算某一特征取值为\(i\)时对应的各个类别的概率，而实际用频率，即\(\dfrac{|D_{ik}|}{|D_i|}\)来估计概率，当对应的样本少时，这种估计并不准确（可以类比投硬币实验，实验次数少，估计的结果不准确），存在偏差，使得分布不均匀，而分布不均匀导致计算的熵偏低，进而使得信息增益较大，所以有以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征问题<br>拓展：如果每一个特征取值下样本数目足够多，则用频率可近似估计概率，每一个特征计算的条件熵基本一样，就不存在所谓的以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征问题</p>
</blockquote>
<p>信息增益比\(g_R(D, A)\)定义为信息增益\(g(D, A)\)与训练集\(D\)关于特征\(A\)的值的熵\(H_A(D)\)之比$$g_R(D, A)=\dfrac{g(D, A)}{H_A(D)}$$其中\(H_A(D) = -\sum_{i=1}^{n}\dfrac{|D_i|}{|D|}\log_2 \dfrac{|D_i|}{|D|}\)，\(n\)是特征\(A\)取值的个数</p>
<blockquote>
<p>如果\(i\)的取值越多，则\(H_A(D)\)的值越大</p>
</blockquote>
<h2 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h2><h3 id="text-ID3-算法"><a href="#text-ID3-算法" class="headerlink" title="\(\text{ID3}\)算法"></a>\(\text{ID3}\)算法</h3><p>输入：训练数据集\(D\)，特征集\(A\)和阈值\(\varepsilon\)<br>输出：决策树\(T\)</p>
<ol>
<li>若集合\(D\)中所有实例属于同一个类别\(C_k\)，则决策树\(T\)为单节点树，将类别\(C_k\)作为该节点的类标记，返回\(T\)（递归基准条件）</li>
<li>若\(A=\varnothing\)，则\(T\)为单节点树，将\(D\)中具有实例数最大的类\(C_k\)作为该节点的类标记，返回\(T\)（递归基准条件）</li>
<li>计算\(A\)中各个特征对数据集\(D\)的信息增益，选择信息增益最大的特征\(A_g\)</li>
<li>如果\(A_g\)的信息增益小于阈值\(\varepsilon\)，则置\(T\)为单节点树，将\(D\)中实例数最大的类\(C_k\)作为该节点的类标记，返回\(T\)（递归基准条件）</li>
<li>否则，对\(A_g\)的每一个可能取值\(a_i\)，依\(A_g=a_i\)将\(D\)分割为若干个非空子集\(D_i\)，将\(D_i\)中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树\(T\)，返回\(T\)</li>
<li>对第\(i\)个子节点，以\(D_i\)为训练集，以\(A-\{A_g\}\)为特征集，递归调用\(1\backsim 5\)，得到子树\(T_i\)，返回\(T_i\)</li>
</ol>
<blockquote>
<p><code>ID3</code>算法的核心是在决策树各个节点上应用<strong>信息增益准则选择特征</strong>，递归构建决策树，<code>ID3</code>相当于用极大似然法进行概率模型的选择。<code>ID3</code>算法只有树的生成，该算法生成的树容易产生过拟合</p>
</blockquote>
<h3 id="text-C4-5-算法"><a href="#text-C4-5-算法" class="headerlink" title="\(\text{C4.5}\)算法"></a>\(\text{C4.5}\)算法</h3><p>输入：训练数据集\(D\)，特征集\(A\)和阈值\(\varepsilon\)<br>输出：决策树\(T\)</p>
<ol>
<li>若集合\(D\)中所有实例属于同一个类别\(C_k\)，则决策树\(T\)为单节点树，将类别\(C_k\)作为该节点的类标记，返回\(T\)（递归基准条件）</li>
<li>若\(A=\varnothing\)，则\(T\)为单节点树，将\(D\)中具有实例数最大的类\(C_k\)作为该节点的类标记，返回\(T\)（递归基准条件）</li>
<li>计算\(A\)中各个特征对数据集\(D\)的信息增益比，选择信息增益比最大的特征\(A_g\)</li>
<li>如果\(A_g\)的信息增益比小于阈值\(\varepsilon\)，则置\(T\)为单节点树，将\(D\)中实例数最大的类\(C_k\)作为该节点的类标记，返回\(T\)（递归基准条件）</li>
<li>否则，对\(A_g\)的每一个可能取值\(a_i\)，依\(A_g=a_i\)将\(D\)分割为若干个非空子集\(D_i\)，将\(D_i\)中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树\(T\)，返回\(T\)</li>
<li>对第\(i\)个子节点，以\(D_i\)为训练集，以\(A-\{A_g\}\)为特征集，递归调用\(1\backsim 5\)，得到子树\(T_i\)，返回\(T_i\)</li>
</ol>
<blockquote>
<p>和<code>ID3</code>算法相似，<code>C4.5</code>在生成树的过程中，用<strong>信息增益比来选择特征</strong></p>
</blockquote>
<h2 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h2><blockquote>
<p>决策树生成算法递归产生决策树，直到不能继续进行下去为止，容易产生过拟合，因为在学习过程中，过多考虑对训练数据的正确分类，从而构造出复杂的决策树，解决的方法是考虑决策树的复杂度，对已生成的决策树进行剪枝，进而简化决策树模型</p>
</blockquote>
<p>决策树的剪枝往往通过<strong>极小化决策树整体损失函数</strong>实现。设树\(T\)的叶节点个数为\(|T|\)，\(t\)是树\(T\)的叶节点，该叶节点有\(N_t\)个样本，其中第\(k\)类的样本有\(N_{tk}\)个，\(k=1, 2\dots, K\)，\(H_t(T)\)为叶节点\(t\)上的经验熵，\(\alpha \ge 0\)为超参数，决策树的损失函数定义为$$C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha |T|$$其中熵为$$H_t(T)=-\sum_{k}\dfrac{N_{tk}}{N_t}\log\dfrac{N_{tk}}{N_t}$$将损失函数的第一项定义为$$C(T)=\sum_{t=1}^{|T|}N_tH_t(T) = -\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}\log\dfrac{N_{tk}}{N_t}$$则损失函数可以写为$$C_{\alpha}(T)=C(T)+\alpha |T|$$其中\(C(T)\)表示对训练数据的预测误差（如果每一个叶节点可以较好分类，则对应的熵较小，理想情况下熵为\(0\)，即叶节点中只有一个类别），即模型对训练数据的拟合程度，\(|T|\)表示模型复杂度，\(\alpha\)较大，则促使选择简单模型，\(\alpha\)较小，促使选择复杂模型<br>剪枝就是当\(\alpha\)确定时，选择使得损失函数最小的模型，即损失函数最小的子树。决策树生成只考虑提高信息增益（或者信息增益比）来对训练数据进行拟合，而决策树的剪枝通过优化损失函数考虑减小模型复杂度，<strong>决策树生成学习局部模型，决策树剪枝学习整体模型</strong><br><strong>决策树剪枝算法</strong><br><img src="/images/cart_0.png" alt="决策树剪枝">输入：生成算法产生的整个树\(T\)，参数\(\alpha\)<br>输出：剪枝后的子树\(T_{\alpha}\)</p>
<ol>
<li>计算每个节点的经验熵</li>
<li>递归从树的叶节点向上回缩<br>设一组叶节点回缩到其父节点之前和之后的整体树分别为\(T_B\)和\(T_A\)，其对应的损失函数分别为\(C_{\alpha}(T_B)\)和\(C_{\alpha}(T_A)\)，若$$C_{\alpha}(T_A)\le C_{\alpha}(T_B)$$则进行剪枝，即将父节点变为新的叶节点</li>
<li>返回第\(2\)步，知道不能继续为止，得到损失函数最小的子树\(T_{\alpha}\)</li>
</ol>
<blockquote>
<p>第\(2\)步中只考虑两个树的损失函数之差，其计算可以在局部进行，因此，决策树剪枝算法可以通过<strong>动态规划算法</strong>实现</p>
</blockquote>
<h2 id="text-CART-算法"><a href="#text-CART-算法" class="headerlink" title="\(\text{CART}\)算法"></a>\(\text{CART}\)算法</h2><p>分类与回归树(classification and regression tree, CART)是在给定输入随机变量\(X\)的条件下输出随机变量\(Y\)的条件概率分布的学习方法。<code>CART</code>假设决策树是<strong>二叉树</strong>，<code>CART</code>算法由以下两步组成</p>
<ol>
<li>决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大</li>
<li>决策树剪枝：用验证集对已生成的树进行剪枝，并选择最优子树，用损失函数最小作为剪枝标准</li>
</ol>
<h3 id="text-CART-生成"><a href="#text-CART-生成" class="headerlink" title="\(\text{CART}\)生成"></a>\(\text{CART}\)生成</h3><p><code>CART</code>决策树的生成就是递归构建<strong>二叉决策树</strong>的过程。对回归树用平方误差最小化准则，对分类树用<strong>基尼指数(Gini index)最小化准则</strong>进行特征选择。<br><strong>回归树生成</strong><br>假设\(X\)与\(Y\)分别为输入和输出变量，并且\(Y\)是连续变量，给定训练数据集$$D=\{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}$$考虑如何生成回归树。一棵回归树对应输入空间（特征空间）的一个划分及在划分单元上的输出值。假设将输入空间划分为\(M\)个单元\(R_1, R_2, \dots, R_M\)，且在每个单元\(R_m\)上有一个固定的输出值\(c_m\)，回归树可以表示为$$f(x)=\sum_{m=1}^{M}c_mI(x\in R_m)$$当输入空间划分确定时，可用平方误差\(\sum_{x_i\in R_m}(y_i - f(x_i))^2\)表示回归树对训练集的误差，基于平方误差最小准则求解每个单元上的最优输出值。可知，单元\(R_m\)上的\(c_m\)最优输出值\(\hat{c_m}\)是\(R_m\)上所有输入\(x_i\)对应输出\(y_i\)的均值，即$$\hat{c_m} = \text{ave}(y_i|x_i\in R_m)$$问题转为如何对输入空间划分。采用启发式的方式，选择第\(j\)个变量\(x^{j}\)（也就是第\(j\)个特征）和其取值\(s\)作为切分变量和切分点，定义两个区域$$R_1(j, s) = \{x|x^{j}\le s\}\quad 和\quad R_2(j, s) = \{x|x^{j} &gt; s\}$$然后寻找最优切分变量\(j\)（第\(j\)个特征）和对应的最优切分点\(s\)，即求解$$\mathop{\text{min}}\limits_{j, s}\big [\mathop{\text{min}}\limits_{c_1} \sum_{x_i \in R_1 (j, s)}(y_i - c_1)^2 + \mathop{\text{min}}\limits_{c_2}\sum_{x_i \in R_2 (j, s)}(y_i - c_2)^2\big]$$对固定输入变量\(j\)（第\(j\)个特征）可以找到最优切分点\(s\)$$\hat{c_1}=\text{ave}(y_i|x_i\in R_1(j, s))\quad 和 \quad \hat{c_2}=\text{ave}(y_i|x_i\in R_2(j, s))$$遍历所有输入变量(特征），找到最优切分变量（特征）\(j\)，构成一个对\((j, s)\)，根据此将输入空间分为两个区域。对每个区域重复上述划分过程，直到满足停止条件，生成一颗回归树，这样的回归树称为最小二乘回归树。<br><strong>最小二乘回归树生成算法</strong><br>输入：训练数据集\(D\)<br>输出：回归树\(f(x)\)<br>在训练数据集所在的输入空间（特征空间），递归地将每个区域划分为两个子区域，并决定每个子区域的输出值，<strong>构建二叉决策树</strong></p>
<ol>
<li>选择最优切分变量\(j\)（第\(j\)个特征)和切分点\(s\)，求解$$\mathop{\text{min}}\limits_{j, s}\big [\mathop{\text{min}}\limits_{c_1} \sum_{x_i \in R_1 (j, s)}(y_i - c_1)^2 + \mathop{\text{min}}\limits_{c_2}\sum_{x_i \in R_2 (j, s)}(y_i - c_2)^2\big]$$遍历变量\(j\)，对固定切分变量\(j\)扫描切分点\(s\)选择使得上式达到最小值的对\((j, s)\)</li>
<li>用选定的对\((j, s)\)划分区域，并决定相应的输出值$$R_1(j, s)=\{x|x^j \le s\}, \quad R_2(j, s)=\{x|x^j &gt; s\}\\\hat{c_1}=\dfrac{1}{N_m}\sum_{x_i\in R_m(j, s)}y_j, \quad x\in R_m, \quad m = 1, 2$$</li>
<li>继续对两个子区域重复\(1\backsim 2\)，直到满足停止条件</li>
<li>将输入空间划分为\(M\)个区域\(R_1, R_2, \dots, R_M\)，生成决策树$$f(x) = \sum_{m=1}^{M}\hat{c_m}I(x\in R_m)$$</li>
</ol>
<p><strong>分类树的生成</strong></p>
<blockquote>
<p><strong>基尼指数</strong>：分类问题中，假设有\(K\)个类别，样本点属于第\(k\)个类的概率为\(p_k\)，则概率分布的基尼指数为$$\text{Gini}(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k ^2$$对于二分类问题，若样本点属于第一个类的概率为\(p\)，则概率分布的基尼指数为$$\text{Gini}(p)=2p(1-p)$$对于给定的样本集合\(D\)，其基尼指数为$$\text{Gini}(D)=1-\sum_{k=1}^{K}\big(\dfrac{|C_k|}{|D|}\big)^2$$其中\(C_k\)表示属于第\(k\)类的样本子集，\(K\)为类别个数</p>
</blockquote>
<p>如果样本集合\(D\)根据特征\(A\)是否取某一可能值\(a\)被分为\(D_1\)和\(D_2\)两部分，即$$D_1=\{(x, y)\in D|A(x)=a\}, \quad D_2 = D-D_1$$则在特征\(A\)的条件下，集合\(D\)的基尼指数定义为$$\text{Gini}(D, A)=\dfrac{|D_1|}{|D|}\text{Gini}(D_1)+\dfrac{|D_2|}{|D|}\text{Gini}(D_2)$$基尼指数\(\text{Gini}(D)\)表示集合\(D\)的不确定性，\(\text{Gini}(D, A)\)表示经\(A=a\)分割后集合\(D\)的不确定性。基尼指数越大，样本集合的不确定性就越大，与熵相似<br><strong>\(\text{CART}\)分类树生成算法</strong><br>输入：训练数据集\(D\)，停止条件<br>输出：\(\text{CART}\) 决策树<br>根据训练数据，从根节点开始，递归对每个节点进行以下操作，构建二叉决策树</p>
<ol>
<li>设节点的训练数据集为\(D\)，计算现有特征对该数据集的基尼指数。此时，对每一个特征\(A\)，对其每一个取值\(a\)，根据样本点对\(A=a\)的测试为“是”或“否”（二叉树）将\(D\)分为\(D_1\)和\(D_2\)，计算\(A=a\)的基尼指数，即\(\text{Gini}(D, A)\)</li>
<li>在所有可能的特征\(A\)和它们所有可能切分点\(a\)中，选择基尼指数最小的特征，及对应的切分点最为最优特征与最优切分点。根据此最优特征和最优切分点，从当前节点生成两个子节点，将训练集根据此特征分配到两个子节点中</li>
<li>对两个子节点递归调用\(1\simback 2\)，直到满足停止条件</li>
<li>生成\(\text{CART}\)决策树</li>
</ol>
<blockquote>
<p>算法停止条件：节点中样本个数小于预定阈值，样本集的基尼指数小于阈值（样本基本属于同一个类），或者没有更多特征</p>
</blockquote>
<p>参考：统计学习方法（第二版）–李航</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/浅谈经验模态分解-EMD/" rel="next" title="浅谈经验模态分解-EMD">
                <i class="fa fa-chevron-left"></i> 浅谈经验模态分解-EMD
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/wukong.png"
                alt="穆义" />
            
              <p class="site-author-name" itemprop="name">穆义</p>
              <p class="site-description motion-element" itemprop="description">既已无岸，不必回头，唯有向前，踏碎云霄，放肆桀骜</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#决策树"><span class="nav-number">1.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树学习"><span class="nav-number">1.1.</span> <span class="nav-text">决策树学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征选择"><span class="nav-number">1.2.</span> <span class="nav-text">特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#信息增益"><span class="nav-number">1.2.1.</span> <span class="nav-text">信息增益</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#信息增益比"><span class="nav-number">1.2.2.</span> <span class="nav-text">信息增益比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树的生成"><span class="nav-number">1.3.</span> <span class="nav-text">决策树的生成</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#text-ID3-算法"><span class="nav-number">1.3.1.</span> <span class="nav-text">\(\text{ID3}\)算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#text-C4-5-算法"><span class="nav-number">1.3.2.</span> <span class="nav-text">\(\text{C4.5}\)算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树剪枝"><span class="nav-number">1.4.</span> <span class="nav-text">决策树剪枝</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#text-CART-算法"><span class="nav-number">1.5.</span> <span class="nav-text">\(\text{CART}\)算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#text-CART-生成"><span class="nav-number">1.5.1.</span> <span class="nav-text">\(\text{CART}\)生成</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">穆义</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  

  

  

</body>
</html>
